<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.528">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Automatic differentiation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script defer="" type="text/javascript" src="../../assets/toggle_button.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Automatic differentiation">
<meta property="og:description" content="">
<meta name="twitter:title" content="Automatic differentiation">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.svg" alt="fmin.xyz" class="navbar-logo">
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/MerkulovDaniil/optim" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/@fmin" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/fminxyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../docs/methods/index.html">Methods</a></li><li class="breadcrumb-item"><a href="../../docs/methods/Autograd.html">Automatic differentiation</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/theory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Matrix_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matrix calculus</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/theory/convex sets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex sets</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Affine_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Affine set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Convex_set.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Conic_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conic set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projection</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Convex_function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Conjugate_set.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Conjugate function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Dual norm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dual norm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Subgradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient and subdifferential</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Optimality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimality conditions. KKT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Convex_optimization_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex optimization problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Duality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Duality</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Rates_of_convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rates of convergence</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/line_search/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Line search</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/binary_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Binary search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/golden_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Golden search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/inexact.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inexact Line Search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/parabola.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Successive parabolic interpolation</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/zom/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Zero order methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/bee_algorithm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bee algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/nelder-mead.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nelderâ€“Mead</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/simulated-annealing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulated annealing</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/fom/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">First order methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/GD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Subgradient descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Projected_subgradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projected subgradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Mirror_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mirror descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/SGD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic gradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/SAG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic average gradient</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/ADAM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ADAM: A Method for Stochastic Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Lookahead.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lookahead Optimizer: <span class="math inline">k</span> steps forward, <span class="math inline">1</span> step back</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/adaptive_metrics/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Adaptive metric methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Newton.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Newton method</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Quasi_newton.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quasi Newton methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/CG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Natural_gradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Natural gradient descent</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/Simplex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Programming and simplex algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/Autograd.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/exercises/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/matrix_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matrix calculus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convex_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex sets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/separation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Separation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/conjugate_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate sets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convex_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/subgradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient and subdifferential</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/conjugate_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/gop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General optimization problems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/duality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Duality</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rates of convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/line_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Line search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/cvxpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CVXPY library</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/automatic_differentiation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/zom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Zero order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/fom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">First order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/uncategorized.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uncategorized</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/applications/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/A-Star.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="math inline">A^*</span> algorithm for path finding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/deep_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/ellipsoid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Minimum volume ellipsoid</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/knapsack_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knapsack problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear least squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/MLE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Maximum likelihood estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/Neural_Lipschitz_constant.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural network Lipschitz constant</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/NN_Loss_Surface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Network Loss Surface Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principal component analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/rendezvous.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rendezvous problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/salesman_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Travelling salesman problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/total_variation_inpainting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Total variation in-painting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/two_way_partitioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Two way partitioning problem</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/benchmarks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/benchmarks/CNN_on_Fashion_MNIST.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNN on FashionMNIST</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/benchmarks/linear_least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Least Squares</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/materials/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/materials/tutorials/Colab tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick start to the Colab</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#problem" id="toc-problem" class="nav-link active" data-scroll-target="#problem"><span class="header-section-number">1</span> Problem</a></li>
  <li><a href="#finite-differences" id="toc-finite-differences" class="nav-link" data-scroll-target="#finite-differences"><span class="header-section-number">2</span> Finite differences</a></li>
  <li><a href="#forward-mode-automatic-differentiation" id="toc-forward-mode-automatic-differentiation" class="nav-link" data-scroll-target="#forward-mode-automatic-differentiation"><span class="header-section-number">3</span> Forward mode automatic differentiation</a></li>
  <li><a href="#backward-mode-automatic-differentiation" id="toc-backward-mode-automatic-differentiation" class="nav-link" data-scroll-target="#backward-mode-automatic-differentiation"><span class="header-section-number">4</span> Backward mode automatic differentiation</a>
  <ul class="collapse">
  <li><a href="#what-automatic-differentiation-ad-is-not" id="toc-what-automatic-differentiation-ad-is-not" class="nav-link" data-scroll-target="#what-automatic-differentiation-ad-is-not"><span class="header-section-number">4.1</span> What automatic differentiation (AD) is NOT:</a></li>
  </ul></li>
  <li><a href="#important-stories-from-matrix-calculus" id="toc-important-stories-from-matrix-calculus" class="nav-link" data-scroll-target="#important-stories-from-matrix-calculus"><span class="header-section-number">5</span> Important stories from matrix calculus</a>
  <ul class="collapse">
  <li><a href="#univariate-chain-rule" id="toc-univariate-chain-rule" class="nav-link" data-scroll-target="#univariate-chain-rule"><span class="header-section-number">5.1</span> Univariate chain rule</a></li>
  <li><a href="#multivariate-chain-rule" id="toc-multivariate-chain-rule" class="nav-link" data-scroll-target="#multivariate-chain-rule"><span class="header-section-number">5.2</span> Multivariate chain rule</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="header-section-number">5.3</span> Backpropagation</a></li>
  <li><a href="#jacobian-vector-product" id="toc-jacobian-vector-product" class="nav-link" data-scroll-target="#jacobian-vector-product"><span class="header-section-number">5.4</span> Jacobian vector product</a></li>
  <li><a href="#hessian-vector-product" id="toc-hessian-vector-product" class="nav-link" data-scroll-target="#hessian-vector-product"><span class="header-section-number">5.5</span> Hessian vector product</a></li>
  </ul></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code"><span class="header-section-number">6</span> Code</a></li>
  <li><a href="#materials" id="toc-materials" class="nav-link" data-scroll-target="#materials"><span class="header-section-number">7</span> Materials</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MerkulovDaniil/optim/edit/master/docs/methods/Autograd.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button type="button" name="button" class="btn" id="toggleSpoilers">
ðŸ”½
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../docs/methods/index.html">Methods</a></li><li class="breadcrumb-item"><a href="../../docs/methods/Autograd.html">Automatic differentiation</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Automatic differentiation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="problem" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="problem"><span class="header-section-number">1</span> Problem</h2>
<p>Suppose we need to solve the following problem:</p>
<p><span class="math display">
L(w) \to \min_{w \in \mathbb{R}^d}
</span></p>
<p>Such problems typically arise in machine learning, when you need to find optimal hyperparameters <span class="math inline">w</span> of an ML model (i.e.&nbsp;train a neural network). You may use a lot of algorithms to approach this problem, but given the modern size of the problem, where <span class="math inline">d</span> could be dozens of billions it is very challenging to solve this problem without information about the gradients using zero-order optimization algorithms. That is why it would be beneficial to be able to calculate the gradient vector <span class="math inline">\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \ldots, \frac{\partial L}{\partial w_d}\right)^T</span>. Typically, first-order methods perform much better in huge-scale optimization, while second-order methods require too much memory.</p>
</section>
<section id="finite-differences" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="finite-differences"><span class="header-section-number">2</span> Finite differences</h2>
<p>The naive approach to get approximate values of gradients is <strong>Finite differences</strong> approach. For each coordinate, one can calculate the partial derivative approximation:</p>
<p><span class="math display">
\dfrac{\partial L}{\partial w_k} (w) \approx \dfrac{L(w+\varepsilon e_k) - L(w)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
</span></p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>If the time needed for one calculation of <span class="math inline">L(w)</span> is <span class="math inline">T</span>, what is the time needed for calculating <span class="math inline">\nabla_w L</span> with this approach?</p>
<div class="callout callout-style-default callout-answer no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-answer" data-collapse="true">
<p><span class="math inline">2dT</span>, which is extremely long for the huge scale optimization. Moreover, this exact scheme is unstable, which means that you will have to choose between accuracy and stability.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-theorem no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-theorem">
<p>There is an algorithm to compute <span class="math inline">\nabla_w L</span> in <span class="math inline">\mathcal{O}(T)</span> operations. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</div>
</div>
</div>
</div>
</section>
<section id="forward-mode-automatic-differentiation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="forward-mode-automatic-differentiation"><span class="header-section-number">3</span> Forward mode automatic differentiation</h2>
<p>To dive deep into the idea of automatic differentiation we will consider a simple function for calculating derivatives:</p>
<p><span class="math display">
L(w_1, w_2) = w_2 \log w_1 + \sqrt{w_2 \log w_1}
</span></p>
<p>Letâ€™s draw a <em>computational graph</em> of this function:</p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="comp_graph.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of computation graph of primitive arithmetic operations for the function <span class="math inline">L(w_1, w_2)</span></figcaption>
</figure>
</div>
<p>Letâ€™s go from the beginning of the graph to the end and calculate the derivative <span class="math inline">\dfrac{\partial L}{\partial w_1}</span>:</p>
<table class="table">
<colgroup>
<col style="width: 1%">
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Derivative</th>
<th style="text-align: center;">Scheme</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;"><span class="math inline">w_1 = w_1, w_2 = w_2</span></td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial w_1}{\partial w_1} = 1, \dfrac{\partial w_2}{\partial w_1} = 0</span></td>
<td style="text-align: center;"><img src="comp_graph_1.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;"><span class="math inline">v_1 = \log w_1</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_1}{\partial w_1} &amp;= \frac{\partial v_1}{\partial w_1} \frac{\partial w_1}{\partial w_1}\\ &amp;= \frac{1}{w_1} 1\end{aligned}</span></td>
<td style="text-align: center;"><img src="comp_graph_2.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;"><span class="math inline">v_2 = w_2 v_1</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_2}{\partial w_1} &amp;= \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_1} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_1} \\&amp;= w_2\frac{\partial v_1}{\partial w_1} + v_1\frac{\partial w_2}{\partial w_1}\end{aligned}</span></td>
<td style="text-align: center;"><img src="comp_graph_3.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;"><span class="math inline">v_3 = \sqrt{v_2}</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_3}{\partial w_1} &amp;= \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_1} \\ &amp;= \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_1}\end{aligned}</span></td>
<td style="text-align: center;"><img src="comp_graph_4.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;"><span class="math inline">L = v_2 + v_3</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial w_1} &amp;= \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_1} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_1} \\&amp;= 1\frac{\partial v_2}{\partial w_1} + 1\frac{\partial v_3}{\partial w_1}\end{aligned}</span></td>
<td style="text-align: center;"><img src="comp_graph_5.svg" class="img-fluid"></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Make the same computations for <span class="math inline">\dfrac{\partial L}{\partial w_2}</span></p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<table class="table">
<colgroup>
<col style="width: 1%">
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Derivative</th>
<th style="text-align: center;">Scheme</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;"><span class="math inline">w_1 = w_1, w_2 = w_2</span></td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial w_1}{\partial w_2} = 0, \dfrac{\partial w_2}{\partial w_2} = 1</span></td>
<td style="text-align: center;"><img src="cgraph_ex_1.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;"><span class="math inline">v_1 = \log w_1</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_1}{\partial w_2} &amp;= \frac{\partial v_1}{\partial w_2} \frac{\partial w_2}{\partial w_2}\\ &amp;= 0 \cdot 1\end{aligned}</span></td>
<td style="text-align: center;"><img src="cgraph_ex_2.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;"><span class="math inline">v_2 = w_2 v_1</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_2}{\partial w_2} &amp;= \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_2} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_2} \\&amp;= w_2\frac{\partial v_1}{\partial w_2} + v_1\frac{\partial w_2}{\partial w_2}\end{aligned}</span></td>
<td style="text-align: center;"><img src="cgraph_ex_3.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;"><span class="math inline">v_3 = \sqrt{v_2}</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial v_3}{\partial w_2} &amp;= \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_2} \\ &amp;= \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_2}\end{aligned}</span></td>
<td style="text-align: center;"><img src="cgraph_ex_4.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;"><span class="math inline">L = v_2 + v_3</span></td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial w_2} &amp;= \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_2} \\&amp;= 1\frac{\partial v_2}{\partial w_2} + 1\frac{\partial v_3}{\partial w_2}\end{aligned}</span></td>
<td style="text-align: center;"><img src="cgraph_ex_5.svg" class="img-fluid"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-theorem no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Forward mode automatic differentiation algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-theorem">
<p>Suppose, we have a computational graph <span class="math inline">v_i, i \in [1; N]</span>. Our goal is to calculate the derivative of the output of this graph with respect to some input variable <span class="math inline">w_k</span>, i.e.&nbsp;<span class="math inline">\dfrac{\partial v_N}{\partial w_k}</span>. This idea implies propagation of the gradient with respect to the input variable from start to end, that is why we can introduce the notation:</p>
<p><span class="math display">
\overline{v_i} = \dfrac{\partial v_i}{\partial w_k}
</span></p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="auto_diff_forward.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of forward chain rule to calculate the derivative of the function <span class="math inline">L</span> with respect to <span class="math inline">w_k</span>.</figcaption>
</figure>
</div>
<ul>
<li>For <span class="math inline">i = 1, \ldots, N</span>:
<ul>
<li>Compute <span class="math inline">v_i</span> as a function of its parents (inputs) <span class="math inline">x_1, \ldots, x_{t_i}</span>: <span class="math display">
  v_i = v_i(x_1, \ldots, x_{t_i})
  </span></li>
<li>Compute the derivative <span class="math inline">\overline{v_i}</span> using the forward chain rule: <span class="math display">
  \overline{v_i} = \sum_{j = 1}^{t_i}\dfrac{\partial v_i}{\partial x_j}\dfrac{\partial x_j}{\partial w_k}
  </span></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<p>Note, that this approach does not require storing all intermediate computations, but one can see, that for calculating the derivative <span class="math inline">\dfrac{\partial L}{\partial w_k}</span> we need <span class="math inline">\mathcal{O}(T)</span> operations. This means, that for the whole gradient, we need <span class="math inline">d\mathcal{O}(T)</span> operations, which is the same as for finite differences, but we do not have stability issues, or inaccuracies now (the formulas above are exact).</p>
</section>
<section id="backward-mode-automatic-differentiation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="backward-mode-automatic-differentiation"><span class="header-section-number">4</span> Backward mode automatic differentiation</h2>
<p>We will consider the same function</p>
<p><span class="math display">
L(w_1, w_2) = w_2 \log w_1 + \sqrt{w_2 \log w_1}
</span></p>
<p>with a computational graph:</p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="comp_graph.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of computation graph of primitive arithmetic operations for the function <span class="math inline">L(w_1, w_2)</span></figcaption>
</figure>
</div>
<p>Assume, that we have some values of the parameters <span class="math inline">w_1, w_2</span> and we have already performed a forward pass (i.e.&nbsp;single propagation through the computational graph from left to right). Suppose, also, that we somehow saved all intermediate values of <span class="math inline">v_i</span>. Letâ€™s go from the end of the graph to the beginning and calculate the derivatives <span class="math inline">\dfrac{\partial L}{\partial w_1}, \dfrac{\partial L}{\partial w_1}</span>:</p>
<table class="table">
<colgroup>
<col style="width: 1%">
<col style="width: 20%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Derivative</th>
<th style="text-align: center;">Scheme</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial L}{\partial L} = 1</span></td>
<td style="text-align: center;"><img src="revad_1.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial v_3} &amp;= \frac{\partial L}{\partial L} \frac{\partial L}{\partial v_3}\\ &amp;= \frac{\partial L}{\partial L} 1\end{aligned}</span></td>
<td style="text-align: center;"><img src="revad_2.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial v_2} &amp;= \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial v_2} + \frac{\partial L}{\partial L}\frac{\partial L}{\partial v_2} \\&amp;= \frac{\partial L}{\partial v_3}\frac{1}{2\sqrt{v_2}} + \frac{\partial L}{\partial L}1\end{aligned}</span></td>
<td style="text-align: center;"><img src="revad_3.svg" class="img-fluid"></td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial v_1} &amp;=\frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial v_1} \\ &amp;= \frac{\partial L}{\partial v_2}w_2\end{aligned}</span></td>
<td style="text-align: center;"><img src="revad_4.svg" class="img-fluid"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;"><span class="math inline">\begin{aligned}\frac{\partial L}{\partial w_1} &amp;= \frac{\partial L}{\partial v_1}\frac{\partial v_1}{\partial w_1} \\&amp;= \frac{\partial L}{\partial v_1}\frac{1}{w_1}\end{aligned}</span> <span class="math inline">\begin{aligned}\frac{\partial L}{\partial w_2} &amp;= \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} \\&amp;= \frac{\partial L}{\partial v_1}v_1\end{aligned}</span></td>
<td style="text-align: center;"><img src="revad_5.svg" class="img-fluid"></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Note, that for the same price of computations as it was in the forward mode we have the full vector of gradient <span class="math inline">\nabla_w L</span>. Is it a free lunch? What is the cost of acceleration?</p>
<div class="callout callout-style-default callout-answer no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-answer" data-collapse="true">
<p>Note, that for using the reverse mode AD you need to store all intermediate computations from the forward pass. This problem could be somehow mitigated with the gradient checkpointing approach, which involves necessary recomputations of some intermediate values. This could significantly reduce the memory footprint of the large machine-learning model.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-theorem no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reverse mode automatic differentiation algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-theorem">
<p>Suppose, we have a computational graph <span class="math inline">v_i, i \in [1; N]</span>. Our goal is to calculate the derivative of the output of this graph with respect to all inputs variable <span class="math inline">w</span>, i.e.&nbsp;<span class="math inline">\nabla_w v_N = \left( \frac{\partial v_N}{\partial w_1}, \ldots, \frac{\partial v_N}{\partial w_d}\right)^T</span>. This idea implies propagation of the gradient of the function with respect to the intermediate variables from the end to the origin, that is why we can introduce the notation:</p>
<p><span class="math display">
\overline{v_i}  = \dfrac{\partial L}{\partial v_i} = \dfrac{\partial v_N}{\partial v_i}
</span></p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="auto_diff_reverse.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of reverse chain rule to calculate the derivative of the function <span class="math inline">L</span> with respect to the node <span class="math inline">v_i</span>.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>FORWARD PASS</strong></p>
<p>For <span class="math inline">i = 1, \ldots, N</span>:</p>
<ul>
<li>Compute and store the values of <span class="math inline">v_i</span> as a function of its parents (inputs)</li>
</ul></li>
<li><p><strong>BACKWARD PASS</strong></p>
<p>For <span class="math inline">i = N, \ldots, 1</span>:</p>
<ul>
<li>Compute the derivative <span class="math inline">\overline{v_i}</span> using the backward chain rule and information from all of its children (outputs) (<span class="math inline">x_1, \ldots, x_{t_i}</span>): <span class="math display">
  \overline{v_i} = \dfrac{\partial L}{\partial v_i} = \sum_{j = 1}^{t_i} \dfrac{\partial L}{\partial x_j} \dfrac{\partial x_j}{\partial v_i}
  </span></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Which of the AD modes would you choose (forward/ reverse) for the following computational graph of primitive arithmetic operations? Suppose, you are needed to compute the jacobian <span class="math inline">J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}</span></p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ad_choose.svg" class="img-fluid figure-img"></p>
<figcaption>Which mode would you choose for calculating gradients there?</figcaption>
</figure>
</div>
<p>Note, that the reverse mode computational time is proportional to the number of outputs here, while the forward mode works proportionally to the number of inputs there. This is why it would be a good idea to consider the forward mode AD.</p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="forward_vs_reverse_ad.svg" class="img-fluid figure-img"></p>
<figcaption>This graph nicely illustrates the idea of choice between the modes. The <span class="math inline">n = 100</span> dimension is fixed and the graph presents the time needed for Jacobian calculation w.r.t. <span class="math inline">x</span> for <span class="math inline">f(x) = Ax</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Which of the AD modes would you choose (forward/ reverse) for the following computational graph of primitive arithmetic operations? Suppose, you are needed to compute the jacobian <span class="math inline">J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}</span>. Note, that <span class="math inline">G</span> is an arbitrary computational graph</p>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ad_mixed.svg" class="img-fluid figure-img"></p>
<figcaption>Which mode would you choose for calculating gradients there?</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-answer no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-answer" data-collapse="true">
<p>It is generally impossible to say it without some knowledge about the specific structure of the graph <span class="math inline">G</span>. Note, that there are also plenty of advanced approaches to mix forward and reverse mode AD, based on the specific <span class="math inline">G</span> structure.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforward Architecture
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="feedforward.svg" class="img-fluid figure-img"></p>
<figcaption>Feedforward neural network architecture</figcaption>
</figure>
</div>
<p><strong>FORWARD</strong></p>
<ul>
<li><p><span class="math inline">v_0 = x</span> typically we have a batch of data <span class="math inline">x</span> here as an input.</p></li>
<li><p>For <span class="math inline">k = 1, \ldots, t-1, t</span>:</p>
<ul>
<li><span class="math inline">v_k = \sigma(v_{k-1}w_k)</span>. Note, that practically speaking the data has dimension <span class="math inline">x \in \mathbb{R}^{b \times d}</span>, where <span class="math inline">b</span> is the batch size (for the single data point <span class="math inline">b=1</span>). While the weight matrix <span class="math inline">w_k</span> of a <span class="math inline">k</span> layer has a shape <span class="math inline">n_{k-1} \times n_k</span>, where <span class="math inline">n_k</span> is the dimension of an inner representation of the data.</li>
</ul></li>
<li><p><span class="math inline">L = L(v_t)</span> - calculate the loss function.</p></li>
</ul>
<p><strong>BACKWARD</strong></p>
<ul>
<li><p><span class="math inline">v_{t+1} = L, \dfrac{\partial L}{\partial L} = 1</span></p></li>
<li><p>For <span class="math inline">k = t, t-1, \ldots, 1</span>:</p>
<ul>
<li><span class="math inline">\underset{b \times n_k}{\dfrac{\partial L}{\partial v_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \underset{n_{k+1} \times n_k}{\dfrac{\partial v_{k+1}}{\partial v_{k}}}</span></li>
<li><span class="math inline">\underset{b \times n_{k-1} \cdot n_k}{\dfrac{\partial L}{\partial w_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \cdot \underset{n_{k+1} \times n_{k-1} \cdot n_k}{\dfrac{\partial v_{k+1}}{\partial w_{k}}}</span></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient propagation through the linear least squares
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linear_least_squares_layer.svg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">x</span> could be found as a solution of linear system</figcaption>
</figure>
</div>
<p>Suppose, we have an invertible matrix <span class="math inline">A</span> and a vector <span class="math inline">b</span>, the vector <span class="math inline">x</span> is the solution of the linear system <span class="math inline">Ax = b</span>, namely one can write down an analytical solution <span class="math inline">x = A^{-1}b</span>, in this example we will show, that computing all derivatives <span class="math inline">\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}, \dfrac{\partial L}{\partial x}</span>, i.e.&nbsp;the backward pass, costs approximately the same as the forward pass.</p>
<p>It is known, that the differential of the function does not depend on the parametrization:</p>
<p><span class="math display">
dL = \left\langle\dfrac{\partial L}{\partial x}, dx \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
</span></p>
<p>Given the linear system, we have:</p>
<p><span class="math display">
\begin{align*}
Ax &amp;= b \\
dAx + Adx = db &amp;\to dx = A^{-1}(db - dAx)
\end{align*}
</span></p>
<p>The straightforward substitution gives us:</p>
<p><span class="math display">
\left\langle\dfrac{\partial L}{\partial x}, A^{-1}(db - dAx) \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
</span></p>
<p><span class="math display">
\left\langle -A^{-T}\dfrac{\partial L}{\partial x} x^T, dA \right\rangle + \left\langle A^{-T}\dfrac{\partial L}{\partial x},db \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
</span></p>
<p>Therefore:</p>
<p><span class="math display">
\dfrac{\partial L}{\partial A} = -A^{-T}\dfrac{\partial L}{\partial x} x^T \quad \dfrac{\partial L}{\partial b} =  A^{-T}\dfrac{\partial L}{\partial x}
</span></p>
<p>It is interesting, that the most computationally intensive part here is the matrix inverse, which is the same as for the forward pass. Sometimes it is even possible to store the result itself, which makes the backward pass even cheaper.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient propagation through the SVD
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Suppose, we have the rectangular matrix <span class="math inline">W \in \mathbb{R}^{m \times n}</span>, which has a singular value decomposition:</p>
<p><span class="math display">
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
</span></p>
<ol type="1">
<li><p>Similarly to the previous example:</p>
<p><span class="math display">
\begin{split}
W &amp;= U \Sigma V^T \\
dW &amp;= dU \Sigma V^T + U d\Sigma V^T + U \Sigma dV^T \\
U^T dW V &amp;= U^TdU \Sigma V^TV + U^TU d\Sigma V^TV + U^TU \Sigma dV^TV \\
U^T dW V &amp;= U^TdU \Sigma + d\Sigma + \Sigma dV^TV
\end{split}
</span></p></li>
<li><p>Note, that <span class="math inline">U^T U = I \to dU^TU + U^T dU = 0</span>. But also <span class="math inline">dU^TU = (U^T dU)^T</span>, which actually involves, that the matrix <span class="math inline">U^TdU</span> is antisymmetric:</p>
<p><span class="math display">
(U^T dU)^T +  U^T dU = 0 \quad \to \quad \text{diag}( U^T dU) = (0, \ldots, 0)
</span></p>
<p>The same logic could be applied to the matrix <span class="math inline">V</span> and</p>
<p><span class="math display">
\text{diag}(dV^T V) = (0, \ldots, 0)
</span></p></li>
<li><p>At the same time, the matrix <span class="math inline">d \Sigma</span> is diagonal, which means (look at the 1.) that</p>
<p><span class="math display">
\text{diag}(U^T dW V) = d \Sigma
</span></p>
<p>Here on both sides, we have diagonal matrices.</p></li>
<li><p>Now, we can decompose the differential of the loss function as a function of <span class="math inline">\Sigma</span> - such problems arise in ML problems, where we need to restrict the matrix rank:</p>
<p><span class="math display">
\begin{split}
dL &amp;= \left\langle\dfrac{\partial L}{\partial \Sigma}, d\Sigma \right\rangle \\
&amp;= \left\langle\dfrac{\partial L}{\partial \Sigma}, \text{diag}(U^T dW V)\right\rangle \\
&amp;= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right)
\end{split}
</span></p>
<p>As soon as we have diagonal matrices inside the product, the trace of the diagonal part of the matrix will be equal to the trace of the whole matrix:</p>
<p><span class="math display">
\begin{split}
dL &amp;= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right) \\
&amp;= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T U^T dW V \right)  \\
&amp;= \left\langle\dfrac{\partial L}{\partial \Sigma}, U^T dW V \right\rangle \\
&amp;= \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle
\end{split}
</span></p></li>
<li><p>Finally, using another parametrization of the differential</p>
<p><span class="math display">
\left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle = \left\langle\dfrac{\partial L}{\partial W}, dW \right\rangle
</span></p>
<p><span class="math display">
\dfrac{\partial L}{\partial W} =  U \dfrac{\partial L}{\partial \Sigma} V^T,
</span></p>
<p>This nice result allows us to connect the gradients <span class="math inline">\dfrac{\partial L}{\partial W}</span> and <span class="math inline">\dfrac{\partial L}{\partial \Sigma}</span>.</p></li>
</ol>
</div>
</div>
</div>
</div>
<section id="what-automatic-differentiation-ad-is-not" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="what-automatic-differentiation-ad-is-not"><span class="header-section-number">4.1</span> What automatic differentiation (AD) is NOT:</h3>
<ul>
<li>AD is not a finite differences</li>
<li>AD is not a symbolic derivative</li>
<li>AD is not just the chain rule</li>
<li>AD is not just backpropagation</li>
<li>AD (reverse mode) is time-efficient and numerically stable</li>
<li>AD (reverse mode) is memory inefficient (you need to store all intermediate computations from the forward pass). :::</li>
</ul>
<div class="img-fluid quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="differentiation_scheme.svg" class="img-fluid figure-img"></p>
<figcaption>Different approaches for taking derivatives</figcaption>
</figure>
</div>
</section>
</section>
<section id="important-stories-from-matrix-calculus" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="important-stories-from-matrix-calculus"><span class="header-section-number">5</span> Important stories from matrix calculus</h2>
<p>We will illustrate some important matrix calculus facts for specific cases</p>
<section id="univariate-chain-rule" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="univariate-chain-rule"><span class="header-section-number">5.1</span> Univariate chain rule</h3>
<p>Suppose, we have the following functions <span class="math inline">R: \mathbb{R} \to \mathbb{R} , L: \mathbb{R} \to \mathbb{R}</span> and <span class="math inline">W \in \mathbb{R}</span>. Then</p>
<p><span class="math display">
\dfrac{\partial R}{\partial W} = \dfrac{\partial R}{\partial L} \dfrac{\partial L}{\partial W}
</span></p>
</section>
<section id="multivariate-chain-rule" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="multivariate-chain-rule"><span class="header-section-number">5.2</span> Multivariate chain rule</h3>
<p>The simplest example:</p>
<p><span class="math display">
\dfrac{\partial }{\partial t} f(x_1(t), x_2(t)) = \dfrac{\partial f}{\partial x_1} \dfrac{\partial x_1}{\partial t} + \dfrac{\partial f}{\partial x_2} \dfrac{\partial x_2}{\partial t}
</span></p>
<p>Now, weâ€™ll consider <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}</span>:</p>
<p><span class="math display">
\dfrac{\partial }{\partial t} f(x_1(t), \ldots, x_n(t)) = \dfrac{\partial f}{\partial x_1} \dfrac{\partial x_1}{\partial t} + \ldots + \dfrac{\partial f}{\partial x_n} \dfrac{\partial x_n}{\partial t}
</span></p>
<p>But if we will add another dimension <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}^m</span>, than the <span class="math inline">j</span>-th output of <span class="math inline">f</span> will be:</p>
<p><span class="math display">
\dfrac{\partial }{\partial t} f_j(x_1(t), \ldots, x_n(t)) = \sum\limits_{i=1}^n \dfrac{\partial f_j}{\partial x_i} \dfrac{\partial x_i}{\partial t} = \sum\limits_{i=1}^n J_{ji}  \dfrac{\partial x_i}{\partial t},
</span></p>
<p>where matrix <span class="math inline">J \in \mathbb{R}^{m \times n}</span> is the jacobian of the <span class="math inline">f</span>. Hence, we could write it in a vector way:</p>
<p><span class="math display">
\dfrac{\partial f}{\partial t} = J \dfrac{\partial x}{\partial t}\quad \iff \quad \left(\dfrac{\partial f}{\partial t}\right)^\top =  \left( \dfrac{\partial x}{\partial t}\right)^\top J^\top
</span></p>
</section>
<section id="backpropagation" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">5.3</span> Backpropagation</h3>
<p>Backpropagation is a specific application of reverse-mode automatic differentiation within neural networks. It is the standard algorithm for computing gradients in neural networks, especially for training with stochastic gradient descent. Hereâ€™s how it works:</p>
<ul>
<li>Perform a forward pass through the network to compute activations and outputs.</li>
<li>Calculate the loss function at the output, which measures the difference between the network prediction and the actual target values.</li>
<li>Commence the backward pass by computing the gradient of the loss with respect to the networkâ€™s outputs.</li>
<li>Propagate these gradients back through the network, layer by layer, using the chain rule to calculate the gradients of the loss with respect to each weight and bias.</li>
<li>The critical point of backpropagation is that it efficiently calculates the gradient of a complex, multilayered function by decomposing it into simpler derivative calculations. This aspect makes the update of a large number of parameters in deep networks computationally feasible.</li>
</ul>
</section>
<section id="jacobian-vector-product" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="jacobian-vector-product"><span class="header-section-number">5.4</span> Jacobian vector product</h3>
<p>The power of automatic differentiation is encapsulated in the computation of the Jacobian-vector product. Instead of calculating the entire Jacobian matrix, which is computationally expensive and often unnecessary, AD computes the product of the Jacobian and a vector directly. This is crucial for gradients in neural networks where the Jacobian may be very large, but the end goal is the product of this Jacobian with the gradient of the loss with respect to the outputs (vector). The reason why it works so fast in practice is that the Jacobian of the operations is already developed effectively in automatic differentiation frameworks. Typically, we even do not construct or store the full Jacobian, doing matvec directly instead. Note, for some functions (for example, any element-wise function of the input vector) matvec costs linear time, instead of quadratic and requires no additional memory to store a Jacobian.</p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: element-wise exponent
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p><span class="math display">
y = \exp{(z)} \qquad J = \text{diag}(\exp(z)) \qquad \overline{z} = \overline{y} J
</span></p>
</div>
</div>
</div>
</div>
<p>See the examples of Vector-Jacobian Products from the autodidact library:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>defvjp(anp.add,         <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, g),</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, g))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>defvjp(anp.multiply,    <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, y <span class="op">*</span> g),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, x <span class="op">*</span> g))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>defvjp(anp.subtract,    <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, g),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, <span class="op">-</span>g))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>defvjp(anp.divide,      <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x,   g <span class="op">/</span> y),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, <span class="op">-</span> g <span class="op">*</span> x <span class="op">/</span> y<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>defvjp(anp.true_divide, <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x,   g <span class="op">/</span> y),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, <span class="op">-</span> g <span class="op">*</span> x <span class="op">/</span> y<span class="op">**</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="hessian-vector-product" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="hessian-vector-product"><span class="header-section-number">5.5</span> Hessian vector product</h3>
<p>Interestingly, a similar idea could be used to compute Hessian-vector products, which is essential for second-order optimization or conjugate gradient methods. For a scalar-valued function <span class="math inline">f : \mathbb{R}^n \to \mathbb{R}</span> with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point <span class="math inline">x \in \mathbb{R}^n</span> is written as <span class="math inline">\partial^2 f(x)</span>. A Hessian-vector product function is then able to evaluate</p>
<p><span class="math display">
v \mapsto \partial^2 f(x) \cdot v
</span></p>
<p>for any vector <span class="math inline">v \in \mathbb{R}^n</span>.</p>
<p>The trick is not to instantiate the full Hessian matrix: if <span class="math inline">n</span> is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store. Luckily, <code>grad</code> (in the jax/autograd/pytorch/tensorflow) already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity</p>
<p><span class="math display">
\partial^2 f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial g(x),
</span></p>
<p>where <span class="math inline">g(x) = \partial f(x) \cdot v</span> is a new vector-valued function that dots the gradient of <span class="math inline">f</span> at <span class="math inline">x</span> with the vector <span class="math inline">v</span>. Notice that weâ€™re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know <code>grad</code> is efficient.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hvp(f, x, v):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grad(<span class="kw">lambda</span> x: jnp.vdot(grad(f)(x), v))(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="code" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="code"><span class="header-section-number">6</span> Code</h2>
<p><a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb">Open In Colab</a></p>
</section>
<section id="materials" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="materials"><span class="header-section-number">7</span> Materials</h2>
<ul>
<li><a href="https://github.com/mattjj/autodidact">Autodidact</a> - a pedagogical implementation of Autograd</li>
<li><a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec06.pdf">CSC321</a> Lecture 6</li>
<li><a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">CSC321</a> Lecture 10</li>
<li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Why</a> you should understand backpropagation :)</li>
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">JAX autodiff cookbook</a></li>
<li><a href="https://harvard-iacs.github.io/2019-CS207/lectures/lecture10/notebook/">Materials</a> from CS207: Systems Development for Computational Science course with very intuitive explanation.</li>
<li><a href="https://www.youtube.com/watch?v=za2LgI8JFCw">Great lecture on AD</a> from Dmitry Kropotov (in Russian).</li>
</ul>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Masterâ€™s Thesis (in Finnish), Univ. Helsinki, 1970.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/fmin\.xyz");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MerkulovDaniil/optim/edit/master/docs/methods/Autograd.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>