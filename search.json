[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods.\n\n\n\nDive into the wealth of resources available:\n\nTheoryMethodsExercisesApplicationsBenchmarks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\nConvex sets\n\n\n\n\nProjection\n\n\n\n\nSeparation\n\n\n\n\nConjugate sets\n\n\n\n\nConvex functions\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\nConjugate functions\n\n\n\n\nGeneral optimization problems\n\n\n\n\nDuality\n\n\n\n\nRates of convergence\n\n\n\n\nLine search\n\n\n\n\nCVXPY library\n\n\n\n\nAutomatic differentiation\n\n\n\n\nZero order methods\n\n\n\n\nFirst order methods\n\n\n\n\nUncategorized\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\nOpen Education for All: At 💎fmin.xyz, we steadfastly believe in dismantling barriers to education. All content on this site is open-sourced and freely accessible to everyone, echoing our commitment to inclusive learning.\nDesigned for Learners at Various Levels: Whether you are a researcher, a student, or an enthusiast with some prior expertise, 💎fmin.xyz is tailored to cater to your needs, providing a vast array of content ranging from fundamental theories to advanced applications.\nStructured Learning: Every page on 💎fmin.xyz is meticulously structured as a self-sufficient unit, providing concise and comprehensive material on a specific topic. This modular design allows for flexible and personalized learning paths.\nContinuous Improvement: The structure and content of 💎fmin.xyz are dynamic. We are devoted to refining and expanding our resources iteratively, adapting to the evolving needs of our learning community and the advancements in the field.\n\n\n\n\n\nRich Content: Delve into a wide spectrum of topics from matrix calculus, convex sets, to various optimization methods and their applications. With a plethora of exercises and benchmarks, 💎fmin.xyz is a treasure trove of knowledge waiting to be explored.\nInteractive Visualizations: Engage with our extensive collection of Python visualizations designed to elucidate key mathematical concepts, enhancing your understanding and learning experience.\nEfficient Search Functionality: With a user-friendly built-in search engine, finding specific content on 💎fmin.xyz is just a click away. Navigate through our extensive repository with ease and efficiency.\n\n\n\n\n\nEasy to Use & Share: Each page is formatted as a Markdown file or Jupyter notebook, facilitating easy copying, sharing, and utilization for your purposes.\nBuilt with Quarto: 💎fmin.xyz is powered by the Quarto system, ensuring a smooth and responsive user experience.\nContribute & Collaborate: Your insights are valuable! Contributions are not just welcomed but encouraged. Check out our guides at the repository and join us in refining and expanding this educational endeavor.\n\nEmbark on a learning adventure with 💎fmin.xyz, where knowledge is open, and education is accessible. Happy learning!"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Dive into the wealth of resources available:\n\nTheoryMethodsExercisesApplicationsBenchmarks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\nConvex sets\n\n\n\n\nProjection\n\n\n\n\nSeparation\n\n\n\n\nConjugate sets\n\n\n\n\nConvex functions\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\nConjugate functions\n\n\n\n\nGeneral optimization problems\n\n\n\n\nDuality\n\n\n\n\nRates of convergence\n\n\n\n\nLine search\n\n\n\n\nCVXPY library\n\n\n\n\nAutomatic differentiation\n\n\n\n\nZero order methods\n\n\n\n\nFirst order methods\n\n\n\n\nUncategorized\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#our-principles",
    "href": "index.html#our-principles",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Open Education for All: At 💎fmin.xyz, we steadfastly believe in dismantling barriers to education. All content on this site is open-sourced and freely accessible to everyone, echoing our commitment to inclusive learning.\nDesigned for Learners at Various Levels: Whether you are a researcher, a student, or an enthusiast with some prior expertise, 💎fmin.xyz is tailored to cater to your needs, providing a vast array of content ranging from fundamental theories to advanced applications.\nStructured Learning: Every page on 💎fmin.xyz is meticulously structured as a self-sufficient unit, providing concise and comprehensive material on a specific topic. This modular design allows for flexible and personalized learning paths.\nContinuous Improvement: The structure and content of 💎fmin.xyz are dynamic. We are devoted to refining and expanding our resources iteratively, adapting to the evolving needs of our learning community and the advancements in the field."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Rich Content: Delve into a wide spectrum of topics from matrix calculus, convex sets, to various optimization methods and their applications. With a plethora of exercises and benchmarks, 💎fmin.xyz is a treasure trove of knowledge waiting to be explored.\nInteractive Visualizations: Engage with our extensive collection of Python visualizations designed to elucidate key mathematical concepts, enhancing your understanding and learning experience.\nEfficient Search Functionality: With a user-friendly built-in search engine, finding specific content on 💎fmin.xyz is just a click away. Navigate through our extensive repository with ease and efficiency."
  },
  {
    "objectID": "index.html#technical-insights",
    "href": "index.html#technical-insights",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Easy to Use & Share: Each page is formatted as a Markdown file or Jupyter notebook, facilitating easy copying, sharing, and utilization for your purposes.\nBuilt with Quarto: 💎fmin.xyz is powered by the Quarto system, ensuring a smooth and responsive user experience.\nContribute & Collaborate: Your insights are valuable! Contributions are not just welcomed but encouraged. Check out our guides at the repository and join us in refining and expanding this educational endeavor.\n\nEmbark on a learning adventure with 💎fmin.xyz, where knowledge is open, and education is accessible. Happy learning!"
  },
  {
    "objectID": "docs/theory/convex sets/index.html",
    "href": "docs/theory/convex sets/index.html",
    "title": "",
    "section": "",
    "text": "In this chapter, a variety of convex sets and related definitions are described.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Theory",
      "Convex sets"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html",
    "href": "docs/theory/convex sets/Convex_set.html",
    "title": "1 Definitions",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2\n\n\n\n\n\n\nThe set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets.\n\n\n\n\n\n\nLet x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0.\n\n\n\nThe set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets.\n\n\n\n\n\n\nThe Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#line-segment",
    "href": "docs/theory/convex sets/Convex_set.html#line-segment",
    "title": "1 Definitions",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-set",
    "href": "docs/theory/convex sets/Convex_set.html#convex-set",
    "title": "1 Definitions",
    "section": "",
    "text": "The set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-combination",
    "href": "docs/theory/convex sets/Convex_set.html#convex-combination",
    "title": "1 Definitions",
    "section": "",
    "text": "Let x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-hull",
    "href": "docs/theory/convex sets/Convex_set.html#convex-hull",
    "title": "1 Definitions",
    "section": "",
    "text": "The set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#minkowski-addition",
    "href": "docs/theory/convex sets/Convex_set.html#minkowski-addition",
    "title": "1 Definitions",
    "section": "",
    "text": "The Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#by-definition",
    "href": "docs/theory/convex sets/Convex_set.html#by-definition",
    "title": "1 Definitions",
    "section": "2.1 By definition",
    "text": "2.1 By definition\n\nx_1, x_2 \\in S, \\; 0 \\le \\theta \\le 1 \\;\\; \\rightarrow \\;\\; \\theta x_1 + (1-\\theta)x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\Vert \\mathbf{x} - \\mathbf{x}_c \\Vert \\leq r \\}) - is convex.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\Vert x - x_0\\Vert _2 \\leq \\Vert x-y\\Vert _2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\Vert x - a\\Vert _2 \\leq \\theta\\Vert x - b\\Vert _2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#preserving-convexity",
    "href": "docs/theory/convex sets/Convex_set.html#preserving-convexity",
    "title": "1 Definitions",
    "section": "2.2 Preserving convexity",
    "text": "2.2 Preserving convexity\n\n2.2.1 The linear combination of convex sets is convex\nLet there be 2 convex sets S_x, S_y, let the set\n\nS = \\left\\{s \\mid s = c_1 x + c_2 y, \\; x \\in S_x, \\; y \\in S_y, \\; c_1, c_2 \\in \\mathbb{R}\\right\\}\n\nTake two points from S: s_1 = c_1 x_1 + c_2 y_1, s_2 = c_1 x_2 + c_2 y_2 and prove that the segment between them \\theta s_1 + (1 - \\theta)s_2, \\theta \\in [0,1] also belongs to S\n\n\\theta s_1 + (1 - \\theta)s_2\n\n\n\\theta (c_1 x_1 + c_2 y_1) + (1 - \\theta)(c_1 x_2 + c_2 y_2)\n\n\nc_1 (\\theta x_1 + (1 - \\theta)x_2) + c_2 (\\theta y_1 + (1 - \\theta)y_2)\n\n\nc_1 x + c_2 y \\in S\n\n\n\n2.2.2 The intersection of any (!) number of convex sets is convex\nIf the desired intersection is empty or contains one point, the property is proved by definition. Otherwise, take 2 points and a segment between them. These points must lie in all intersecting sets, and since they are all convex, the segment between them lies in all sets and, therefore, in their intersection.\n\n\n2.2.3 The image of the convex set under affine mapping is convex\n\nS \\subseteq \\mathbb{R}^n \\text{ convex}\\;\\; \\rightarrow \\;\\; f(S) = \\left\\{ f(x) \\mid x \\in S \\right\\} \\text{ convex} \\;\\;\\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\nExamples of affine functions: extension, projection, transposition, set of solutions of linear matrix inequality \\left\\{ x \\mid x_1 A_1 + \\ldots + x_m A_m \\preceq B\\right\\}. Here A_i, B \\in \\mathbf{S}^p are symmetric matrices p \\times p.\nNote also that the prototype of the convex set under affine mapping is also convex.\n\nS \\subseteq \\mathbb{R}^m \\text{ convex}\\; \\rightarrow \\; f^{-1}(S) = \\left\\{ x \\in \\mathbb{R}^n \\mid f(x) \\in S \\right\\} \\text{ convex} \\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. \n\nP = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}.\n\nDetermine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\\mathbb{V} x \\ge \\alpha\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html",
    "href": "docs/theory/convex sets/Affine_sets.html",
    "title": "1 Line",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line passing through them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\theta \\in \\mathbb{R}\n\n\n\n\n\n\n\nFigure 1: Illustration of a line between two vectors x_1 and x_2",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#affine-combination",
    "href": "docs/theory/convex sets/Affine_sets.html#affine-combination",
    "title": "1 Line",
    "section": "3.1 Affine combination",
    "text": "3.1 Affine combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called affine combination of x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#affine-hull",
    "href": "docs/theory/convex sets/Affine_sets.html#affine-hull",
    "title": "1 Line",
    "section": "3.2 Affine hull",
    "text": "3.2 Affine hull\nThe set of all affine combinations of points in set S is called the affine hull of S:\n\n\\mathbf{aff}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1\\right\\}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe set \\mathbf{aff}(S) is the smallest affine set containing S.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#interior",
    "href": "docs/theory/convex sets/Affine_sets.html#interior",
    "title": "1 Line",
    "section": "3.3 Interior",
    "text": "3.3 Interior\nThe interior of the set S is defined as the following set:\n\n\\mathbf{int} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\subset S\\}\n\nwhere B(\\mathbf{x}, \\varepsilon) = \\mathbf{x} + \\varepsilon B is the ball centered at point \\mathbf{x} with radius \\varepsilon.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#relative-interior",
    "href": "docs/theory/convex sets/Affine_sets.html#relative-interior",
    "title": "1 Line",
    "section": "3.4 Relative Interior",
    "text": "3.4 Relative Interior\nThe relative interior of the set S is defined as the following set:\n\n\\mathbf{relint} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\cap \\mathbf{aff} (S) \\subseteq S\\}\n\n\n\n\n\n\n\nFigure 2: Difference between interior and relative interior\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny non-empty convex set S \\subseteq \\mathbb{R}^n has a non-empty relative interior \\mathbf{relint}(S).\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nGive an example of a set S \\subseteq \\mathbb{R}^n, which has an empty interior, but at the same time has a non-empty relative interior \\mathbf{relint}(S).",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html",
    "href": "docs/theory/Rates_of_convergence.html",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "In order to compare perfomance of algorithms we need to define a terminology for different types of convergence. Let \\{x_k\\} be a sequence in \\mathbb{R}^n that converges to some point x^*\n\n\nWe can define the linear convergence in a two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and 0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note, that sometimes this type of convergence is also called exponential or geometric.\n\n\n\nThe convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or 0 &lt; C_k &lt; \\infty, C_k \\to 0. Note, that superlinear convergence is also linear convergence (one can even say, that it is linear convergence with q=0).\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and 0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression.\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and 0 &lt; C &lt; \\infty.\n\n\n\nDifference between the convergence speed\n\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#linear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#linear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "We can define the linear convergence in a two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and 0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note, that sometimes this type of convergence is also called exponential or geometric.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#superlinear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#superlinear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "The convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or 0 &lt; C_k &lt; \\infty, C_k \\to 0. Note, that superlinear convergence is also linear convergence (one can even say, that it is linear convergence with q=0).",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#sublinear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#sublinear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and 0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#quadratic-convergence",
    "href": "docs/theory/Rates_of_convergence.html#quadratic-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and 0 &lt; C &lt; \\infty.\n\n\n\nDifference between the convergence speed\n\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#root-test",
    "href": "docs/theory/Rates_of_convergence.html#root-test",
    "title": "1 Speed of convergence",
    "section": "2.1 Root test",
    "text": "2.1 Root test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of non-negative numbers, converging to zero, and let\n\nq = \\lim_{k \\to \\infty} \\sup_k \\; r_k ^{1/k}\n\n\nIf 0 \\leq q \\lt 1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q = 1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case q \\gt 1 is impossible.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#ratio-test",
    "href": "docs/theory/Rates_of_convergence.html#ratio-test",
    "title": "1 Speed of convergence",
    "section": "2.2 Ratio test",
    "text": "2.2 Ratio test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of strictly positive numbers converging to zero. Let\n\nq = \\lim_{k \\to \\infty} \\dfrac{r_{k+1}}{r_k}\n\n\nIf there exists q and 0 \\leq q \\lt 1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q does not exist, but q = \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k} \\lt 1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with a constant not exceeding q.\nIf \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} =1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\gt 1 is impossible.\nIn all other cases (i.e., when \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\lt 1 \\leq \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k}) we cannot claim anything concrete about the convergence rate \\{r_k\\}_{k=m}^\\infty.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html",
    "href": "docs/theory/Matrix_calculus.html",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note, that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?\n\n\n\n\n\n\n\nLet A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\n\n\n\n\nNorm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 4. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\n\n\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad (A.12)\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that \n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A) ,\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\|A\\|_F^2 = \\langle A, A\\rangle = \\text{tr }(A^T A)\nUsing SVD: A = U \\Sigma V^T \\quad A^T = V \\Sigma U^T\n\\|A\\|_F^2 = \\text{tr }(V \\Sigma U^T U \\Sigma V^T) = \\text{tr }(V \\Sigma^2 V^T) = \\text{tr }(V^T V \\Sigma^2) = \\text{tr }(\\Sigma^2) = \\sum\\limits_{1}^q \\sigma_i^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\n\n\n\nFigure 3: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\nOne can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\n\n\n\nFigure 4: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?\n\n\n\n\n\n\n\nThe determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det} AB = (\\text{det} A)(\\text{det} B);\n\\text{det} A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#vectors-and-matrices",
    "href": "docs/theory/Matrix_calculus.html#vectors-and-matrices",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note, that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#matrix-and-vector-product",
    "href": "docs/theory/Matrix_calculus.html#matrix-and-vector-product",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "Let A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#norms-and-scalar-products",
    "href": "docs/theory/Matrix_calculus.html#norms-and-scalar-products",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "Norm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#eigenvalues-eigenvectors-and-the-singular-value-decomposition",
    "href": "docs/theory/Matrix_calculus.html#eigenvalues-eigenvectors-and-the-singular-value-decomposition",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "A scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 4. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\n\n\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad (A.12)\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that \n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A) ,\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\|A\\|_F^2 = \\langle A, A\\rangle = \\text{tr }(A^T A)\nUsing SVD: A = U \\Sigma V^T \\quad A^T = V \\Sigma U^T\n\\|A\\|_F^2 = \\text{tr }(V \\Sigma U^T U \\Sigma V^T) = \\text{tr }(V \\Sigma^2 V^T) = \\text{tr }(V^T V \\Sigma^2) = \\text{tr }(\\Sigma^2) = \\sum\\limits_{1}^q \\sigma_i^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\n\n\n\nFigure 3: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#canonical-tensor-decomposition",
    "href": "docs/theory/Matrix_calculus.html#canonical-tensor-decomposition",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "One can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\n\n\n\nFigure 4: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#determinant-and-trace",
    "href": "docs/theory/Matrix_calculus.html#determinant-and-trace",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "The determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det} AB = (\\text{det} A)(\\text{det} B);\n\\text{det} A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#gradient",
    "href": "docs/theory/Matrix_calculus.html#gradient",
    "title": "1 Basic linear algebra background",
    "section": "2.1 Gradient",
    "text": "2.1 Gradient\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then vector, which contains all first-order partial derivatives:\n\n\\nabla f(x) = \\dfrac{df}{dx} = \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x_1} \\\\\n    \\frac{\\partial f}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial x_n}\n\\end{pmatrix}\n\nnamed gradient of f(x). This vector indicates the direction of the steepest ascent. Thus, vector −\\nabla f(x) means the direction of the steepest descent of the function in the point. Moreover, the gradient vector is always orthogonal to the contour line in the point.\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the gradient is: \n\\nabla f(x, y) =\n\\begin{bmatrix}\n2x \\\\\n2y \\\\\n\\end{bmatrix}\n This gradient points in the direction of the steepest ascent of the function.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the magnitude of the gradient relate to the steepness of the function?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#hessian",
    "href": "docs/theory/Matrix_calculus.html#hessian",
    "title": "1 Basic linear algebra background",
    "section": "2.2 Hessian",
    "text": "2.2 Hessian\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then matrix, containing all the second order partial derivatives:\n\nf''(x) = \\nabla^2 f(x) = \\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix}\n    \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_n \\partial x_n}\n\\end{pmatrix}\n\n\n\n\n\n\n\nSchwartz theorem\n\n\n\n\n\nLet f: \\mathbb{R}^n \\rightarrow \\mathbb{R} be a function. If the mixed partial derivatives \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} and \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} are both continuous on an open set containing a point a, then they are equal at the point a. That is, \n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (a) = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} (a)\n\nGiven the Schwartz theorem, if the mixed partials are continuous on an open set, the Hessian matrix is symmetric. That means the entries above the main diagonal mirror those below the main diagonal:\n\n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} \\quad \\nabla^2 f(x)  =(\\nabla^2 f(x))^T\n\nThis symmetry simplifies computations and analysis involving the Hessian matrix in various applications, particularly in optimization.\n\n\n\n\n\n\n\n\n\n\nSchwartz counterexample\n\n\n\n\n\n\nf(x,y) =\n\\begin{cases}\n    \\frac{xy\\left(x^2 - y^2\\right)}{x^2 + y^2} & \\text{ for } (x,\\, y) \\ne (0,\\, 0),\\\\\n    0 & \\text{ for } (x, y) = (0, 0).\n\\end{cases}\n\n                        \n                                            \nOne can verify, that \\frac{\\partial^2 f}{ \\partial x \\partial y} (0, 0) \\neq \\frac{\\partial^2 f}{ \\partial y \\partial x} (0, 0), although the mixed partial derivatives do exist, and at every other point the symmetry does hold.\n\n\n\n\nIn fact, Hessian could be a tensor in such a way: \\left(f(x): \\mathbb{R}^n \\to \\mathbb{R}^m \\right) is just 3d tensor, every slice is just hessian of corresponding scalar function \\left( H\\left(f_1(x)\\right), H\\left(f_2(x)\\right), \\ldots, H\\left(f_m(x)\\right)\\right).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the Hessian is:\n\nH_f(x, y) = \\begin{bmatrix} 2 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the curvature of the function in different directions.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow can the Hessian matrix be used to determine the concavity or convexity of a function?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#jacobian",
    "href": "docs/theory/Matrix_calculus.html#jacobian",
    "title": "1 Basic linear algebra background",
    "section": "2.3 Jacobian",
    "text": "2.3 Jacobian\nThe extension of the gradient of multidimensional f(x):\\mathbb{R}^n\\to\\mathbb{R}^m is the following matrix:\n\nJ_f = f'(x) = \\dfrac{df}{dx^T} = \\begin{pmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_1} & \\dots  & \\frac{\\partial f_m}{\\partial x_1} \\\\\n    \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_2} & \\dots  & \\frac{\\partial f_m}{\\partial x_2} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_1}{\\partial x_n} & \\frac{\\partial f_2}{\\partial x_n} & \\dots  & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function\n\nf(x, y) = \\begin{bmatrix}\nx + y \\\\\nx - y \\\\\n\\end{bmatrix},\n the Jacobian is: \nJ_f(x, y) = \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the rate of change of the function with respect to its inputs.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the Jacobian matrix relate to the gradient for scalar-valued functions?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nCan we somehow connect those three definitions above (gradient, jacobian, and hessian) using a single correct statement?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#summary",
    "href": "docs/theory/Matrix_calculus.html#summary",
    "title": "1 Basic linear algebra background",
    "section": "2.4 Summary",
    "text": "2.4 Summary\n\nf(x) : X \\to Y; \\qquad \\frac{\\partial f(x)}{\\partial x} \\in G\n\n\n\n\n\n\n\n\n\n\nX\nY\nG\nName\n\n\n\n\n\\mathbb{R}\n\\mathbb{R}\n\\mathbb{R}\nf'(x) (derivative)\n\n\n\\mathbb{R}^n\n\\mathbb{R}\n\\mathbb{R^n}\n\\dfrac{\\partial f}{\\partial x_i} (gradient)\n\n\n\\mathbb{R}^n\n\\mathbb{R}^m\n\\mathbb{R}^{n \\times m}\n\\dfrac{\\partial f_i}{\\partial x_j} (jacobian)\n\n\n\\mathbb{R}^{m \\times n}\n\\mathbb{R}\n\\mathbb{R}^{m \\times n}\n\\dfrac{\\partial f}{\\partial x_{ij}}",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#taylor-approximations",
    "href": "docs/theory/Matrix_calculus.html#taylor-approximations",
    "title": "1 Basic linear algebra background",
    "section": "2.5 Taylor approximations",
    "text": "2.5 Taylor approximations\nTaylor approximations provide a way to approximate functions locally by polynomials. The idea is that for a smooth function, we can approximate it by its tangent (for the first order) or by its parabola (for the second order) at a point.\n\n2.5.1 First-order Taylor approximation\nThe first-order Taylor approximation, also known as the linear approximation, is centered around some point x_0. If f: \\mathbb{R}^n \\rightarrow \\mathbb{R} is a differentiable function, then its first-order Taylor approximation is given by:\n\nf_{x_0}^I(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0)\n\nWhere:\n\nf(x_0) is the value of the function at the point x_0.\n\\nabla f(x_0) is the gradient of the function at the point x_0.\n\nIt is very usual to replace the f(x) with f_{x_0}^I(x) near the point x_0 for simple analysis of some approaches.\n\n\n\n\n\n\nFigure 5: First order Taylor approximation near the point x_0\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x) = e^x around the point x_0 = 0, the first order Taylor approximation is: \nf_{x_0}^I(x) = 1 + x\n The second-order Taylor approximation is: \nf_{x_0}^{II}(x) = 1 + x + \\frac{x^2}{2}\n These approximations provide polynomial representations of the function near the point x_0.\n\n\n\n\n\n\n2.5.2 Second-order Taylor approximation\nThe second-order Taylor approximation, also known as the quadratic approximation, includes the curvature of the function. For a twice-differentiable function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, its second-order Taylor approximation centered at some point x_0 is:\n\nf_{x_0}^{II}(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0) + \\frac{1}{2} (x - x_0)^T \\nabla^2 f(x_0) (x - x_0)\n\nWhere:\n\n\\nabla^2 f(x_0) is the Hessian matrix of f at the point x_0.\n\n\n\n\n\n\n\nFigure 6: Second order Taylor approximation near the point x_0\n\n\n\nWhen using the linear approximation of the function is not sufficient one can consider replacing the f(x) with f_{x_0}^{II}(x) near the point x_0. In general, Taylor approximations give us a way to locally approximate functions. The first-order approximation is a plane tangent to the function at the point x_0, while the second-order approximation includes the curvature and is represented by a parabola. These approximations are especially useful in optimization and numerical methods because they provide a tractable way to work with complex functions.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCalculate first and second order Taylor approximation of the function f(x) = \\dfrac{1}{2}x^T A x - b^T x + c\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhy might one choose to use a Taylor approximation instead of the original function in certain applications?\n\n\n\n\nNote, that even the second-order approximation could become inaccurate very quickly. The code for the picture below is available here: 👨‍💻\nYour browser does not support the video tag.",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#naive-approach",
    "href": "docs/theory/Matrix_calculus.html#naive-approach",
    "title": "1 Basic linear algebra background",
    "section": "3.1 Naive approach",
    "text": "3.1 Naive approach\nThe basic idea of the naive approach is to reduce matrix/vector derivatives to the well-known scalar derivatives.  One of the most important practical tricks here is to separate indices of sum (i) and partial derivatives (k). Ignoring this simple rule tends to produce mistakes.",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#differential-approach",
    "href": "docs/theory/Matrix_calculus.html#differential-approach",
    "title": "1 Basic linear algebra background",
    "section": "3.2 Differential approach",
    "text": "3.2 Differential approach\nThe guru approach implies formulating a set of simple rules, which allows you to calculate derivatives just like in a scalar case. It might be convenient to use the differential notation here. 3\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x \\in S be an interior point of the set S, and let D : U \\rightarrow V be a linear operator. We say that the function f is differentiable at the point x with derivative D if for all sufficiently small h \\in U the following decomposition holds: \nf(x + h) = f(x) + D[h] + o(\\|h\\|)\n If for any linear operator D : U \\rightarrow V the function f is not differentiable at the point x with derivative D, then we say that f is not differentiable at the point x.\n\n\n\n\n\n3.2.1 Differentials\nAfter obtaining the differential notation of df we can retrieve the gradient using the following formula:\n\ndf(x) = \\langle \\nabla f(x), dx\\rangle\n\nThen, if we have a differential of the above form and we need to calculate the second derivative of the matrix/vector function, we treat “old” dx as the constant dx_1, then calculate d(df) = d^2f(x)\n\nd^2f(x) = \\langle \\nabla^2 f(x) dx_1, dx\\rangle = \\langle H_f(x) dx_1, dx\\rangle\n\n\n\n3.2.2 Properties\nLet A and B be the constant matrices, while X and Y are the variables (or matrix functions).\n\ndA = 0\nd(\\alpha X) = \\alpha (dX)\nd(AXB) = A(dX )B\nd(X+Y) = dX + dY\nd(X^T) = (dX)^T\nd(XY) = (dX)Y + X(dY)\nd\\langle X, Y\\rangle = \\langle dX, Y\\rangle+ \\langle X, dY\\rangle\nd\\left( \\dfrac{X}{\\phi}\\right) = \\dfrac{\\phi dX - (d\\phi) X}{\\phi^2}\nd\\left( \\det X \\right) = \\det X \\langle X^{-T}, dX \\rangle\nd\\left(\\text{tr } X \\right) = \\langle I, dX\\rangle\ndf(g(x)) = \\dfrac{df}{dg} \\cdot dg(x)\nH = (J(\\nabla f))^T\nd(X^{-1})=-X^{-1}(dX)X^{-1}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\nabla^2 f(x), if f(x) = \\dfrac12 \\langle Ax, x\\rangle - \\langle b, x\\rangle + c.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(x), if f(x) = \\ln \\langle x, Ax\\rangle.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nIt is essential for A to be positive definite, because it is a logarithm argument. So, A \\in \\mathbb{S}^n_{++}Let’s find the differential first: \n\\begin{split}\ndf &= d \\left( \\ln \\langle x, Ax\\rangle \\right) = \\dfrac{d \\left( \\langle x, Ax\\rangle \\right)}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle dx, Ax\\rangle +  \\langle x, d(Ax)\\rangle}{ \\langle x, Ax\\rangle} = \\\\\n&= \\dfrac{\\langle Ax, dx\\rangle + \\langle x, Adx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle Ax, dx\\rangle + \\langle A^T x, dx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle (A + A^T) x, dx\\rangle}{ \\langle x, Ax\\rangle}\n\\end{split}\n\nNote, that our main goal is to derive the form df = \\langle \\cdot, dx\\rangle \ndf = \\left\\langle  \\dfrac{2 A x}{ \\langle x, Ax\\rangle} , dx\\right\\rangle\n Hence, the gradient is \\nabla f(x) = \\dfrac{2 A x}{ \\langle x, Ax\\rangle}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\Vert AX - B\\Vert_F.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the gradient \\nabla f(x) and hessian \\nabla^2f(x), if f(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Dual norm.html",
    "href": "docs/theory/Dual norm.html",
    "title": "1 Dual norm",
    "section": "",
    "text": "1 Dual norm\n\n\n\np-norm and q-norm are dual if this holds\n\n\nLet \\Vert x\\Vert be the norm in the primal space x \\in S \\subseteq \\mathbb{R}^n, then the following expression defines dual norm:\n\n\\Vert y\\Vert _\\star = \\sup\\limits_{\\Vert x\\Vert  \\leq 1} \\langle y,x\\rangle\n\nThe intuition for the finite-dimensional space is how the linear function (element of the dual space) f_y(\\cdot) could stretch the elements of the primal space with respect to their size, i.e. \\Vert y\\Vert _* = \\sup\\limits_{x \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert x\\Vert }.\n\n\n2 Properties\n\nOne can easily define the dual norm as:\n\n  \\Vert x\\Vert _* = \\sup\\limits_{y \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert y\\Vert }\n  \nThe dual norm is also a norm itself\nFor any x \\in E, y \\in E^*: x^\\top y \\leq \\Vert x\\Vert \\cdot \\Vert y\\Vert _*\n\\left(\\Vert x\\Vert _p\\right)_* = \\Vert x\\Vert _q if \\dfrac{1}{p} + \\dfrac{1}{q} = 1, where p, q \\geq 1\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe Euclidian norm is self dual \\left(\\Vert x\\Vert_2\\right)_\\star = \\Vert x\\Vert _2.\n\n\n\n\n                        \n                                            \n\n\n3 Examples\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet f(x) = \\Vert x\\Vert. Prove that f^\\star(y) = \\mathbb{O}_{\\Vert y\\Vert _\\star \\leq 1}\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Theory",
      "Dual norm"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html",
    "href": "docs/theory/Convex_function.html",
    "title": "1 Convexity definitions",
    "section": "",
    "text": "The function f(x), which is defined on the convex set S \\subseteq \\mathbb{R}^n, is called convex on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1.\nIf the above inequality holds as strict inequality x_1 \\neq x_2 and 0 &lt; \\lambda &lt; 1, then the function is called strictly convex on S.\n\n\n\n\n\n\nFigure 1: Difference between convex and non-convex function\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nf(x) = x^p, \\; p &gt; 1,\\; x \\in \\mathbb{R}_+\nf(x) = \\|x\\|^p,\\; p &gt; 1, x \\in \\mathbb{R}^n\nf(x) = e^{cx},\\; c \\in \\mathbb{R}, x \\in \\mathbb{R}\nf(x) = -\\ln x,\\; x \\in \\mathbb{R}_{++}\nf(x) = x\\ln x,\\; x \\in \\mathbb{R}_{++}\nThe sum of the largest k coordinates f(x) = x_{(1)} + \\ldots + x_{(k)},\\; x \\in \\mathbb{R}^n\nf(X) = \\lambda_{max}(X),\\; X = X^T\nf(X) = - \\log \\det X, \\; X \\in S^n_{++}\n\n\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\text{epi } f = \\left\\{[x,\\mu] \\in S \\times \\mathbb{R}: f(x) \\le \\mu\\right\\}\n\nis called epigraph of the function f(x).\n\n\n\n\n\n\nFigure 2: Epigraph of a function\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\mathcal{L}_\\beta = \\left\\{ x\\in S : f(x) \\le \\beta\\right\\}\n\nis called sublevel set or Lebesgue set of the function f(x).\n\n\n\n\n\n\nFigure 3: Sublevel set of a function with respect to level \\beta",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#jensens-inequality",
    "href": "docs/theory/Convex_function.html#jensens-inequality",
    "title": "1 Convexity definitions",
    "section": "",
    "text": "The function f(x), which is defined on the convex set S \\subseteq \\mathbb{R}^n, is called convex on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1.\nIf the above inequality holds as strict inequality x_1 \\neq x_2 and 0 &lt; \\lambda &lt; 1, then the function is called strictly convex on S.\n\n\n\n\n\n\nFigure 1: Difference between convex and non-convex function\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nf(x) = x^p, \\; p &gt; 1,\\; x \\in \\mathbb{R}_+\nf(x) = \\|x\\|^p,\\; p &gt; 1, x \\in \\mathbb{R}^n\nf(x) = e^{cx},\\; c \\in \\mathbb{R}, x \\in \\mathbb{R}\nf(x) = -\\ln x,\\; x \\in \\mathbb{R}_{++}\nf(x) = x\\ln x,\\; x \\in \\mathbb{R}_{++}\nThe sum of the largest k coordinates f(x) = x_{(1)} + \\ldots + x_{(k)},\\; x \\in \\mathbb{R}^n\nf(X) = \\lambda_{max}(X),\\; X = X^T\nf(X) = - \\log \\det X, \\; X \\in S^n_{++}",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#epigraph",
    "href": "docs/theory/Convex_function.html#epigraph",
    "title": "1 Convexity definitions",
    "section": "",
    "text": "For the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\text{epi } f = \\left\\{[x,\\mu] \\in S \\times \\mathbb{R}: f(x) \\le \\mu\\right\\}\n\nis called epigraph of the function f(x).\n\n\n\n\n\n\nFigure 2: Epigraph of a function",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#sublevel-set",
    "href": "docs/theory/Convex_function.html#sublevel-set",
    "title": "1 Convexity definitions",
    "section": "",
    "text": "For the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\mathcal{L}_\\beta = \\left\\{ x\\in S : f(x) \\le \\beta\\right\\}\n\nis called sublevel set or Lebesgue set of the function f(x).\n\n\n\n\n\n\nFigure 3: Sublevel set of a function with respect to level \\beta",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#first-order-differential-criterion-of-convexity",
    "href": "docs/theory/Convex_function.html#first-order-differential-criterion-of-convexity",
    "title": "1 Convexity definitions",
    "section": "2.1 First-order differential criterion of convexity",
    "text": "2.1 First-order differential criterion of convexity\nThe differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x)\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x\n\n\n\n\n\n\n\nFigure 4: Convex function is greater or equal than Taylor linear approximation at any point",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#second-order-differential-criterion-of-convexity",
    "href": "docs/theory/Convex_function.html#second-order-differential-criterion-of-convexity",
    "title": "1 Convexity definitions",
    "section": "2.2 Second-order differential criterion of convexity",
    "text": "2.2 Second-order differential criterion of convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq 0\n\nIn other words, \\forall y \\in \\mathbb{R}^n:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq 0",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#connection-with-epigraph",
    "href": "docs/theory/Convex_function.html#connection-with-epigraph",
    "title": "1 Convexity definitions",
    "section": "2.3 Connection with epigraph",
    "text": "2.3 Connection with epigraph\nThe function is convex if and only if its epigraph is a convex set.\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet a norm \\Vert \\cdot \\Vert be defined in the space U. Consider the set:\n\nK := \\{(x,t) \\in U \\times \\mathbb{R}^+ : \\Vert x \\Vert \\leq t \\}\n\nwhich represents the epigraph of the function x \\mapsto \\Vert x \\Vert. This set is called the cone norm. According to the statement above, the set K is convex.\nIn the case where U = \\mathbb{R}^n and \\Vert x \\Vert = \\Vert x \\Vert_2 (Euclidean norm), the abstract set K transitions into the set:\n\n\\{(x,t) \\in \\mathbb{R}^n \\times \\mathbb{R}^+ : \\Vert x \\Vert_2 \\leq t \\}",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#connection-with-sublevel-set",
    "href": "docs/theory/Convex_function.html#connection-with-sublevel-set",
    "title": "1 Convexity definitions",
    "section": "2.4 Connection with sublevel set",
    "text": "2.4 Connection with sublevel set\nIf f(x) - is a convex function defined on the convex set S \\subseteq \\mathbb{R}^n, then for any \\beta sublevel set \\mathcal{L}_\\beta is convex.\nThe function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is closed if and only if for any \\beta sublevel set \\mathcal{L}_\\beta is closed.",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#reduction-to-a-line",
    "href": "docs/theory/Convex_function.html#reduction-to-a-line",
    "title": "1 Convexity definitions",
    "section": "2.5 Reduction to a line",
    "text": "2.5 Reduction to a line\nf: S \\to \\mathbb{R} is convex if and only if S is a convex set and the function g(t) = f(x + tv) defined on \\left\\{ t \\mid x + tv \\in S \\right\\} is convex for any x \\in S, v \\in \\mathbb{R}^n, which allows checking convexity of the scalar function to establish convexity of the vector function.",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#criteria-of-strong-convexity",
    "href": "docs/theory/Convex_function.html#criteria-of-strong-convexity",
    "title": "1 Convexity definitions",
    "section": "3.1 Criteria of strong convexity",
    "text": "3.1 Criteria of strong convexity\n\n3.1.1 First-order differential criterion of strong convexity\nDifferentiable f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is \\mu-strongly convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x) + \\dfrac{\\mu}{2}\\|y-x\\|^2\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x + \\dfrac{\\mu}{2}\\|\\Delta x\\|^2\n\n\n\n3.1.2 Second-order differential criterion of strong convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is called \\mu-strongly convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq \\mu I\n\nIn other words:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq \\mu \\|y\\|^2",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html",
    "href": "docs/theory/Conjugate function.html",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "Let f: \\mathbb{R}^n \\to \\mathbb{R}. The function f^*: \\mathbb{R}^n \\to \\mathbb{R} is called convex conjugate (Fenchel’s conjugate, dual, Legendre transform) f(x) and is defined as follows:\n\nf^*(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right).\n\nLet’s notice, that the domain of the function f^* is the set of those y, where the supremum is finite.\n\n\n\n\n\n\nFigure 1: Illustration of conjugate function\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nice intuition behind the conjugate function. On the left, we have a slow parabola (say, f(x) = \\frac{x^2}{10}, which implies a small magnitude of the slope with a large magnitude of x_0. On the right, we have the conjugate function f^*(y) = 2.5 y^2, which has a large slope with the small value of y_0.)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nUsing the geometric intuition above, draw the conjugate function to the function below:\n\n\n\n\n\n\nFigure 3: You can use geometric inution from above to draw f^{*}(y).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Take a look at the constant slope x_0 from the y_1 to y_2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nf^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)\n  \n\n\n\n\nThe scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right) = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y) is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = ax + b.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x\nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nFind f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}.\nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = e^x.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}.\nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x.\nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i.\nHence, f^*(y) = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nRevenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by\n\nM(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n\nThe function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as \nM(p) = (−S)^*(−p).\n Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed).",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html#properties",
    "href": "docs/theory/Conjugate function.html#properties",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "Figure 2: Nice intuition behind the conjugate function. On the left, we have a slow parabola (say, f(x) = \\frac{x^2}{10}, which implies a small magnitude of the slope with a large magnitude of x_0. On the right, we have the conjugate function f^*(y) = 2.5 y^2, which has a large slope with the small value of y_0.)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nUsing the geometric intuition above, draw the conjugate function to the function below:\n\n\n\n\n\n\nFigure 3: You can use geometric inution from above to draw f^{*}(y).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Take a look at the constant slope x_0 from the y_1 to y_2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nf^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html#examples",
    "href": "docs/theory/Conjugate function.html#examples",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "The scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right) = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y) is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = ax + b.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x\nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nFind f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}.\nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = e^x.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}.\nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x.\nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i.\nHence, f^*(y) = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nRevenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by\n\nM(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n\nThe function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as \nM(p) = (−S)^*(−p).\n Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed).",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html",
    "href": "docs/methods/zom/nelder-mead.html",
    "title": "1 Problem",
    "section": "",
    "text": "Sometimes the multidimensional function is so difficult to evaluate that even expressing the 1^{\\text{st}} derivative for gradient-based methods of finding optimum becomes an impossible task. In this case, we can only rely on the values of the function at each point. Or, in other words, on the 0 order oracle calls.\nLet’s take, for instance, Mishra’s Bird function:\n\nf(x,y) = \\sin{y} \\cdot e^{\\left( 1 - \\cos{x} \\right)^2} + \\cos{x} \\cdot e^{\\left( 1 - \\sin{y} \\right)^2} + (x - y)^2\n\n\n\n\nIllustration\n\n\nThis function is usually subjected to the domain (x+5)^2 + (y+5)^2 &lt; 25, but for the sake of picture beauty we will mainly use domain [-10; 0] \\times [-10; 0].\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#related-definitions",
    "href": "docs/methods/zom/nelder-mead.html#related-definitions",
    "title": "1 Problem",
    "section": "2.1 Related definitions:",
    "text": "2.1 Related definitions:\n\n\\textbf{Simplex} – polytope with the least possible number of vertices in n-dimensional space. (So, it’s (n+1)-polytope.) In our 2D case it will be triangle.\n\\textbf{Best point }x_1 – vertex of the simplex, function value in which is the smallest among all vertices.\n\\textbf{Worst point }x_{n+1} – vertex of the simplex, function value in which is the largest among all vertices.\n\\textbf{Other points }x_2, \\ldots, x_n – vertices of the simplex, ordered in such way that f(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1}). This implies that \\{ x_1, x_2, \\ldots, x_n \\} are best points in relation to x_{n+1} and \\{ x_2, \\ldots, x_n, x_{n+1} \\} are worst points in relation to x_1.\n\\textbf{Centroid }x_o – center of mass in the polytope. In Nelder-Mead the centroid is calculated for the polytope, constituted by best vertices. In our 2D case it will be the center of the triangle side, which contains 2 best points x_o = \\dfrac{x_1 + x_2}{2}.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#main-idea",
    "href": "docs/methods/zom/nelder-mead.html#main-idea",
    "title": "1 Problem",
    "section": "2.2 Main idea",
    "text": "2.2 Main idea\nThe algorithm maintains the set of test points in the form of simplex. For each point the function value is calculated and points are ordered accordingly. Depending on those values, the simplex exchanges the worst point of the set for the new one, which is closer to the local minimum. In some sense, the simplex is crawling to the minimal value in the domain.\nThe simplex movements finish when its sides become too small (termination condition by sides) or its area becomes too small (termination condition by area). I prefer the second condition, because it takes into account cases when simplex becomes degenerate (three or more vertices on one axis).",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#steps-of-the-algorithm",
    "href": "docs/methods/zom/nelder-mead.html#steps-of-the-algorithm",
    "title": "1 Problem",
    "section": "2.3 Steps of the algorithm",
    "text": "2.3 Steps of the algorithm\n1. Ordering\nOrder vertices according to values in them:\n\nf(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1})\n\nCheck the termination condition. Possible exit with solution x_{\\min} = x_1.\n2. Centroid calculation\n\nx_o = \\dfrac{\\sum\\limits_{k=1}^{n}{x_k}}{n}\n\n3. Reflection\nCalculate the reflected point x_r:\n\nx_r = x_o + \\alpha \\left( x_o - x_{n+1} \\right)\n\nwhere \\alpha – reflection coefficient, \\alpha &gt; 0. (If \\alpha \\leqslant 0, reflected point x_r will not overlap the centroid)\nThe next step is figured out according to the value of f(x_r) in dependency to values in points x_1 (best) and x_n (second worst):\n\nf(x_r) &lt; f(x_1): Go to step 4.\nf(x_1) \\leqslant f(x_r) &lt; f(x_n): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1.\nf(x_r) \\geqslant f(x_n): Go to step 5.\n\n4. Expansion\nCalculate the expanded point x_e:\n\nx_e = x_o + \\gamma \\left( x_r - x_o \\right)\n\nwhere \\gamma – expansion coefficient, \\gamma &gt; 1. (If \\gamma &lt; 1, expanded point x_e will be contracted towards centroid, if \\gamma = 1: x_e = x_r)\nThe next step is figured out according to the ratio between f(x_e) and f(x_r):\n\nf(x_e) &lt; f(x_r): new simplex with x_{n+1} \\rightarrow x_e. Go to step 1.\nf(x_e) &gt; f(x_r): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1.\n\n5. Contraction\nCalculate the contracted point x_c:\n\nx_c = x_o + \\beta \\left( x_{n+1} - x_o \\right)\n\nwhere \\beta – contraction coefficient, 0 &lt; \\beta \\leqslant 0.5. (If \\beta &gt; 0.5, contraction is insufficient, if \\beta \\leqslant 0, contracted point x_c overlaps the centroid)\nThe next step is figured out according to the ratio between f(x_c) and f(x_{n+1}):\n\nf(x_c) &lt; f(x_{n+1}): new simplex with x_{n+1} \\rightarrow x_c. Go to step 1.\nf(x_c) \\geqslant f(x_{n+1}): Go to step 6.\n\n6. Shrinkage\nReplace all points of simplex x_i with new ones, except for the best point x_1:\n\nx_i = x_1 + \\sigma \\left( x_i - x_1 \\right)\n\nwhere \\sigma – shrinkage coefficient, 0 &lt; \\sigma &lt; 1. (If \\sigma \\geqslant 1, shrinked point x_i overlaps the best point x_1, if \\sigma \\leqslant 0, shrinked point x_i becomes extended)\nGo to step 1.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#some-random-initial-simplex-and-default-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#some-random-initial-simplex-and-default-set-of-parameters",
    "title": "1 Problem",
    "section": "3.1 Some random initial simplex and default set of parameters",
    "text": "3.1 Some random initial simplex and default set of parameters\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#different-initial-simplex-and-same-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#different-initial-simplex-and-same-set-of-parameters",
    "title": "1 Problem",
    "section": "3.2 Different initial simplex and same set of parameters",
    "text": "3.2 Different initial simplex and same set of parameters\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#same-initial-simplex-and-different-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#same-initial-simplex-and-different-set-of-parameters",
    "title": "1 Problem",
    "section": "3.3 Same initial simplex and different set of parameters",
    "text": "3.3 Same initial simplex and different set of parameters\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#round-domain",
    "href": "docs/methods/zom/nelder-mead.html#round-domain",
    "title": "1 Problem",
    "section": "3.4 Round domain",
    "text": "3.4 Round domain\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#examples-with-all-sets-of-simplexes",
    "href": "docs/methods/zom/nelder-mead.html#examples-with-all-sets-of-simplexes",
    "title": "1 Problem",
    "section": "3.5 Examples with all sets of simplexes",
    "text": "3.5 Examples with all sets of simplexes\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html",
    "href": "docs/methods/zom/bee_algorithm.html",
    "title": "1 Algorithm",
    "section": "",
    "text": "The Bee Algorithm was mathematically first described relatively recently. It is one of the representatives of a large family of algorithms that allow modeling swarm intelligence. This article will provide an example of the application of the Bee Algorithm to search for the global extremum of the function. The two-dimensional Schwefel function, having a large number of local minima, and the Rosenbrock function, whose global minimum lies in a narrow, parabolic valley, were chosen as the target functions.\n\n\nA colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here.\n\n\n\nAnd now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While\n\n\n\n\nTwo standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube x_i \\in [-500, 500] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube x_i \\in [-5, 10] for all i = 1, ..., d, although it may be restricted to the hypercube x_i \\in [-2.048, 2.048] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#intuition",
    "href": "docs/methods/zom/bee_algorithm.html#intuition",
    "title": "1 Algorithm",
    "section": "",
    "text": "A colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#mathematical-interpretation",
    "href": "docs/methods/zom/bee_algorithm.html#mathematical-interpretation",
    "title": "1 Algorithm",
    "section": "",
    "text": "And now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#examples",
    "href": "docs/methods/zom/bee_algorithm.html#examples",
    "title": "1 Algorithm",
    "section": "",
    "text": "Two standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube x_i \\in [-500, 500] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube x_i \\in [-5, 10] for all i = 1, ..., d, although it may be restricted to the hypercube x_i \\in [-2.048, 2.048] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html",
    "href": "docs/methods/line_search/inexact.html",
    "title": "1 Sufficient decrease",
    "section": "",
    "text": "This strategy of inexact line search works well in practice, as well as it has the following geometric interpretation:\n\n1 Sufficient decrease\nLet’s consider the following scalar function while being at a specific point of x_k:\n\n\\phi(\\alpha) = f(x_k - \\alpha\\nabla f(x_k)), \\alpha \\geq 0\n\nconsider first order approximation of \\phi(\\alpha):\n\n\\phi(\\alpha) \\approx f(x_k) - \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\nA popular inexact line search condition stipulates that \\alpha should first of all give sufficient decrease in the objective function f, as measured by the following inequality:\n\nf(x_k - \\alpha \\nabla f (x_k)) \\leq f(x_k) - c_1 \\cdot \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\nfor some constant c_1 \\in (0,1). (Note, that c_1 = 1 stands for the first order Taylor approximation of \\phi(\\alpha)). This is also called Armijo condition. The problem of this condition is, that it could accept arbitrary small values \\alpha, which may slow down solution of the problem. In practice, c_1 is chosen to be quite small, say c_1 \\approx 10^{−4}.\n\n\n2 Curvature condition\nTo rule out unacceptably short steps one can introduce a second requirement:\n\n-\\nabla f (x_k - \\alpha \\nabla f(x_k))^\\top \\nabla f(x_k) \\geq c_2 \\nabla f(x_k)^\\top(- \\nabla f(x_k))\n\nfor some constant c_2 \\in (c_1,1), where c_1 is a constant from Armijo condition. Note that the left-handside is simply the derivative \\nabla_\\alpha \\phi(\\alpha), so the curvature condition ensures that the slope of \\phi(\\alpha) at the target point is greater than c_2 times the initial slope \\nabla_\\alpha \\phi(\\alpha)(0). Typical values of c_2 \\approx 0.9 for Newton or quasi-Newton method. The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions.\n\n\n3 Goldstein conditions\nLet’s consider also 2 linear scalar functions \\phi_1(\\alpha), \\phi_2(\\alpha):\n\n\\phi_1(\\alpha) = f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|^2\n\nand\n\n\\phi_2(\\alpha) = f(x_k) - c_2 \\alpha \\|\\nabla f(x_k)\\|^2\n\nNote, that Goldstein-Armijo conditions determine the location of the function \\phi(\\alpha) between \\phi_1(\\alpha) and \\phi_2(\\alpha). Typically, we choose c_1 = \\rho and c_2 = 1 - \\rho, while $ (0.5, 1)$.\n\n\n\nIllustration\n\n\n\n\n4 References\n\nNumerical Optimization by J.Nocedal and S.J.Wright.\nInteractive Wolfe Line Search Example by fmin library.",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html",
    "href": "docs/methods/line_search/golden_search.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nThe idea is quite similar to the dichotomy method. There are two golden points on the line segment (left and right) and the insightful idea is, that on the next iteration one of the points will remain the golden point.\n\n\n\nIllustration\n\n\n\n\n2 Algorithm\ndef golden_search(f, a, b, epsilon):\n    tau = (sqrt(5) + 1) / 2\n    y = a + (b - a) / tau**2\n    z = a + (b - a) / tau\n    while b - a &gt; epsilon:\n        if f(y) &lt;= f(z):\n            b = z\n            z = y\n            y = a + (b - a) / tau**2\n        else:\n            a = y\n            y = z\n            z = a + (b - a) / tau\n    return (a + b) / 2\n\n\n3 Bounds\n\n|x_{k+1} - x_*| \\leq b_{k+1} - a_{k+1} = \\left( \\frac{1}{\\tau} \\right)^{N-1} (b - a) \\approx 0.618^k(b-a),\n\nwhere \\tau = \\frac{\\sqrt{5} + 1}{2}.\n\nThe geometric progression constant more than the dichotomy method - 0.618 worse than 0.5\nThe number of function calls is less than for the dichotomy method - 0.707 worse than 0.618 - (for each iteration of the dichotomy method, except for the first one, the function is calculated no more than 2 times, and for the gold method - no more than one)",
    "crumbs": [
      "Methods",
      "Line search",
      "Golden search"
    ]
  },
  {
    "objectID": "docs/methods/index.html",
    "href": "docs/methods/index.html",
    "title": "1 General formulation",
    "section": "",
    "text": "\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. }  g_i(x) \\leq& 0, \\; i = 1,\\ldots,m\\\\\nh_j(x) =& 0, \\; j = 1,\\ldots,k\\\\\n\\end{split}\n\nSome necessary or/and sufficient conditions are known (See {% include link.html title=‘Optimality conditions. KKT’%} and {% include link.html title=‘Convex optimization problem’ %}) * In fact, there might be very challenging to recognize the convenient form of optimization problem. * Analytical solution of KKT could be inviable.\n\n\nTypically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#iterative-methods",
    "href": "docs/methods/index.html#iterative-methods",
    "title": "1 General formulation",
    "section": "",
    "text": "Typically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#unsolvability",
    "href": "docs/methods/index.html#unsolvability",
    "title": "1 General formulation",
    "section": "2.1 Unsolvability",
    "text": "2.1 Unsolvability\nIn general, optimization problems are unsolvable. ¯\\(ツ)/¯\nConsider the following simple optimization problem of a function over unit cube:\n\n\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. } &  x \\in \\mathbb{B}^n\n\\end{split}\n\nWe assume, that the objective function f (\\cdot) : \\mathbb{R}^n \\to \\mathbb{R} is Lipschitz continuous on \\mathbb{B}^n:\n\n| f (x) − f (y) | \\leq L \\| x − y \\|_{\\infty} \\forall x,y \\in \\mathbb{B}^n,\n\nwith some constant L (Lipschitz constant). Here \\mathbb{B}^n - the n-dimensional unit cube\n\n\\mathbb{B}^n = \\{x \\in \\mathbb{R}^n \\mid 0 \\leq x_i \\leq 1, i = 1, \\ldots, n\\}\n\nOur goal is to find such \\tilde{x}: \\vert f(\\tilde{x}) - f^*\\vert \\leq \\varepsilon for some positive \\varepsilon. Here f^* is the global minima of the problem. Uniform grid with p points on each dimension guarantees at least this quality:\n\n\\| \\tilde{x} − x_* \\|_{\\infty} \\leq \\frac{1}{2p},\n\nwhich means, that\n\n|f (\\tilde{x}) − f (x_*)| \\leq \\frac{L}{2p}\n\nOur goal is to find the p for some \\varepsilon. So, we need to sample $ ()^n$ points, since we need to measure function in p^n points. Doesn’t look scary, but if we’ll take L = 2, n = 11, \\varepsilon = 0.01, computations on the modern personal computers will take 31,250,000 years.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#stopping-rules",
    "href": "docs/methods/index.html#stopping-rules",
    "title": "1 General formulation",
    "section": "2.2 Stopping rules",
    "text": "2.2 Stopping rules\n\nArgument closeness:\n\n  \\| x_k - x_*  \\|_2 &lt; \\varepsilon\n  \nFunction value closeness:\n\n  \\| f_k - f^* \\|_2 &lt; \\varepsilon\n  \nCloseness to a critical point\n\n  \\| f'(x_k) \\|_2 &lt; \\varepsilon\n  \n\nBut x_* and f^* = f(x_*) are unknown!\nSometimes, we can use the trick:\n\n\\|x_{k+1} - x_k \\| = \\|x_{k+1} - x_k + x_* - x_* \\| \\leq \\|x_{k+1} - x_* \\| + \\| x_k - x_* \\| \\leq 2\\varepsilon\n\nNote: it’s better to use relative changing of these values, i.e. \\dfrac{\\|x_{k+1} - x_k \\|_2}{\\| x_k \\|_2}.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#local-nature-of-the-methods",
    "href": "docs/methods/index.html#local-nature-of-the-methods",
    "title": "1 General formulation",
    "section": "2.3 Local nature of the methods",
    "text": "2.3 Local nature of the methods",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html",
    "href": "docs/methods/fom/Subgradient descent.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Рассматривается классическая задача выпуклой оптимизации:\n\n\\min_{x \\in S} f(x),\n\nПодразумевается, что f(x) - выпуклая функция на выпуклом множестве S. Для начала будем рассматривать задачу безусловной минимизации (БМ), S = \\mathbb{R}^n\nВектор g называется субградиентом функции f(x): S \\to \\mathbb{R} в точке x_0, если \\forall x \\in S:\n\nf(x)  \\geq f(x_0) +  \\langle g, x - x_0 \\rangle\n\nГрадиентный спуск предполагает, что функция f(x) является дифференцируемой в каждой точке задачи. Теперь же, мы будем предполагать лишь выпуклость.\nИтак, мы имеем оракул первого порядка:\nВход: x \\in \\mathbb R^n\nВыход: \\partial f(x) и f(x)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#bounds",
    "href": "docs/methods/fom/Subgradient descent.html#bounds",
    "title": "1 Introduction",
    "section": "2.1 Bounds",
    "text": "2.1 Bounds\n\n2.1.1 Vanilla version\nЗапишем как близко мы подошли к оптимуму x^* = \\text{arg}\\min\\limits_{x \\in \\mathbb{R}^n} f(x) = \\text{arg} f^* на последней итерации:\n\n\\begin{align*}\n\\| x_{k+1} - x^* \\|^2 & = \\|x_k - x^* - \\alpha_k g_k\\|^2 = \\\\\n                      & = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\\end{align*}\n\nДля субградиента: \\langle g_k, x_k - x^* \\rangle \\leq f(x_k) - f(x^*) = f(x_k) - f^*. Из написанного выше:\n\n\\begin{align*}\n2\\alpha_k \\langle g_k, x_k - x^* \\rangle =  \\| x_k - x^* \\|^2 + \\alpha_k^2 g_k^2 - \\| x_{k+1} - x^* \\|^2\n\\end{align*}\n\nПросуммируем полученное неравенство для k = 0, \\ldots, T-1\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k \\langle g_k, x_k - x^* \\rangle &=  \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq \\| x_0 - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2\n\\end{align*}\n\nЗдесь мы предположили R^2 = \\|x_0 - x^*\\|^2, \\qquad \\|g_k\\| \\leq G. Предполагая \\alpha_k = \\alpha (постоянный шаг), имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq \\dfrac{R^2}{2 \\alpha} + \\dfrac{\\alpha}{2}G^2 T\n\\end{align*}\n\nМинимизация правой части по \\alpha дает \\alpha^* = \\dfrac{R}{G}\\sqrt{\\dfrac{1}{T}}\n\n\\begin{align*}\n\\tag{Subgradient Bound}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T}\n\\end{align*}\n\nТогда (используя неравенство Йенсена и свойство субградиента f(x^*) \\geq f(x_k) + \\langle g_k, x^* - x_k \\rangle) запишем оценку на т.н. Regret, а именно:\n\n\\begin{align*}\nf(\\overline{x}) - f^* &= f \\left( \\frac{1}{T}\\sum\\limits_{k=0}^{T-1} x_k \\right) - f^* \\leq \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1} (f(x_k) - f^* )\\right) \\\\\n& \\leq  \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^* \\rangle\\right) \\\\\n& \\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nВажные моменты:\n\nПолучение оценок не для x_T, а для среднего арифметического по итерациям \\overline{x} - типичный трюк при получении оценок для методов, где есть выпуклость, но нет удобного убывания на каждой итерации. Нет гарантий успеха на каждой итерации, но есть гарантия успеха в среднем\nДля выбора оптимального шага необходимо знать (предположить) число итераций заранее. Возможный выход: инициализировать T небольшим значением, после достижения этого количества итераций удваивать T и рестартовать алгоритм. Более интеллектуальный способ: адаптивный выбор длины шага.\n\n\n\n2.1.2 Steepest subgradient descent\nПопробуем выбирать на каждой итерации длину шага более оптимально. Тогда:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\nМинимизируя выпуклую правую часть по \\alpha_k, получаем:\n\n\\alpha_k = \\dfrac{\\langle g_k, x_k - x^*\\rangle}{\\| g_k\\|^2}\n\nОценки изменятся следующим образом:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 - \\dfrac{\\langle g_k, x_k - x^*\\rangle^2}{\\| g_k\\|^2}\n\n\n\\langle g_k, x_k - x^*\\rangle^2 = \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) \\| g_k\\|^2\n\n\n\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\sum\\limits_{k=0}^{T-1}\\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 \\right) G^2\n\n\n\\dfrac{1}{T}\\left(\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle \\right)^2 \\leq \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq R^2  G^2\n\nЗначит,\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle  \\leq GR \\sqrt{T}\n\nЧто приводит к абсолютно такой же оценке \\mathcal{O}\\left(\\dfrac{1}{\\sqrt{T}}\\right) на невязку по значению функции. На самом деле, для такого класса функций нельзя получить результат лучше, чем \\dfrac{1}{\\sqrt{T}} или \\dfrac{1}{\\varepsilon^2} по итерациям\n\n\n2.1.3 Online learning\nРассматривается следующая игра: есть игрок и природа. На каждом из k = 0, \\ldots, T-1 шагов:\n\nИгрок выбирает действие x_k\nПрирода (возможно, враждебно) выбирает выпуклую функцию f_k, сообщает игроку значение f(x_k), g_k \\in \\partial f(x_k)\nИгрок вычисляет следующее действие, чтобы минимизировать регрет:\n\n\n\\tag{Regret}\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x)\n\nВ такой постановке цель игрока состоит в том, чтобы выбрать стратегию, которая минимизирует разницу его действия с наилучшим выбором на каждом шаге.\nНесмотря на весьма сложную (на первый взгляд) постановку задачи, существует стратегия, при которой регрет растет как \\sqrt{T}, что означает, что усредненный регрет \\dfrac{1}{T} R_{T-1} падает, как \\dfrac{1}{\\sqrt{T}}\nЕсли мы возьмем оценку (Subgradient Bound) для субградиентного метода, полученную выше, мы имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq G \\|x_0 - x^*\\| \\sqrt{T}\n\\end{align*}\n\nОднако, в её выводе мы нигде не использовали тот факт, что x^* = \\text{arg}\\min\\limits_{x \\in S} f(x). Более того, мы вообще не использовали никакой специфичности точки x^*. Тогда можно записать это для произвольной точки y:\n\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq G \\|x_0 - y\\| \\sqrt{T}\n\nЗапишем тогда оценки для регрета, взяв y = \\text{arg}\\min\\limits_{x \\in S}\\sum\\limits_{k = 0}^{T-1} f_k(x):\n\n\\begin{align*}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\sum\\limits_{k = 0}^{T-1} f_k(y) = \\\\\n&= \\sum\\limits_{k = 0}^{T-1} \\left( f_k(x_k) - f_k(y)\\right) \\leq \\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq \\\\\n&\\leq G \\|x_0 - y\\| \\sqrt{T}\n\\end{align*}\n\nИтого мы имеем для нашей стратегии с постоянным шагом:\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq G \\| x_0 - x^* \\| \\dfrac{1}{\\sqrt{T}}, \\qquad \\alpha_k = \\alpha = \\dfrac{\\|x_0 - x^*\\|}{G}\\sqrt{\\dfrac{1}{T}}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#least-squares-with-l_1-regularization",
    "href": "docs/methods/fom/Subgradient descent.html#least-squares-with-l_1-regularization",
    "title": "1 Introduction",
    "section": "3.1 Least squares with l_1 regularization",
    "text": "3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R}^n} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\nAlgorithm will be written as:\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\nwhere signum function is taken element-wise.\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#support-vector-machines",
    "href": "docs/methods/fom/Subgradient descent.html#support-vector-machines",
    "title": "1 Introduction",
    "section": "3.2 Support vector machines",
    "text": "3.2 Support vector machines\nLet D = \\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^n, y_i \\in \\{\\pm 1\\}\\}\nWe need to find \\omega \\in \\mathbb{R}^n and b \\in \\mathbb{R} such that\n\n\\min_{\\omega \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\dfrac{1}{2}\\|\\omega\\|_2^2 + C\\sum\\limits_{i=1}^m max[0, 1 - y_i(\\omega^\\top x_i + b)]",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SAG.html",
    "href": "docs/methods/fom/SAG.html",
    "title": "",
    "section": "",
    "text": "A classical problem of minimizing finite sum of the smooth and convex functions was considered.\n\n\\min\\limits_{x \\in \\mathbb{R}^{p}} g(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost. Baseline solution to the problem is to calculate the loss function and the corresponding gradient vector only on the small subset of indicies from i = 1, \\ldots, n, which usually refers as Stochastic gradient descent. The authors claim, that the convergence rate of proposed algorithm is the same a for the full Gradient Descent method (\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) for convex functions and \\mathcal{O}\\left(\\dfrac{1}{k}\\right) for strongly convex objectives), but the iteration costs remain the same as for the stochastic version.\nThe method itself takes the following form:\n\n\\tag{SAG}\nx_{k+1}=x_{k}-\\frac{\\alpha_{k}}{p} \\sum_{i=1}^{p} y^{i}_{k}\n\nwhere at each iteration only a random summand of a gradient is updated:\n\n\\tag{SAG}\ny^{i}_{k}=\\left\\{\\begin{array}{ll}{f_{i}^{\\prime}\\left(x_{k}\\right)}, & {\\text { if } i=i_{k}} \\\\ {y^{i}_{k-1}}, & {\\text { otherwise }}\\end{array}\\right.\n\n\nThere is a dependency on dimensionality factor n in bounds. However, it can be improved using restart technique.\nEmpirical results were only shown on logistic regression with Tikhonov regularization problems on different datasets.\nBatch and non-uniform versions are also presented in the paper.\nThe first known paper, that contains proof of linear convergence for the convex case.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic average gradient"
    ]
  },
  {
    "objectID": "docs/methods/fom/SAG.html#summary",
    "href": "docs/methods/fom/SAG.html#summary",
    "title": "",
    "section": "",
    "text": "A classical problem of minimizing finite sum of the smooth and convex functions was considered.\n\n\\min\\limits_{x \\in \\mathbb{R}^{p}} g(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost. Baseline solution to the problem is to calculate the loss function and the corresponding gradient vector only on the small subset of indicies from i = 1, \\ldots, n, which usually refers as Stochastic gradient descent. The authors claim, that the convergence rate of proposed algorithm is the same a for the full Gradient Descent method (\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) for convex functions and \\mathcal{O}\\left(\\dfrac{1}{k}\\right) for strongly convex objectives), but the iteration costs remain the same as for the stochastic version.\nThe method itself takes the following form:\n\n\\tag{SAG}\nx_{k+1}=x_{k}-\\frac{\\alpha_{k}}{p} \\sum_{i=1}^{p} y^{i}_{k}\n\nwhere at each iteration only a random summand of a gradient is updated:\n\n\\tag{SAG}\ny^{i}_{k}=\\left\\{\\begin{array}{ll}{f_{i}^{\\prime}\\left(x_{k}\\right)}, & {\\text { if } i=i_{k}} \\\\ {y^{i}_{k-1}}, & {\\text { otherwise }}\\end{array}\\right.\n\n\nThere is a dependency on dimensionality factor n in bounds. However, it can be improved using restart technique.\nEmpirical results were only shown on logistic regression with Tikhonov regularization problems on different datasets.\nBatch and non-uniform versions are also presented in the paper.\nThe first known paper, that contains proof of linear convergence for the convex case.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic average gradient"
    ]
  },
  {
    "objectID": "docs/methods/fom/SAG.html#bounds",
    "href": "docs/methods/fom/SAG.html#bounds",
    "title": "",
    "section": "2 Bounds",
    "text": "2 Bounds\nFor a constant step size \\alpha = \\dfrac{1}{16 L}, where L stands for the Lipschitz constant of a gradient of each function f_i(x) (in practice, it means that L = \\max\\limits_{i=1, \\ldots, n} L_i).\n\n\\mathbb{E}\\left[g\\left(\\overline{x}_{k}\\right)\\right]-g\\left(x^{*}\\right) \\leqslant \\frac{32 n}{k} C_{0},\n\nwhere C_0=g\\left(x_0\\right)-g\\left(x^*\\right)+\\frac{4L}{n} \\| x_0 - x^\\ast\\|^2 +\\frac{\\sigma^2}{16L} in convex case and\n\n\\mathbb{E}\\left[g\\left(x_{k}\\right)\\right]-g\\left(x^*\\right) \\leqslant\\left(1-\\min \\left\\{\\frac{\\mu}{16 L}, \\frac{1}{8 n}\\right\\}\\right)^{k} C_{0}\n\nin \\mu - strongly convex case.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic average gradient"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html",
    "href": "docs/methods/fom/Mirror_descent.html",
    "title": "1 Возвращение к истокам",
    "section": "",
    "text": "Метод зеркального спуска является естественным обобщением метода проекции субградиента в случае обобщения l_2 нормы на более общий случай какой-то функции расстояния.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#dual-norm",
    "href": "docs/methods/fom/Mirror_descent.html#dual-norm",
    "title": "1 Возвращение к истокам",
    "section": "0.1 Dual norm:",
    "text": "0.1 Dual norm:\nОпределение: Сопряженной нормой \\|\\cdot\\|_* к данной \\|\\cdot\\| называется:\n\n\\|y\\|_* = \\max \\{\\langle y,x\\rangle: \\|x\\|=1 \\}\n\nПример: (\\|\\cdot \\|_p)_* = \\|\\cdot \\|_q, \\qquad \\dfrac{1}{p} + \\dfrac{1}{q} = 1\nДоказательство:\nНеравенство Гельдера:\n\n\\sum_{k=1}^n |x_k\\,y_k| \\le \\biggl( \\sum_{k=1}^n |x_k|^p \\biggr)^{\\frac{1}{p}} \\biggl( \\sum_{k=1}^n |y_k|^q \\biggr)^{\\frac{1}{q}}\n\\text{ for all }x, y \\in \\mathbb{C}^n\n\nСвойства:\n\nДвойственная норма \\|\\cdot\\|_* является нормой\nl_2 норма сопряжена сама себе\nДвойственная норма к двойственной норме - исходная норма\n(\\|\\cdot\\|_1)_* = \\|\\cdot\\|_\\infty, \\;(\\|\\cdot\\|_\\infty)_* = \\|\\cdot\\|_1\nОбобщенное неравенство Коши Шварца: \\langle y,x \\rangle \\leq \\|y\\|_*\\|x\\|, следствие: \\|x\\|^2 \\pm 2 \\langle y,x \\rangle + \\|y\\|_*^2 \\geq 0",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#bregman-divergence",
    "href": "docs/methods/fom/Mirror_descent.html#bregman-divergence",
    "title": "1 Возвращение к истокам",
    "section": "0.2 Bregman divergence",
    "text": "0.2 Bregman divergence\nПопробуем интуитивно ввести понятие обобщенного расстояния, именуемого расстоянием Брэгмана. Для каждой точки y она возвращает расстояние этой точки до x - V_x(y). В самом простом случае можно взять V_x(y) = \\frac{1}{2}\\|x-y\\|^2, \\;\\; \\nabla V_x(y) = y-x. Рассмотрим уже классическую запись:\n\n\\begin{align*}\n\\|x_{k+1} - y\\|^2  &= \\|x_{k+1}-x_k \\|^2 + \\|x_k - y\\|^2 - 2 \\langle x_k - x_{k+1} ,x_k - y\\rangle \\\\\n\\tag{Req1}\nV_{x_{k+1}}(y) &= V_{x_{k+1}}(x_k) + V_{x_{k}}(y) - \\langle \\nabla V_{x_{k+1}(x_k)}, x_k - y \\rangle\n\\end{align*}\n\nДля вводимого обобщенного расстояния будем требовать выполнения (Req1), кроме того (как будет видно при получении оценок), приятным свойством было бы еще следующее требование:\n\n\\tag{Req2}\nV_x(y) \\geq \\frac{1}{2} \\|x-y\\|^2\n\nОпределение: Дивергенцией (расстоянием) Брэгмана называется функция следующая V_x(y). Пусть S \\subseteq \\mathbb{R}^n - замкнутое выпуклое множество, тогда функция \\phi : S \\to \\mathbb{R} называется прокс-функцией (distance generating function), если \\phi является 1 - сильно выпуклой, т.е.:\n\n\\phi(y) \\geq \\phi(x) + \\langle \\nabla \\phi(x), y-x\\rangle + \\frac{1}{2} \\|y-x\\|^2, \\qquad \\forall x,y \\in S\n\nТогда прокс-функцией индуцируется расстояние Брэгмана:\n\nV_x(y) = \\phi(y) - \\phi(x) - \\langle\\nabla \\phi(x), y-x\\rangle\n\nЗаметим, что определение сильной выпуклости зависит от выбора прямой нормы \\|\\cdot\\|. Это важное замечание, поскольку именно это свойство позволит в будущем подстраивать расстояние под геометрию пространства.\n\n0.2.1 Examples\n\nВыберем норму в прямом пространстве \\|\\cdot\\| = \\|\\cdot\\|_2, пусть \\phi(x) = \\frac{1}{2}\\|x\\|^2, тогда расстояние Брэгмана V_x(y) = \\frac{1}{2}\\|x-y\\|^2. Такой выбор совпадает с тем, что мы видели ранее в методе проекции субградиента\nВыберем теперь другую норму \\|\\cdot\\| = \\| \\cdot \\|_1, пусть \\phi(x) = \\sum\\limits_{i \\in [n]}x_i \\log x_i - антиэнтропия. Тогда эта функция будет 1 сильно выпукла на выпуклом множестве S : \\left\\{x \\in S : x \\geq 0, \\sum\\limits_{i \\in [n]} x_i = 1\\right\\} (вероятностном симплексе), а соответствующая ей дивергенция Брэгмана: V_x(y) = \\sum\\limits_{i \\in [n]} y_i \\log \\frac{y_i}{x_i} = D(y \\| x) - расстояние Кульбака - Ляйблера.\nЕще немного примеров отсюда:\n\n\n\n\nIllustration\n\n\n\n\n0.2.2 Свойства\n\nАксиома тождества V_x(x) = 0\nСовместимость с Евклидовой нормой: V_x(y) \\geq \\frac{1}{2}\\|x-y\\|^2 \\geq 0\n(Не)равенство треугольника: \\langle -\\nabla V_x(y), y-z\\rangle = V_x(z) - V_y(z) - V_x(y)\n\nПервые два свойства очевидны из определения. Докажем третье:\n\n\\begin{align*}\n\\langle -\\nabla V_x(y), y-z\\rangle &= \\langle \\nabla \\phi(x) - \\nabla \\phi(y) , y-z\\rangle =\\\\\n& = (\\phi(z) - \\phi(x) - \\langle \\nabla \\phi(x), z -x \\rangle) \\\\\n& - (\\phi(z) - \\phi(y) - \\langle \\nabla \\phi(y), z - y \\rangle) \\\\\n& - (\\phi(y) - \\phi(x) - \\langle \\nabla \\phi(x), y - x \\rangle) \\\\\n& = V_x(z) - V_y(z) - V_x(y)\n\\end{align*}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#алгоритм-зеркального-спуска-mirror-descent",
    "href": "docs/methods/fom/Mirror_descent.html#алгоритм-зеркального-спуска-mirror-descent",
    "title": "1 Возвращение к истокам",
    "section": "1.1 Алгоритм зеркального спуска (mirror descent):",
    "text": "1.1 Алгоритм зеркального спуска (mirror descent):\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right)\n\nИнтересные фишки:\n\nТакая же скорость сходимости, как и для метода проекции субградиента.\nРаботает в существенно более широком классе практических задач",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#онлайн-версия",
    "href": "docs/methods/fom/Mirror_descent.html#онлайн-версия",
    "title": "1 Возвращение к истокам",
    "section": "1.2 Онлайн версия",
    "text": "1.2 Онлайн версия\nСовершенно ясно, что в наших оценках на каждом шаге может быть новая функция f_k(x) на заданном классе. Поэтому, аналогичные оценки получаются и для онлайн постановки:\n\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq \\sqrt{2 M G^2 T}\n\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq \\sqrt{\\dfrac{2 M G^2}{T}}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#еще-одна-интерпретация",
    "href": "docs/methods/fom/Mirror_descent.html#еще-одна-интерпретация",
    "title": "1 Возвращение к истокам",
    "section": "1.3 Еще одна интерпретация",
    "text": "1.3 Еще одна интерпретация\nДавайте покажем, что полученный алгоритм имеет еще одну очень интуитивную интерпретацию:\n\ny_{k} = \\nabla \\phi(x_k) Отображение в сопряженное пространство с помощью функции \\nabla \\phi(x)\ny_{k+1} = y_k - \\alpha_k \\nabla f_k(x_k) Градиентный шаг в сопряженном пространстве\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S}V_{\\nabla \\phi^*(y_{k+1})}(x) Обратное отображение с помощью функции \\nabla \\phi^*(x) и проекция на бюджетное множество\n\n\n\n\nIllustration\n\n\nДля доказательства эквивалентности таких записей, следует сначала доказать факт того, что:\n\n\\left( \\nabla \\phi(x) \\right)^{-1} =  \\nabla \\phi^*(y)\n\nДля этого пусть y = \\nabla \\phi(x). Заметим, что для сопряженной функции справедливо неравенство Фенхеля - Юнга: \\phi^*(y) + \\phi(x) \\geq xy, в случае, если \\phi(x) - дифференцируема, такое преобразование называется преобразованием Лежандра и выполняется равенство: \\phi^*(y) + \\phi(x) = xy. Дифференцируя равенство по y, получаем \\nabla \\phi^*(y) = x. Таким образом,\n\n\\nabla\\phi^*(y) = \\nabla\\phi^*(\\nabla \\phi(x)) = x, \\qquad \\nabla\\phi(x) = \\nabla\\phi(\\nabla \\phi^*(y)) = y\n\nДоказательство:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{\\nabla \\phi^*(y_{k+1})}(x) \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x) - \\phi(\\nabla \\phi^*(y_{k+1})) - \\left\\langle \\nabla \\phi (\\nabla \\phi^*(y_{k+1})),x - \\nabla \\phi^*(y_{k+1})\\right\\rangle\\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x)  - \\left\\langle y_{k+1},x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x)  - \\left\\langle \\nabla \\phi(x_k) - \\alpha_k g_k,x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x) - \\phi(x_k) - \\left\\langle \\nabla \\phi(x_k),x \\right\\rangle + \\left\\langle \\alpha_k g_k,x \\right\\rangle  \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{x_k}(x) + \\left\\langle \\alpha_k g_k,x \\right\\rangle \\right\\}\n\\end{align*}\n\nВ последней строчке мы пришли к той формулировке, которую писали раньше. Заметим так же, еще одну интересную концепцию:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left(f(x_k)+  \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right)\n\\end{align*}\n\nЗдесь левая часть минимизируемого выражения представляет собой аппроксимацию первого порядка, а правая часть представляет собой проекционный член.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#direction-of-local-steepest-descent",
    "href": "docs/methods/fom/GD.html#direction-of-local-steepest-descent",
    "title": "1 Summary",
    "section": "2.1 Direction of local steepest descent",
    "text": "2.1 Direction of local steepest descent\nLet’s consider a linear approximation of the differentiable function f along some direction h, \\|h\\|_2 = 1:\n\nf(x + \\eta h) = f(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta)\n\nWe want h to be a decreasing direction:\n\nf(x + \\eta h) &lt; f(x)\n\n\nf(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta) &lt; f(x)\n\nand going to the limit at \\eta \\rightarrow 0:\n\n\\langle f'(x), h \\rangle \\leq 0\n\nAlso from Cauchy–Bunyakovsky–Schwarz inequality:\n\n|\\langle f'(x), h \\rangle | \\leq \\| f'(x) \\|_2 \\| h \\|_2 \\;\\;\\;\\to\\;\\;\\; \\langle f'(x), h \\rangle \\geq -\\| f'(x) \\|_2 \\| h \\|_2 = -\\| f'(x) \\|_2\n\nThus, the direction of the antigradient\n\nh = -\\dfrac{f'(x)}{\\|f'(x)\\|_2}\n\ngives the direction of the steepest local decreasing of the function f.\nThe result of this method is\n\nx_{k+1} = x_k - \\eta f'(x_k)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#gradient-flow-ode",
    "href": "docs/methods/fom/GD.html#gradient-flow-ode",
    "title": "1 Summary",
    "section": "2.2 Gradient flow ODE",
    "text": "2.2 Gradient flow ODE\nLet’s consider the following ODE, which is referred as Gradient Flow equation.\n\n\\tag{GF}\n\\frac{dx}{dt} = -f'(x(t))\n\nand discretize it on a uniform grid with \\eta step:\n\n\\frac{x_{k+1} - x_k}{\\eta} = -f'(x_k),\n\nwhere x_k \\equiv x(t_k) and \\eta = t_{k+1} - t_k - is the grid step.\nFrom here we get the expression for x_{k+1}\n\nx_{k+1} = x_k - \\eta f'(x_k),\n\nwhich is exactly gradient descent.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#necessary-local-minimum-condition",
    "href": "docs/methods/fom/GD.html#necessary-local-minimum-condition",
    "title": "1 Summary",
    "section": "2.3 Necessary local minimum condition",
    "text": "2.3 Necessary local minimum condition\n\n\\begin{align*}\n& f'(x) = 0\\\\\n& -\\eta f'(x) = 0\\\\\n& x - \\eta f'(x) = x\\\\\n& x_k - \\eta f'(x_k) = x_{k+1}\n\\end{align*}\n\nThis is, surely, not a proof at all, but some kind of intuitive explanation.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#minimizer-of-lipschitz-parabola",
    "href": "docs/methods/fom/GD.html#minimizer-of-lipschitz-parabola",
    "title": "1 Summary",
    "section": "2.4 Minimizer of Lipschitz parabola",
    "text": "2.4 Minimizer of Lipschitz parabola\nSome general highlights about Lipschitz properties are needed for explanation. If a function f: \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and its gradient satisfies Lipschitz conditions with constant L, then \\forall x,y \\in \\mathbb{R}^n:\n\n|f(y) - f(x) - \\langle \\nabla f(x), y-x \\rangle| \\leq \\frac{L}{2} \\|y-x\\|^2,\n\nwhich geometrically means, that if we’ll fix some point x_0 \\in \\mathbb{R}^n and define two parabolas:\n\n\\phi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2} \\|x-x_0\\|^2,\n\n\n\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2} \\|x-x_0\\|^2.\n\nThen\n\n\\phi_1(x) \\leq f(x) \\leq \\phi_2(x) \\quad \\forall x \\in \\mathbb{R}^n.\n\nNow, if we have global upper bound on the function, in a form of parabola, we can try to go directly to its minimum.\n\n\\begin{align*}\n& \\nabla \\phi_2(x) = 0 \\\\\n& \\nabla f(x_0) + L (x^* - x_0) = 0 \\\\\n& x^* = x_0 - \\frac{1}{L}\\nabla f(x_0) \\\\\n& x_{k+1} = x_k - \\frac{1}{L} \\nabla f(x_k)\n\\end{align*}\n\n\n\n\nIllustration\n\n\nThis way leads to the \\frac{1}{L} stepsize choosing. However, often the L constant is not known.\nBut if the function is twice continuously differentiable and its gradient has Lipschitz constant L, we can derive a way to estimate this constant \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x) \\| \\leq L\n\nor\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#constant-stepsize",
    "href": "docs/methods/fom/GD.html#constant-stepsize",
    "title": "1 Summary",
    "section": "3.1 Constant stepsize",
    "text": "3.1 Constant stepsize\nFor f \\in C_L^{1,1}:\n\n\\eta_k = \\eta\n\n\nf(x_k) - f(x_{k+1}) \\geq \\eta \\left(1 - \\frac{1}{2}L\\eta \\right) \\|\\nabla f(x_k)\\|^2\n\nWith choosing \\eta = \\frac{1}{L}, we have:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#fixed-sequence",
    "href": "docs/methods/fom/GD.html#fixed-sequence",
    "title": "1 Summary",
    "section": "3.2 Fixed sequence",
    "text": "3.2 Fixed sequence\n\n\\eta_k = \\dfrac{1}{\\sqrt{k+1}}\n\nThe latter 2 strategies are the simplest in terms of implementation and analytical analysis. It is clear that this approach does not often work very well in practice (the function geometry is not known in advance).",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#exact-line-search-aka-steepest-descent",
    "href": "docs/methods/fom/GD.html#exact-line-search-aka-steepest-descent",
    "title": "1 Summary",
    "section": "3.3 Exact line search aka steepest descent",
    "text": "3.3 Exact line search aka steepest descent\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_{k+1}) = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nMore theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.\nInteresting theoretical property of this method is that each following iteration is orthogonal to the previous one:\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nOptimality conditions:\n\n\\nabla f(x_{k+1})^\\top \\nabla f(x_k) = 0",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#goldstein-armijo",
    "href": "docs/methods/fom/GD.html#goldstein-armijo",
    "title": "1 Summary",
    "section": "3.4 Goldstein-Armijo",
    "text": "3.4 Goldstein-Armijo",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#convex-case",
    "href": "docs/methods/fom/GD.html#convex-case",
    "title": "1 Summary",
    "section": "4.1 Convex case",
    "text": "4.1 Convex case\n\n4.1.1 Lipischitz continuity of the gradient\nAssume that f: \\mathbb{R}^n \\to \\mathbb{R} is convex and differentiable, and additionally \n\\|\\nabla f(x) − \\nabla f(y) \\| \\leq L \\|x − y \\| \\; \\forall x, y \\in \\mathbb{R}^n\n\ni.e. , \\nabla f is Lipschitz continuous with constant L &gt; 0.\nSince \\nabla f Lipschitz with constant L, which means \\nabla^2 f \\preceq LI, we have \\forall x, y, z:\n\n(x − y)^\\top(\\nabla^2 f(z) − LI)(x − y) \\leq 0\n\n\n(x − y)^\\top\\nabla^2 f(z)(x − y) \\leq L \\|x-y\\|^2\n\nNow we’ll consider second order Taylor approximation of f(y) and Taylor’s Remainder Theorem (we assume, that the function f is continuously differentiable), we have \\forall x, y, \\exists z ∈ [x, y]:\n\n\\begin{align*}\nf(y) &= f(x) + \\nabla f(x)^\\top(y − x) + \\frac{1}{2}(x − y)^\\top \\nabla^2 f(z)(x − y) \\\\\n& \\leq f(x) + \\nabla f(x)^\\top(y − x) + \\frac{L}{2} \\|x-y\\|^2\n\\end{align*}\n\nFor the gradient descent we have x = x_k, y = x_{k+1}, x_{k+1} = x_k - \\eta_k\\nabla f(x_k):\n\n\\begin{align*}\nf(x_{k+1}) &\\leq  f(x_k) + \\nabla f(x_k)^\\top(-\\eta_k\\nabla f(x_k)) + \\frac{L}{2} \\| \\eta_k\\nabla f(x_k) \\|^2  \\\\\n& \\leq f(x_k) - \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\|\\nabla f(x_k)\\|^2\n\\end{align*}\n\n\n\n4.1.2 Optimal constant stepsize\nNow, if we’ll consider constant stepsize strategy and will maximize \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\to \\max\\limits_{\\eta}, we’ll get \\eta = \\dfrac{1}{L}.\n\nf(x_{k+1}) \\leq f(x_k) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2\n\n\n\n4.1.3 Convexity\n\nf(x_{k}) \\leq f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*)\n\nThat’s why we have:\n\n\\begin{align*}\nf(x_{k+1}) & \\leq  f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2 \\\\\n& = f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_k − x^* − \\dfrac{1}{L}\\nabla f(x_k)\\|^2\\right) \\\\\n& =  f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_{k+1} − x^*\\|^2\\right)\n\\end{align*}\n\nThus, summing over all iterations, we have:\n\n\\begin{align*}\n\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) &\\leq \\dfrac{L}{2} \\left(\\|x_0 − x^*\\|^2 − \\|x_{k} − x^*\\|^2\\right) \\\\\n& \\leq  \\dfrac{L}{2} \\|x_0 − x^*\\|^2 =  \\dfrac{LR^2}{2},\n\\end{align*}\n\nwhere R = \\|x_0 - x^*\\|. And due to function monotonicity:\n\nf(x_k) - f(x^*) \\leq \\dfrac{1}{k}\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) \\leq \\dfrac{LR^2}{2k} = \\dfrac{R^2}{2\\eta k}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#strongly-convex-case",
    "href": "docs/methods/fom/GD.html#strongly-convex-case",
    "title": "1 Summary",
    "section": "4.2 Strongly convex case",
    "text": "4.2 Strongly convex case\nIf the function is strongly convex:\n\nf(y) \\geq f(x) + \\nabla f(x)^\\top (y − x) + \\dfrac{\\mu}{2}\\|y − x \\|^2 \\; \\forall x, y \\in \\mathbb{R}^n\n\n…\n\n\\|x_{k+1} − x^*\\|^2 \\leq (1 − \\eta \\mu)\\|x_k − x^* \\|^2\n # Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert f(x_k) - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert x_k - x^* \\Vert \\leq\n\n\n\n\nConvexLipschitz-continuous function(G)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{GR}{k}\nSublinear\n\n\n\nConvexLipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{LR^2}{k}\nSublinear\n\n\n\n\\mu-Strongly convexLipschitz-continuous gradient(L)\n\nLinear\n(1 - \\eta \\mu)^k R^2\n\n\n\\mu-Strongly convexLipschitz-continuous hessian(M)\n\nLocally linear R &lt; \\overline{R}\n\\dfrac{\\overline{R}R}{\\overline{R} - R} \\left( 1 - \\dfrac{2\\mu}{L+3\\mu}\\right)\n\n\n\n\nR = \\| x_0 - x^*\\| - initial distance\n\\overline{R} = \\dfrac{2\\mu}{M}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/index.html",
    "href": "docs/methods/adaptive_metrics/index.html",
    "title": "",
    "section": "",
    "text": "It is known, that antigradient -\\nabla f (x_0) is the direction of the steepest descent of the function f(x) at point x_0. However, we can introduce another concept for choosing the best direction of function decreasing.\nGiven f(x) and a point x_0. Define B_\\varepsilon(x_0) = \\{x \\in \\mathbb{R}^n : d(x, x_0) = \\varepsilon^2 \\} as the set of points with distance \\varepsilon to x_0. Here we presume the existence of a distance function d(x, x_0).\n\nx^* = \\text{arg}\\min_{x \\in B_\\varepsilon(x_0)} f(x)\n\nThen, we can define another steepest descent direction in terms of minimizer of function on a sphere:\n\ns = \\lim_{\\varepsilon \\to 0} \\frac{x^* - x_0}{\\varepsilon}\n\nLet us assume that the distance is defined locally by some metric A:\n\nd(x, x_0) = (x-x_0)^\\top A (x-x_0)\n\nLet us also consider first order Taylor approximation of a function f(x) near the point x_0:\n\n\\tag{A1}\nf(x_0 + \\delta x) \\approx f(x_0) + \\nabla f(x_0)^\\top \\delta x\n\nNow we can explicitly pose a problem of finding s, as it was stated above.\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} f(x_0 + \\delta x) \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing \\text{(A1)} it can be written as:\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} \\nabla f(x_0)^\\top \\delta x \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing Lagrange multipliers method, we can easily conclude, that the answer is:\n\n\\delta x = - \\frac{2 \\varepsilon^2}{\\nabla f (x_0)^\\top A^{-1} \\nabla f (x_0)} A^{-1} \\nabla f\n\nWhich means, that new direction of steepest descent is nothing else, but A^{-1} \\nabla f(x_0).\nIndeed, if the space is isotropic and A = I, we immediately have gradient descent formula, while Newton method uses local Hessian as a metric matrix.\n\n\n\n\n\n\n\n\nNewton method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuasi Newton methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConjugate gradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural gradient descent\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Methods",
      "Adaptive metric methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html",
    "href": "docs/methods/adaptive_metrics/Newton.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\n\n\nIllustration\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}.\n\n\n\nLet us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\nIllustration\n\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\nYour browser does not support the video tag.\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#newtons-method-to-find-the-equation-roots",
    "href": "docs/methods/adaptive_metrics/Newton.html#newtons-method-to-find-the-equation-roots",
    "title": "1 Intuition",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\n\n\nIllustration\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#second-order-taylor-approximation-of-the-function",
    "href": "docs/methods/adaptive_metrics/Newton.html#second-order-taylor-approximation-of-the-function",
    "title": "1 Intuition",
    "section": "",
    "text": "Let us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\nIllustration\n\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\nYour browser does not support the video tag.\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#theorem",
    "href": "docs/methods/adaptive_metrics/Newton.html#theorem",
    "title": "1 Intuition",
    "section": "2.1 Theorem",
    "text": "2.1 Theorem\nLet f(x) be a strongly convex twice continuously differentiated function at \\mathbb{R}^n, for the second derivative of which inequalities are executed: \\mu I_n\\preceq f''(x) \\preceq L I_n. Then Newton’s method with a constant step locally converges to solving the problem with superlinear speed. If, in addition, Hessian is Lipschitz continuous, then this method converges locally to x^* at a quadratic rate.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#possible-directions",
    "href": "docs/methods/adaptive_metrics/Newton.html#possible-directions",
    "title": "1 Intuition",
    "section": "3.1 Possible directions",
    "text": "3.1 Possible directions\n\nNewton’s damped method (adaptive stepsize)\nQuasi-Newton methods (we don’t calculate the Hessian, we build its estimate - BFGS)\nQuadratic evaluation of the function by the first order oracle (superlinear convergence)\nThe combination of the Newton method and the gradient descent (interesting direction)\nHigher order methods (most likely useless)",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html",
    "href": "docs/methods/adaptive_metrics/CG.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Illustration\n\n\nOriginally, the conjugate gradients method was created to solve a system of linear equations.\n\nAx = b\n\nWithout special efforts the problem can be presented in the form of minimization of the quadratic function, and then generalized on a case of non quadratic function. We will start with the parabolic case and try to construct a conjugate gradients method for it. Let us consider the classical problem of minimization of the quadratic function:\n\nf(x) = \\frac{1}{2}x^\\top A x - b^\\top x + c \\to \\min\\limits_{x \\in \\mathbb{R}^n }\n\nHere x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{n \\times n}, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#example-1",
    "href": "docs/methods/adaptive_metrics/CG.html#example-1",
    "title": "1 Introduction",
    "section": "4.1 Example 1",
    "text": "4.1 Example 1\nProve that if a set of vectors d_1, \\ldots, d_k - are A-conjugate (each pair of vectors is A-conjugate), these vectors are linearly independent. A \\in \\mathbb{S}^n_{++}.\nSolution:\nWe’ll show, that if \\sum\\limits_{i=1}^k\\alpha_k d_k = 0, than all coefficients should be equal to zero:\n\n  \\begin{align*}\n  0 &= \\sum\\limits_{i=1}^n\\alpha_k d_k \\\\\n    &= d_j^\\top A \\left( \\sum\\limits_{i=1}^n\\alpha_k d_k \\right) \\\\\n    &=  \\sum\\limits_{i=1}^n \\alpha_k d_j^\\top A d_k  \\\\\n    &=  \\alpha_j d_j^\\top A d_j  + 0 + \\ldots + 0\\\\\n  \\end{align*}\n  \nThus, \\alpha_j = 0, for all other indices one have perform the same process",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html",
    "href": "docs/methods/Autograd.html",
    "title": "1 Problem",
    "section": "",
    "text": "Suppose we need to solve the following problem:\n\nL(w) \\to \\min_{w \\in \\mathbb{R}^d}\n\nSuch problems typically arise in machine learning, when you need to find optimal hyperparameters w of an ML model (i.e. train a neural network). You may use a lot of algorithms to approach this problem, but given the modern size of the problem, where d could be dozens of billions it is very challenging to solve this problem without information about the gradients using zero-order optimization algorithms. That is why it would be beneficial to be able to calculate the gradient vector \\nabla_w L = \\left( \\frac{\\partial L}{\\partial w_1}, \\ldots, \\frac{\\partial L}{\\partial w_d}\\right)^T. Typically, first-order methods perform much better in huge-scale optimization, while second-order methods require too much memory.",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#what-automatic-differentiation-ad-is-not",
    "href": "docs/methods/Autograd.html#what-automatic-differentiation-ad-is-not",
    "title": "1 Problem",
    "section": "4.1 What automatic differentiation (AD) is NOT:",
    "text": "4.1 What automatic differentiation (AD) is NOT:\n\nAD is not a finite differences\nAD is not a symbolic derivative\nAD is not just the chain rule\nAD is not just backpropagation\nAD (reverse mode) is time-efficient and numerically stable\nAD (reverse mode) is memory inefficient (you need to store all intermediate computations from the forward pass). :::\n\n\n\n\nDifferent approaches for taking derivatives",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#univariate-chain-rule",
    "href": "docs/methods/Autograd.html#univariate-chain-rule",
    "title": "1 Problem",
    "section": "5.1 Univariate chain rule",
    "text": "5.1 Univariate chain rule\nSuppose, we have the following functions R: \\mathbb{R} \\to \\mathbb{R} , L: \\mathbb{R} \\to \\mathbb{R} and W \\in \\mathbb{R}. Then\n\n\\dfrac{\\partial R}{\\partial W} = \\dfrac{\\partial R}{\\partial L} \\dfrac{\\partial L}{\\partial W}",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#multivariate-chain-rule",
    "href": "docs/methods/Autograd.html#multivariate-chain-rule",
    "title": "1 Problem",
    "section": "5.2 Multivariate chain rule",
    "text": "5.2 Multivariate chain rule\nThe simplest example:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), x_2(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\dfrac{\\partial f}{\\partial x_2} \\dfrac{\\partial x_2}{\\partial t}\n\nNow, we’ll consider f: \\mathbb{R}^n \\to \\mathbb{R}:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), \\ldots, x_n(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\ldots + \\dfrac{\\partial f}{\\partial x_n} \\dfrac{\\partial x_n}{\\partial t}\n\nBut if we will add another dimension f: \\mathbb{R}^n \\to \\mathbb{R}^m, than the j-th output of f will be:\n\n\\dfrac{\\partial }{\\partial t} f_j(x_1(t), \\ldots, x_n(t)) = \\sum\\limits_{i=1}^n \\dfrac{\\partial f_j}{\\partial x_i} \\dfrac{\\partial x_i}{\\partial t} = \\sum\\limits_{i=1}^n J_{ji}  \\dfrac{\\partial x_i}{\\partial t},\n\nwhere matrix J \\in \\mathbb{R}^{m \\times n} is the jacobian of the f. Hence, we could write it in a vector way:\n\n\\dfrac{\\partial f}{\\partial t} = J \\dfrac{\\partial x}{\\partial t}\\quad \\iff \\quad \\left(\\dfrac{\\partial f}{\\partial t}\\right)^\\top =  \\left( \\dfrac{\\partial x}{\\partial t}\\right)^\\top J^\\top",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#backpropagation",
    "href": "docs/methods/Autograd.html#backpropagation",
    "title": "1 Problem",
    "section": "5.3 Backpropagation",
    "text": "5.3 Backpropagation\nBackpropagation is a specific application of reverse-mode automatic differentiation within neural networks. It is the standard algorithm for computing gradients in neural networks, especially for training with stochastic gradient descent. Here’s how it works:\n\nPerform a forward pass through the network to compute activations and outputs.\nCalculate the loss function at the output, which measures the difference between the network prediction and the actual target values.\nCommence the backward pass by computing the gradient of the loss with respect to the network’s outputs.\nPropagate these gradients back through the network, layer by layer, using the chain rule to calculate the gradients of the loss with respect to each weight and bias.\nThe critical point of backpropagation is that it efficiently calculates the gradient of a complex, multilayered function by decomposing it into simpler derivative calculations. This aspect makes the update of a large number of parameters in deep networks computationally feasible.",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#jacobian-vector-product",
    "href": "docs/methods/Autograd.html#jacobian-vector-product",
    "title": "1 Problem",
    "section": "5.4 Jacobian vector product",
    "text": "5.4 Jacobian vector product\nThe power of automatic differentiation is encapsulated in the computation of the Jacobian-vector product. Instead of calculating the entire Jacobian matrix, which is computationally expensive and often unnecessary, AD computes the product of the Jacobian and a vector directly. This is crucial for gradients in neural networks where the Jacobian may be very large, but the end goal is the product of this Jacobian with the gradient of the loss with respect to the outputs (vector). The reason why it works so fast in practice is that the Jacobian of the operations is already developed effectively in automatic differentiation frameworks. Typically, we even do not construct or store the full Jacobian, doing matvec directly instead. Note, for some functions (for example, any element-wise function of the input vector) matvec costs linear time, instead of quadratic and requires no additional memory to store a Jacobian.\n\n\n\n\n\n\nExample: element-wise exponent\n\n\n\n\n\n\ny = \\exp{(z)} \\qquad J = \\text{diag}(\\exp(z)) \\qquad \\overline{z} = \\overline{y} J\n\n\n\n\n\nSee the examples of Vector-Jacobian Products from the autodidact library:\ndefvjp(anp.add,         lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, g))\ndefvjp(anp.multiply,    lambda g, ans, x, y : unbroadcast(x, y * g),\n                        lambda g, ans, x, y : unbroadcast(y, x * g))\ndefvjp(anp.subtract,    lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, -g))\ndefvjp(anp.divide,      lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\ndefvjp(anp.true_divide, lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#hessian-vector-product",
    "href": "docs/methods/Autograd.html#hessian-vector-product",
    "title": "1 Problem",
    "section": "5.5 Hessian vector product",
    "text": "5.5 Hessian vector product\nInterestingly, a similar idea could be used to compute Hessian-vector products, which is essential for second-order optimization or conjugate gradient methods. For a scalar-valued function f : \\mathbb{R}^n \\to \\mathbb{R} with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point x \\in \\mathbb{R}^n is written as \\partial^2 f(x). A Hessian-vector product function is then able to evaluate\n\nv \\mapsto \\partial^2 f(x) \\cdot v\n\nfor any vector v \\in \\mathbb{R}^n.\nThe trick is not to instantiate the full Hessian matrix: if n is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store. Luckily, grad (in the jax/autograd/pytorch/tensorflow) already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity\n\n\\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x),\n\nwhere g(x) = \\partial f(x) \\cdot v is a new vector-valued function that dots the gradient of f at x with the vector v. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know grad is efficient.\nimport jax.numpy as jnp\n\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/materials/index.html",
    "href": "docs/materials/index.html",
    "title": "1 Books",
    "section": "",
    "text": "1 Books\n\n📝 File📚 bibtex\n\n\nUniversal gradient descent. Alexander Gasnikov - (in Russian) - probably, the most comprehensive book on the modern numerical methods, which covers a lot of theoretical and practical aspects of mathematical programming.\n\n\n@article{gasnikov2017universal,\n  title={Universal gradient descent},\n  author={Gasnikov, Alexander},\n  journal={arXiv preprint arXiv:1711.00394},\n  year={2017}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization: Algorithms and Complexity by Sébastien Bubeck\n\n\n@article{bubeck2015convex,\n  title={Convex optimization: Algorithms and complexity},\n  author={Bubeck, Sébastien and others},\n  journal={Foundations and Trends® in Machine Learning},\n  volume={8},\n  number={3-4},\n  pages={231--357},\n  year={2015},\n  publisher={Now Publishers, Inc.}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization materials by Stephen Boyd and Lieven Vandenberghe\n\n\n@book{boyd2004convex,\n  title={Convex optimization},\n  author={Boyd, Stephen and Vandenberghe, Lieven},\n  year={2004},\n  publisher={Cambridge university press}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright\n\n\n@book{nocedal2006numerical,\n  title={Numerical optimization},\n  author={Nocedal, Jorge and Wright, Stephen},\n  year={2006},\n  publisher={Springer Science & Business Media}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nLectures on Convex Optimization by Yurii Nesterov\n\n\n@book{nesterov2018lectures,\n  title={Lectures on convex optimization},\n  author={Nesterov, Yurii},\n  volume={137},\n  publisher={Springer}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nMinimum-volume ellipsoids\n\n\n@book{todd2016minimum,\n  title={Minimum-volume ellipsoids: Theory and algorithms},\n  author={Todd, Michael J},\n  year={2016},\n  publisher={SIAM}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть I. Введение в выпуклый анализ и теорию оптимизации\n\n\n@article{жадан2014методы,\n  title={Методы оптимизации. Часть 1. Введение в выпуклый анализ и теорию оптимизации: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2014}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть II. Численные алгоритмы\n\n\n@article{жадан2015методы,\n  title={Методы оптимизации. Часть 2. Численные алгоритмы: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2015}\n}\n\n\n\n\n\n2 Courses\n\nConvex Optimization and Approximation course by Moritz Hardt @ UC Berkley.\nConvex Optimization course by Ryan Tibshirani @ CMU.\nConvex Optimization course by Lieven Vandenberghe @ UCLA.\nConvex Optimization course by Suvrit Sra @ UC Berkley.\nAdvanced Optimization and Randomized Methods course by Alex Smola and Suvrit Sra @ CMU.\nOptimizaion methods course by Alexandr Katrutsa @ MIPT.\nConvex Analysis and Optimization course by Dimitri Bertsekas @ MIT.\nOptimization for Machine Learning course by Martin Jaggi @ EPFL.\nOptimization for Machine Learning course by Suvrit Sra.\nМетоды оптимизации lectures by Alexander Gasnikov @ MIPT.\nМетоды оптимизации seminars by Daniil Merkulov @ MIPT.\n\n\n\n3 Blogs and personal pages\n\nI’m a bandit blog by Sébastien Bubeck.\nBlog by Moritz Hardt.\nBlog by Sebastian Pokutta with great cheat sheets on optimization.\nBlog by Sebastian Ruder about NLP and optimization.\nPersonal page of Peter Richtarik with announcements and news.\nPersonal page of Suvrit Sra.\nBlog by Fabian Pedregosa.\nBlog with beatiful insights about modern non-convex optimization.\nMachine Learning Research Blog by Francis Bach.\n\n\n\n4 Software and apps\n\nSci hub telegram bot allows you to access almost all the scientific papers in one click.\n\n\n\n5 Other\n\nNice set of python applied math etudes\nExample functions to test optimization algorithms.\n100 numpy exercises\nCollection of Interactive Machine Learning Examples\nML Python libraries overview (Russ)\nNice Visualisation of some ML ideas\nAn Interactive Tutorial on Numerical Optimization",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/exercises/uncategorized.html",
    "href": "docs/exercises/uncategorized.html",
    "title": "Uncategorized",
    "section": "",
    "text": "Uncategorized\n\nShow, that these conditions are equivalent:\n\n  \\|\\nabla f(x) - \\nabla f(z) \\| \\le L \\|x-z\\|\n\nand\n\nf(z) \\le f(x) + \\nabla f(x)^T(z-x) + \\frac L 2 \\|z-x\\|^2\n\nWe say that the function belongs to the class f \\in C^{k,p}_L (Q) if it is k times continuously differentiable on Q, and the p derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} for \\mathbb{R}^n. Notice that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher is the order of the derivative, the stronger is the limitation (fewer functions belong to the class).\n\nProve that the function belongs to the class C_L^{2,1}. \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x)\\| \\leq L\n\nProve that the last condition can be rewritten in the form without loss of generality:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nShow that for gradient descent with the following stepsize selection strategies:\n\nconstant step h_k = \\dfrac{1}{L}\nDropping sequence h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0.\n\nyou can get the estimation of the function decrease at the iteration of the view:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - some constant, L - Lipschitz constant of the function gradient.",
    "crumbs": [
      "Exercises",
      "Uncategorized"
    ]
  },
  {
    "objectID": "docs/exercises/separation.html",
    "href": "docs/exercises/separation.html",
    "title": "Separation",
    "section": "",
    "text": "Separation\n\nLet S_1, S_2 be closed convex sets such that: S_1 \\cap S_2 = \\varnothing. Is it true that \\exists p: (p,x) &lt; (p,y) \\;\\; \\forall x \\in S_1, \\forall y \\in S_2\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_1 x_2 \\ge 1, x_1 &gt; 0\\right\\},\\quad S_2 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_2 \\le \\frac{4}{x_1 - 1} +9\\right\\}\n\nFind a supporting hyperplane for a set of S = \\left\\{ x \\in \\mathbb{R}^2 \\mid e^{x_1} \\le x_2\\right\\} at the boundary point x_0 = (0, 1)\nFind a supporting hyperplane for the set of S = \\left\\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2\\right\\} such that separates it from the point x_0 = \\left(-\\frac{5}{4}, \\frac{5}{16}, \\frac{15}{16}\\right)\nПриведите пример двух строго, но не сильно отделимых множеств. Двух отделимых, но не собственно отделимых множеств.\nIllustrate the difference between the tangent hyperplane and the separating hyperplane by considering a convex set with a non-smooth boundary.\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid \\dfrac{x_1^2}{4} + \\dfrac{x_2^2}{9} + \\dfrac{x_3^2}{25} \\le 1 \\} at (-6/5, 12/5, 0), (0, 9/5, 4), (6/5, 0, -4) (any choice)\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2 \\}, which separates it from the points (5/4, 5/16, 15/16), (4/3, 2/3, 13/12), (11/9, 11/27, 1) (any choice)\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_n^2 \\le 1\\right\\}, \\;\\;\\; S_2 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_{n-1}^2 + 1 \\le x_n \\right\\}\n\nFind a supporting hyperplane for a set S = \\left\\{ x \\in \\mathbb{R}^3 \\mid \\frac{x_1^2}{4}+\\frac{x_2^2}{8}+\\frac{x_3^2}{25} \\le 1 \\right\\} at the border point x_0 = \\left(-1, \\frac{12}{5}, \\frac{\\sqrt{3}}{2}\\right)",
    "crumbs": [
      "Exercises",
      "Separation"
    ]
  },
  {
    "objectID": "docs/exercises/matrix_calculus.html",
    "href": "docs/exercises/matrix_calculus.html",
    "title": "Matrix calculus",
    "section": "",
    "text": "Matrix calculus\n\nFind the derivatives of f(x) = Ax, \\quad \\nabla_x f(x) = ?, \\nabla_A f(x) = ?\nFind \\nabla f(x), if f(x) = c^Tx.\nFind \\nabla f(x), if f(x) = \\dfrac{1}{2}x^TAx + b^Tx + c.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = -e^{-x^Tx}.\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert ^2_2.\nFind \\nabla f(x), if f(x) = \\Vert x\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), if f(x) = \\Vert Ax\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = \\dfrac{-1}{1 + x^T x}.\nCalculate df(x) and \\nabla f(x) for the function f(x) = \\log(x^{T}\\mathrm{A}x).\nFind f^\\prime(X), if f(X) = \\det X\nNote: here under f^\\prime(X) assumes first order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X)\n\nFind f^{\\prime\\prime}(X), if f(X) = \\log \\det X\nNote: here under f^{\\prime\\prime}(X) assumes second order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X) + \\frac{1}{2}\\mathbf{tr}(\\Delta X^T f^{\\prime\\prime}(X) \\Delta X)\n\nFind gradient and hessian of f : \\mathbb{R}^n \\to \\mathbb{R}, if:\n\nf(x) = \\log \\sum\\limits_{i=1}^m \\exp (a_i^T x + b_i), \\;\\;\\;\\; a_1, \\ldots, a_m \\in \\mathbb{R}^n; \\;\\;\\;  b_1, \\ldots, b_m  \\in \\mathbb{R}\n\nWhat is the gradient, Jacobian, Hessian? Is there any connection between those three definitions?\nCalculate: \\dfrac{\\partial }{\\partial X} \\sum \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X} \\prod \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X}\\text{tr}(X), \\;\\; \\dfrac{\\partial }{\\partial X} \\text{det}(X)\nCalculate the Frobenious norm derivative: \\dfrac{\\partial}{\\partial X}\\Vert X\\Vert _F^2\nCalculate the gradient of the softmax regression \\nabla_\\theta L in binary case (K = 2) n - dimensional objects:\n\nh_\\theta(x) = \\begin{bmatrix} P(y = 1 \\vert x; \\theta) \\\\ P(y = 2 \\vert x; \\theta) \\\\ \\vdots \\\\ P(y = K \\vert x; \\theta) \\end{bmatrix} = \\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)T} x) }} \\begin{bmatrix} \\exp(\\theta^{(1)T} x ) \\\\ \\exp(\\theta^{(2)T} x ) \\\\ \\vdots \\\\ \\exp(\\theta^{(K)T} x ) \\\\ \\end{bmatrix}\n\n\nL(\\theta) = - \\left[ \\sum_{i=1}^n  (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right]\n\nFind \\nabla f(X), if f(X) = \\text{tr } AX\nFind \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X\nFind \\nabla f(X), if f(X) = \\ln \\langle Ax, x\\rangle, A \\in \\mathbb{S^n_{++}}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{3}\\Vert x\\Vert _2^3\nCalculate \\nabla f(X), if f(X) = \\Vert AX - B\\Vert _F, X \\in \\mathbb{R}^{k \\times n}, A \\in \\mathbb{R}^{m \\times k}, B \\in \\mathbb{R}^{m \\times n}\nCalculate the derivatives of the loss function with respect to parameters \\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial b} for the single object x_i (or, n = 1)\n\n\n\nIllustration\n\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\langle x, x\\rangle^{\\langle x, x\\rangle}, x \\in \\mathbb{R}^p \\setminus \\{0\\}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{\\langle Ax, x\\rangle}{\\Vert x\\Vert _2^2}, x \\in \\mathbb{R}^p \\setminus \\{0\\}, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert ^2_F, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\Vert xx^T\\Vert _2\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac1n \\sum\\limits_{i=1}^n \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i \\in \\mathbb R^n, \\; \\mu&gt;0.\nMatch functions with their gradients:\n\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}^{-1}\nf(\\mathrm{X}) = \\det \\mathrm{X}\nf(\\mathrm{X}) = \\ln \\det \\mathrm{X}\n\n\n\\nabla f(\\mathrm{X}) = \\mathrm{X}^{-1}\n\\nabla f(\\mathrm{X}) = \\mathrm{I}\n\\nabla f(\\mathrm{X}) = \\det (\\mathrm{X})\\cdot (\\mathrm{X}^{-1})^{T}\n\\nabla f(\\mathrm{X}) = -\\left(\\mathrm{X}^{-2}\\right)^{T}\n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A − tI_n),\n\nwhere A \\in \\mathbb{R}^{n \\times n}, S := \\{t \\in \\mathbb{R} : \\text{det}(A − tI_n) \\neq 0\\}.\nFind the gradient \\nabla f(x), if f(x) = \\text{tr}\\left( AX^2BX^{-T} \\right).",
    "crumbs": [
      "Exercises",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/exercises/index.html",
    "href": "docs/exercises/index.html",
    "title": "",
    "section": "",
    "text": "Files and links should be added.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "docs/exercises/fom.html",
    "href": "docs/exercises/fom.html",
    "title": "First order methods",
    "section": "",
    "text": "First order methods\n\nA function is said to belong to the class f \\in C^{k,p}_L (Q) if it k times is continuously differentiable on Q and the pth derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} on \\mathbb{R}^n. Note that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher the order of the derivative, the stronger the constraint (fewer functions belong to the class)\n\nProve that the function belongs to the class C_L^{2,1} \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\||\\nabla^2 f(x)\\| \\leq L\n\nProve also that the last condition can be rewritten, without generality restriction, as follows:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nNote: by default the Euclidean norm is used for vectors and the spectral norm is used for matrices.\nПокажите, что с помощью следующих стратегий подбора шага в градиентному спуске:\n\nПостоянный шаг h_k = \\dfrac{1}{L}\nУбывающая последовательность h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0\n\nможно получить оценку убывания функции на итерации вида:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - некоторая константа, L - константа Липщица градиента функции\nРассмотрим функцию двух переменных:\n\nf(x_1, x_2) = x_1^2 + k x_2^2,\n\nгде k - некоторый параметр. Постройте график количества итераций, необходимых для сходимости алгоритма наискорейшего спуска (до выполнения условия \\|\\nabla f(x_k)\\| \\leq \\varepsilon = 10^{-7}) в зависимости от значения k. Рассмотрите интервал k \\in [10^{-3}; 10^3] (будет удобно использовать функцию ks = np.logspace(-3,3)) и строить график по оси абсцисс в логарифмическом масштабе plt.semilogx() или plt.loglog() для двойного лог. масштаба.\nСделайте те же графики для функции:\n\nf(x) = \\ln(1 + e^{x^\\top A x}) + \\mathbf{1}^\\top x\n\nОбъясните полученную зависимость.\nДля наглядности можете пользоваться кодом отрисовки картинок:\ndef f_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    x_1, x_2 = x\n    return x_1**2 + k*x_2**2\n\ndef df_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    return np.array([2*x[0], 2*k*x[1]])\n\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\ndef plot_3d_function(x1, x2, f, title, *f_params, minima = None, iterations = None):\n    '''\n    '''\n    low_lim_1 = x1.min()\n    low_lim_2 = x2.min()\n    up_lim_1  = x1.max()\n    up_lim_2  = x2.max()\n\n    X1,X2 = np.meshgrid(x1, x2) # grid of point\n    Z = f((X1, X2), *f_params) # evaluation of the function on the grid\n\n    # set up a figure twice as wide as it is tall\n    fig = plt.figure(figsize=(16,7))\n    fig.suptitle(title)\n\n    #===============\n    #  First subplot\n    #===============\n    # set up the axes for the first plot\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n\n    # plot a 3D surface like in the example mplot3d/surface3d_demo\n    surf = ax.plot_surface(X1, X2, Z, rstride=1, cstride=1, \n                        cmap=cm.RdBu,linewidth=0, antialiased=False)\n\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, f(minima_), 'r*', markersize=10)\n\n\n\n    #===============\n    # Second subplot\n    #===============\n    # set up the axes for the second plot\n    ax = fig.add_subplot(1, 2, 2)\n\n    # plot a 3D wireframe like in the example mplot3d/wire3d_demo\n    im = ax.imshow(Z,cmap=plt.cm.RdBu,  extent=[low_lim_1, up_lim_1, low_lim_2, up_lim_2])\n    cset = ax.contour(x1, x2,Z,linewidths=2,cmap=plt.cm.Set2)\n    ax.clabel(cset,inline=True,fmt='%1.1f',fontsize=10)\n    fig.colorbar(im)\n    ax.set_xlabel(f'$x_1$')\n    ax.set_ylabel(f'$x_2$')\n\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, 'r*', markersize=10)\n\n    if iterations is not None:\n        for point in iterations:\n            ax.plot(*point, 'go', markersize=3)\n        iterations = np.array(iterations).T\n        ax.quiver(iterations[0,:-1], iterations[1,:-1], iterations[0,1:]-iterations[0,:-1], iterations[1,1:]-iterations[1,:-1], scale_units='xy', angles='xy', scale=1, color='blue')\n\n    plt.show()\n\nup_lim  = 4\nlow_lim = -up_lim\nx1 = np.arange(low_lim, up_lim, 0.1)\nx2 = np.arange(low_lim, up_lim, 0.1)\nk=0.5\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0])\n\nfrom scipy.optimize import minimize_scalar\n\ndef steepest_descent(x_0, f, df, *f_params, df_eps = 1e-2, max_iter = 1000):\n    iterations = []\n    x = np.array(x_0)\n    iterations.append(x)\n    while np.linalg.norm(df(x, *f_params)) &gt; df_eps and len(iterations) &lt;= max_iter:\n        res = minimize_scalar(lambda alpha: f(x - alpha * df(x, *f_params), *f_params))\n        alpha_opt = res.x\n        x = x - alpha_opt * df(x, *f_params)\n        iterations.append(x)\n    print(f'Finished with {len(iterations)} iterations')\n    return iterations\n\nx_0 = [10,1]\nk = 30\niterations = steepest_descent(x_0, f_6, df_6, k, df_eps = 1e-9)\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0], iterations = iterations)\nSolve the Hobbit Village problem. Open In Colab\nSolve the problem of constrained optimization using projected gradient descent Open In Colab",
    "crumbs": [
      "Exercises",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/exercises/cvxpy.html",
    "href": "docs/exercises/cvxpy.html",
    "title": "CVXPY library",
    "section": "",
    "text": "CVXPY library\n\nConstrained linear least squares Solve the following problem with cvxpy library.\n\n\\begin{split} &\\|X \\theta - y\\|^2_2 \\to \\min\\limits_{\\theta \\in \\mathbb{R}^{n} } \\\\ \\text{s.t. } & 0_n \\leq \\theta \\leq 1_n \\end{split}\n\nLinear programming A linear program is an optimization problem with a linear objective and affine inequality constraints. A common standard form is the following:\n  \n     \\begin{array}{ll}\n     \\text{minimize}   & c^Tx \\\\\n     \\text{subject to} & Ax \\leq b.\n     \\end{array}\n\nHere A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, and c \\in \\mathbb{R}^n are problem data and x \\in \\mathbb{R}^{n} is the optimization variable. The inequality constraint Ax \\leq b is elementwise. Solve this problem with cvxpy library.\nList the installed solvers in cvxpy using cp.installed_solvers() method.\nSolve the following optimization problem using CVXPY:\n\n\\begin{array}{ll}\n\\text{minimize} & |x| - 2\\sqrt{y}\\\\\n\\text{subject to} & 2 \\geq e^x \\\\\n& x + y = 5,\n\\end{array}\n\nwhere x,y \\in \\mathbb{R} are variables. Find the optimal values of x and y.\nRisk budget allocation Suppose an amount x_i&gt;0 is invested in n assets, labeled i=1,..., n, with asset return covariance matrix \\Sigma \\in \\mathcal{S}_{++}^n. We define the risk of the investments as the standard deviation of the total return\n\nR(x) = (x^T\\Sigma x)^{1/2}.\n\nWe define the (relative) risk contribution of asset i (in the portfolio x) as\n\n\\rho_i = \\frac{\\partial \\log R(x)}{\\partial \\log x_i} =\n\\frac{\\partial R(x)}{R(x)} \\frac{x_i}{\\partial x_i}, \\quad i=1, \\ldots, n.\n\nThus \\rho_i gives the fractional increase in risk per fractional increase in investment i. We can express the risk contributions as\n\n\\rho_i = \\frac{x_i (\\Sigma x)_i} {x^T\\Sigma x}, \\quad i=1, \\ldots, n,\n\nfrom which we see that \\sum_{i=1}^n \\rho_i = 1. For general x, we can have \\rho_i &lt;0, which means that a small increase in investment i decreases the risk. Desirable investment choices have \\rho_i&gt;0, in which case we can interpret \\rho_i as the fraction of the total risk contributed by the investment in asset i. Note that the risk contributions are homogeneous, i.e., scaling x by a positive constant does not affect \\rho_i.\n\nProblem statement: In the risk budget allocation problem, we are given \\Sigma and a set of desired risk contributions \\rho_i^\\mathrm{des}&gt;0 with \\bf{1}^T \\rho^\\mathrm{des}=1; the goal is to find an investment mix x\\succ 0, \\bf{1}^Tx =1, with these risk contributions. When \\rho^\\mathrm{des} = (1/n)\\bf{1}, the problem is to find an investment mix that achieves so-called risk parity.\na) Explain how to solve the risk budget allocation problem using convex optimization. Hint. Minimize (1/2)x^T\\Sigma x - \\sum_{i=1}^n \\rho_i^\\mathrm{des} \\log x_i.\nb) Find the investment mix that achieves risk parity for the return covariance matrix \\Sigma below.\nimport numpy as np\nimport cvxpy as cp\nSigma = np.array(np.matrix(\"\"\"6.1  2.9  -0.8  0.1;\n                     2.9  4.3  -0.3  0.9;\n                    -0.8 -0.3   1.2 -0.7;\n                     0.1  0.9  -0.7  2.3\"\"\"))\nrho = np.ones(4)/4\n\n\n\n\n1 Materials\n\nCVXPY exercises\nAdditional Exercises for Convex Optimization",
    "crumbs": [
      "Exercises",
      "CVXPY library"
    ]
  },
  {
    "objectID": "docs/exercises/convex_functions.html",
    "href": "docs/exercises/convex_functions.html",
    "title": "Convex functions",
    "section": "",
    "text": "Convex functions\n\nShow, that f(x) = \\|x\\| is convex on \\mathbb{R}^n.\nShow, that f(x) = c^\\top x + b is convex and concave.\nShow, that f(x) = x^\\top Ax, where A\\succeq 0 - is convex on \\mathbb{R}^n.\nShow, that f(A) = \\lambda_{max}(A) - is convex, if A \\in S^n.\nProve, that -\\log\\det X is convex on X \\in S^n_{++}.\nShow, that f(x) is convex, using first and second order criteria, if f(x) = \\sum\\limits_{i=1}^n x_i^4.\nFind the set of x \\in \\mathbb{R}^n, where the function f(x) = \\dfrac{-1}{2(1 + x^\\top x)} is convex, strictly convex, strongly convex?\n\nFind the values of a,b,c, where f(x,y,z) = x^2 + 2axy + by^2 + cz^2 is convex, strictly convex, strongly convex?\nВыпуклы ли следующие функции: f(x) = e^x - 1, \\; x \\in \\mathbb{R};\\;\\;\\; f(x_1, x_2) = x_1x_2, \\; x \\in \\mathbb{R}^2_{++};\\;\\;\\; f(x_1, x_2) = 1/(x_1x_2), \\; x \\in \\mathbb{R}^2_{++}?\nДокажите, что множество S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\prod\\limits_{i=1}^n x_i \\geq 1 \\right\\} выпукло.\nProve, that function f(X) = \\mathbf{tr}(X^{-1}), X \\in S^n_{++} is convex, while g(X) = (\\det X)^{1/n}, X \\in S^n_{++} is concave.\nKullback–Leibler divergence between p,q \\in \\mathbb{R}^n_{++} is:\n\nD(p,q) = \\sum\\limits_{i=1}^n (p_i \\log(p_i/q_i) - p_i + q_i)\n\nProve, that D(p,q) \\geq 0 \\forall p,q \\in \\mathbb{R}^n_{++} and D(p,q) = 0 \\leftrightarrow p = q\nHint: \nD(p,q) = f(p) - f(q) - \\nabla f(q)^\\top (p-q), \\;\\;\\;\\; f(p) = \\sum\\limits_{i=1}^n p_i \\log p_i\n\nLet x be a real variable with the values a_1 &lt; a_2 &lt; \\ldots &lt; a_n with probabilities \\mathbb{P}(x = a_i) = p_i. Derive the convexity or concavity of the following functions from p on the set of \\left\\{p \\mid \\sum\\limits_{i=1}^n p_i = 1, p_i \\ge 0 \\right\\}\n\n\\mathbb{E}x\n\\mathbb{P}\\{x \\ge \\alpha\\}\n\\mathbb{P}\\{\\alpha \\le x \\le \\beta\\}\n\\sum\\limits_{i=1}^n p_i \\log p_i​\n\\mathbb{V}x = \\mathbb{E}(x - \\mathbb{E}x)^2\n\\mathbf{quartile}(x) = {\\operatorname{inf}}\\left\\{ \\beta \\mid \\mathbb{P}\\{x \\le \\beta\\} \\ge 0.25 \\right\\}\n\nОпределения выпуклости и сильной выпуклости. Критерии выпуклости и сильной выпуклости первого и второго порядков\nГеометрическая интерпретация выпуклости и сильной выпуклости. (подпирание прямой и параболой)\nПриведите различные три операции, сохраняющие выпуклость функции.\nДоказать, что для a,b \\ge 0; \\;\\;\\; \\theta \\in [0,1]\n\n- \\log \\left( \\dfrac{a+b}{2}\\right) \\le -\\dfrac{\\log a + \\log b}{2}\na^\\theta b^{1-\\theta} \\le \\theta a + (1 - \\theta)b\nHölder’s inequality: \\sum\\limits_{i=1}^n x_i y_i \\le \\left( \\sum\\limits_{i=1}^n \\vert x_i\\vert ^p\\right)^{1/p} \\left( \\sum\\limits_{i=1}^n \\vert y_i\\vert^p\\right)^{1/p}. For p &gt;1, \\;\\; \\dfrac{1}{p} + \\dfrac{1}{q} = 1.\n\nFor x, y \\in \\mathbb{R}^n\nДоказать, что матричная норма f(X) = \\|X\\|_2 = \\sup\\limits_{y \\in \\mathbb{R}^n} \\dfrac{\\|Xy\\|_2}{\\|y\\|_2} выпукла.\nДоказать, что:\n\nесли f(x) - выпукла, то \\exp f(x) также выпукла.\nесли f(x) - выпукла, то g(x)^p выпукла для p \\ge 1, f(x) \\ge 0.\nесли f(x) - вогнута, то 1/f(x) выпукла для f(x) &gt; 0.\n\nВыпукла ли функция f(X, y) = y^T X^{-1}y на множестве \\mathbf{dom} f = \\{X, y \\mid X + X^T \\succeq 0\\} ? Известно, что эта функция выпукла, если X - симметричная матрица (упражнение - доказать). Докажите выпуклость или приведите простой контрпример.\nПусть функция h(x) - выпуклая на \\mathbb{R} неубывающая функция, кроме того: h(x) = 0 при x \\le 0. Докажите, что функция h\\left(\\|x\\|_2\\right) выпукла на \\mathbb{R}^n.\nIs the function returning the arithmetic mean of vector coordinates is a convex one: a(x) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i, what about geometric mean: g(x) = \\prod\\limits_{i=1}^n \\left(x_i \\right)^{1/n}?\nShow, that the following function is convex on the set of all positive denominators\n\nf(x) = \\dfrac{1}{x_1 - \\dfrac{1}{x_2 - \\dfrac{1}{x_3 - \\dfrac{1}{\\ldots}}}}, x \\in \\mathbb{R}^n\n\nВлияют ли линейные члены квадратичной функции на ее выпуклость? Сильную выпуклость?\nПусть f(x) : \\mathbb{R}^n \\to \\mathbb{R} такова, что \\forall x,y \\to f\\left( \\dfrac{x+y}{2}\\right) \\leq \\dfrac{1}{2}(f(x)+f(y)). Является ли такая функция выпуклой?\nFind the set, on which the function f(x,y) = e^{xy} will be convex.\nStudy the following function of two variables f(x,y) = e^{xy}.\n\nIs this function convex?\nProve, that this function will be convex on the line x = y.\nFind another set in \\mathbb{R}^2, on which this function will be convex.\n\nIs f(x) = -x \\ln x - (1-x) \\ln (1-x) convex?\nProve, that adding \\lambda \\|x\\|_2^2 to any convex function g(x) ensures strong convexity of a resulting function f(x) = g(x) + \\lambda \\|x\\|_2^2. Find the constant of the strong convexity \\mu.\nProve, that function\n\nf(x) = \\log\\left( \\sum\\limits_{i=1}^n e^{x_i}\\right)\n\nis convex using any differential criterion.\nProve, that a function f is strongly convex with parameter \\mu if and only if the function \nx \\mapsto f(x)- \\frac{\\mu}{2} \\|x\\|^{2}\n is convex.\nGive an example of a function, that satisfies Polyak Lojasiewicz condition, but doesn’t have convexity property.\nProve, that if g(x) - convex function, then f(x) = g(x) + \\dfrac{\\lambda}{2}\\|x\\|^2_2 will be strongly convex function.\nFind then f(x) = x^T A x is strongly convex and find strong convexity constant.\nLet f: \\mathbb{R}^n \\to \\mathbb{R} be the following function: \nf(x) = \\sum\\limits_{i=1}^k x_{\\lfloor i \\rfloor},\n where 1 \\leq k \\leq n, while the symbol x_{\\lfloor i \\rfloor} stands for the i-th component of sorted (x_{\\lfloor 1 \\rfloor} - maximum component of x and x_{\\lfloor n \\rfloor} - minimum component of x) vector of x. Show, that f is a convex function.\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\mu \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.",
    "crumbs": [
      "Exercises",
      "Convex functions"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_sets.html",
    "href": "docs/exercises/conjugate_sets.html",
    "title": "Conjugate sets",
    "section": "",
    "text": "Conjugate sets\n\nProve that S^* = \\left(\\overline{S}\\right)^*\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*\nProve that if B(0,r) is a ball of radius r at some norm with the center in zero, then \\left( B(0,r) \\right)^* = B(0,1/r)\nFind a dual cone for a monotonous non-negative cone:\n\nK = \\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\}\n\nFind and sketch on the plane a conjugate set to a multi-faceted cone: S = \\mathbf{cone} \\{ (-3,1), (2,3), (4,5)\\}\nDerive the definition of the cone from the definition of the conjugate set.\nName any 3 non-trivial facts about conjugate sets.\nHow to write down a set conjugate to the polyhedron?\nDraw a conjugate set by hand for simple sets. Conjugate to zero, conjugate to the halfline, to two random points, to their convex hull, etc.\nGive examples of self-conjugate sets.\nUsing a lemma about a cone conjugate, conjugate to the sum of cones and a lemma about a cone, conjugate to the intersection of closed convex cones, prove that cones\n\nK_1 = \\{x \\in \\mathbb{R}^n \\mid x = Ay, y \\ge 0, y \\in \\mathbb{R}^m, A \\in \\mathbb{R}^{n \\times}, \\}, \\;\\; K_2 = \\{p \\in \\mathbb{R}^n \\mid A^Tp \\ge 0\\}\n\nare inter conjugated.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -4, \\;\\; -2x_1 + x_2 \\ge -4\\}\n\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge -1, \\;\\; 2x_1 - x_2 \\ge 0, \\;\\; -x_1 + 2x_2 \\ge -2\\}\n\nFind conjugate cone for the cone of positive definite (semi-definite) matrices.\nFind the conjugate cone for the exponential cone:\n\nK = \\{(x, y, z) \\mid y &gt; 0, y e^{x/y} \\leq z\\}\n\nProve that’s fair for closed convex cones:\n\n(K_1 \\cap K_2)^* = K_1^* + K_2^*\n\nFind the dual cone for the following cones:\n\nK = \\{0\\}\nK = \\mathbb{R}^2\nK = \\{(x_1, x_2) \\mid \\vert x_1\\vert \\leq x_2\\}\nK = \\{(x_1, x_2) \\mid x_1 + x_2 = 0\\}\n\nFind and sketch on the plane a conjugate set to a multifaced cone:\n\n  S = \\mathbf{conv} \\left\\{ (-4,-1), (-2,-1), (-2,1)\\right\\} + \\mathbf{cone} \\left\\{ (1,0), (2,1)\\right\\}\n\nFind and sketch on the plane a conjugate set to a polyhedra:\n\nS = \\left\\{ x \\in \\mathbb{R}^2 \\mid -3x_1 + 2x_2 \\le 7, x_1 + 5x_2 \\le 9, x_1 - x_2 \\le 3, -x_2 \\le 1\\right\\}\n\nProve that if we define the conjugate set to S as follows:\n\nS^* = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\le 1 \\;\\; \\forall x \\in S\\},\n\nthen unit ball with the zero point as the center is the only self conjugate set in \\mathbb{R}^n.\nFind the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\nLet L be the subspace of a Euclidian space X. Prove that L^* = L^\\bot, where L^\\bot - orthogonal complement to L.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices. Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nProve, that B_p and B_{p_*} are inter-conjugate, i.e. (B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nProve, that K_p and K_{p_*} are inter-conjugate, i.e. (K_p)^* = K_{p_*}, (K_{p_*})^* = K_p, where K_p = \\left\\{ [x, \\mu] \\in \\mathbb{R}^{n+1} : \\|x\\|_p \\leq \\mu \\right\\}, \\; 1 &lt; p &lt; \\infty is the norm cone (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nSuppose, S = S^*. Could the set S be anything, but a unit ball? If it can, provide an example of another self-conjugate set. If it couldn’t, prove it.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{\\star}, S^{\\star\\star}, S^{\\star\\star\\star}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_\\star} are inter-conjugate, i.e. (B_p)^\\star = B_{p_\\star}, (B_{p_\\star})^\\star = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_\\star are conjugated, i.e. p^{-1} + p^{-1}\\_\\star = 1. You can assume, that p_\\star = \\infty if p = 1 and vice versa.",
    "crumbs": [
      "Exercises",
      "Conjugate sets"
    ]
  },
  {
    "objectID": "docs/exercises/automatic_differentiation.html",
    "href": "docs/exercises/automatic_differentiation.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Automatic differentiation\n\nCalculate the gradient of a Taylor series of a \\cos (x) using autograd library:\nimport autograd.numpy as np # Thinly-wrapped version of Numpy \nfrom autograd import grad \n\ndef taylor_cosine(x): # Taylor approximation to cosine function \n  # Your np code here\n  return ans \nIn the following code for the gradient descent for linear regression change the manual gradient computation to the PyTorch/jax autograd way. Compare those two approaches in time.\nIn order to do this, set the tolerance rate for the function value \\varepsilon = 10^{-9}. Compare the total time required to achieve the specified value of the function for analytical and automatic differentiation. Perform measurements for different values of n from np.logspace(1,4).\nFor each n value carry out at least 3 runs.\nimport numpy as np \n\n# Compute every step manually\n\n# Linear regression\n# f = w * x \n\n# here : f = 2 * x\nX = np.array([1, 2, 3, 4], dtype=np.float32)\nY = np.array([2, 4, 6, 8], dtype=np.float32)\n\nw = 0.0\n\n# model output\ndef forward(x):\n    return w * x\n\n# loss = MSE\ndef loss(y, y_pred):\n    return ((y_pred - y)**2).mean()\n\n# J = MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(w*x - y)\ndef gradient(x, y, y_pred):\n    return np.dot(2*x, y_pred - y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 20\n\nfor epoch in range(n_iters):\n    # predict = forward pass\n    y_pred = forward(X)\n\n    # loss\n    l = loss(Y, y_pred)\n\n    # calculate gradients\n    dw = gradient(X, Y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 2 == 0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')\nCalculate the 4th derivative of hyperbolic tangent function using Jax autograd.\nCompare analytic and autograd (with any framework) approach for the hessian of:\n\nf(x) = \\dfrac{1}{2}x^TAx + b^Tx + c\n\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = tr(AXB)\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\dfrac{1}{2} \\|Ax - b\\|^2_2\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nYou will work with the following function for this exercise,\n\nf(x,y)=e^{−\\left(sin(x)−cos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your own way to visualise it.\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nSuppose, we have the following function f(x) = \\frac{1}{2}\\|x\\|^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0:\n\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n\nYour goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose, \\alpha_0 = \\mathbb{1}^{10}.\n\n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n\n\\frac{\\partial L}{\\partial \\alpha} should be computed at each step using automatic differentiation. Choose any \\beta and the number of steps your need. Describe obtained results.\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x",
    "crumbs": [
      "Exercises",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/benchmarks/index.html",
    "href": "docs/benchmarks/index.html",
    "title": "",
    "section": "",
    "text": "Here you can find comparison of different algorithms with respect to the different hyperparameter choice.",
    "crumbs": [
      "Benchmarks"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html",
    "href": "docs/applications/two_way_partitioning.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Illustration\n\n\nSuppose, we have a set of n objects, which are needed to be split into two groups. Moreover, we have information about the preferences of all possible pairs of objects to be in the same group. This information could be presented in the matrix form: W \\in \\mathbb{R}^{n \\times n}, where \\{w_{ij}\\} is the cost of having i-th and j-th object in the same partitions. It is easy to see, that the total number of partitions is finite and equals to 2^n. So this problem can in principle be solved by simply checking the objective value of each feasible point. Since the number of feasible points grows exponentially, however, this is possible only for small problems (say, with n \\leq 30). In general (and for n larger than, say, 50) the problem is very difficult to solve.\nFor example, bruteforce solution on MacBook Air with M1 processor without any explicit parallelization will take more, than a universe lifetime for n=62.\n\n\n\nIllustration\n\n\nDespite the hardness of the problems, there are several ways to approach it.",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#simple-lower-bound-with-duality",
    "href": "docs/applications/two_way_partitioning.html#simple-lower-bound-with-duality",
    "title": "1 Intuition",
    "section": "2.1 Simple lower bound with duality",
    "text": "2.1 Simple lower bound with duality\nWe now derive the dual function for this problem. The Lagrangian is\n\nL(x, \\nu) = x^\\top W x + \\sum\\limits_{i=1}^n \\nu_i (x^2_i − 1) = x^\\top (W + \\text{diag}(\\nu))x − \\mathbf{1}^\\top \\nu.\n\nWe obtain the Lagrange dual function by minimizing over x:\n\n\\begin{split}\ng(\\nu) &= \\inf_{x \\in\\mathbb{R}^n} x^\\top (W + diag(\\nu))x − \\mathbf{1}^\\top \\nu = \\\\\n&= \\begin{cases}\n\\mathbf{1}^\\top \\nu,  &W + \\text{diag}(\\nu) \\succeq 0 \\\\\n-\\infty, &\\text{ otherwise} \\end{cases}\n\\end{split}\n\nThis dual function provides lower bounds on the optimal value of the difficult problem. For example, we can take any specific value of the dual variable\n\n\\nu = −\\lambda_{min}(W)\\mathbf{1},\n\nThis yields the bound on the optimal value p^*:\n\np^* \\geq g(\\nu) \\geq −\\mathbf{1}^\\top \\nu = n \\lambda_{min}(W)\n\nQuestion Can you obtain the same lower bound without knowledge of duality, but using the idea of eigenvalues?",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html",
    "href": "docs/applications/salesman_problem.html",
    "title": "1 Problem",
    "section": "",
    "text": "Suppose, we have N points in \\mathbb{R}^d Euclidian space (for simplicity, we’ll consider and plot case with d=2). Let’s imagine, that these points are nothing else but houses in some 2d village. Salesman should find the shortest way to go through the all houses only once.\n\n\n\nIllustration\n\n\nThat is, very simple formulation, however, implies NP - hard problem with the factorial growth of possible combinations. The goal is to minimize the following cumulative distance:\n\nd = \\sum_{i=1}^{N-1} \\| x_{y(i+1)}  - x_{y(i)}\\|_2 \\to \\min_{y},\n\nwhere x_k is the k-th point from N and y stands for the N- dimensional vector of indicies, which describes the order of path. Actually, the problem could be formulated as an LP problem, which is easier to solve.",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#crossing-procedure",
    "href": "docs/applications/salesman_problem.html#crossing-procedure",
    "title": "1 Problem",
    "section": "2.1 Crossing procedure",
    "text": "2.1 Crossing procedure\nEach iteration of the algorithm starts with the crossing (breed) procedure. Formally speaking, we should formulate the mapping, that takes two creature vectors as an input and returns its offspring, which inherits parents properties, while remaining consistent. We will use ordered crossover as such procedure.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#mutation",
    "href": "docs/applications/salesman_problem.html#mutation",
    "title": "1 Problem",
    "section": "2.2 Mutation",
    "text": "2.2 Mutation\nIn order to give our algorithm some ability to escape local minima, we provide it with mutation procedure. We simply swap some houses in an individual vector. To be more accurate, we define mutation rate (say, 0.05). On the one hand, the higher the rate, the less stable the population is, on the other, the smaller the rate, the more often algorithm gets stuck in the local minima. We choose \\text{mutation_rate} \\cdot n individuals and in each case swap random \\text{mutation_rate} \\cdot N digits.",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#selection",
    "href": "docs/applications/salesman_problem.html#selection",
    "title": "1 Problem",
    "section": "2.3 Selection",
    "text": "2.3 Selection\nAt the end of the iteration we have increased population (due to crossing results), then we just calculate total path distance to each individual and select top n of them.\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIn general, for any c &gt; 0, where d is the number of dimensions in the Euclidean space, there is a polynomial-time algorithm that finds a tour of length at most (1 + \\frac{1}{c}) times the optimal for geometric instances of TSP in\n\n\\mathcal{O}\\left(N(\\log N)^{(\\mathcal{O}(c{\\sqrt {d}}))^{d-1}}\\right)",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/pca.html",
    "href": "docs/applications/pca.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Imagine, that you have a dataset of points. Your goal is to choose orthogonal axes, that describe your data the most informative way. To be precise, we choose first axis in such a way, that maximize the variance (expressiveness) of the projected data. All the following axes have to be orthogonal to the previously chosen ones, while satisfy largest possible variance of the projections.\nLet’s take a look at the simple 2d data. We have a set of blue points on the plane. We can easily see that the projections on the first axis (red dots) have maximum variance at the final position of the animation. The second (and the last) axis should be orthogonal to the previous one.\n source\nThis idea could be used in a variety of ways. For example, it might happen, that projection of complex data on the principal plane (only 2 components) bring you enough intuition for clustering. The picture below plots projection of the labeled dataset onto the first to principal components (PCs), we can clearly see, that only two vectors (these PCs) would be enough to differ Finnish people from Italian in particular dataset (celiac disease (Dubois et al. 2010))  source",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#iris-dataset",
    "href": "docs/applications/pca.html#iris-dataset",
    "title": "1 Intuition",
    "section": "3.1 🌼 Iris dataset",
    "text": "3.1 🌼 Iris dataset\nConsider the classical Iris dataset\n\n\n\nIllustration\n\n\nsource\nWe have the dataset matrix A \\in \\mathbb{R}^{150 \\times 4}\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html",
    "href": "docs/applications/knapsack_problem.html",
    "title": "1 Introduction",
    "section": "",
    "text": "The knapsack problem or rucksack problem is a problem in combinatorial optimization. Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\n\n\nWikipedia\n\n\n\n\nThe most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases.",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#definitions",
    "href": "docs/applications/knapsack_problem.html#definitions",
    "title": "1 Introduction",
    "section": "",
    "text": "The most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases.",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#exact-solutions",
    "href": "docs/applications/knapsack_problem.html#exact-solutions",
    "title": "1 Introduction",
    "section": "2.1 Exact solutions",
    "text": "2.1 Exact solutions\n\n2.1.1 Full search\nAs for other discrete tasks, the backpack problem can be solved by completely sorting through all possible solutions. Suppose there are n items that can be packed in a backpack. It is necessary to determine the maximum value of the cargo, whose weight does not exceed W. For each item, there are 2 options: the item is either put in a backpack or not. Then enumeration of all possible options has time complexity O(2^n), which allows it to be used only for a small number of objects. With an increase in the number of objects, the task becomes unsolvable by this method in an acceptable time.\n\n\n2.1.2 Dynamic programming algorithm\nA similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume w_{1},\\,w_{2},\\,\\ldots ,\\,w_{n}, W are strictly positive integers. Define m(i,w) to be the maximum value that can be attained with weight less than or equal to w using items up to i (first i items). We can define m(i,w) recursively as follows:\n\nm(i, w) =\n\\begin{cases}\n   m(0,w)=0  \\\\\n   m(i,w)=m(i-1,w) &\\text{$w_{i}&gt;w$ } \\\\\n   m(i,w)=max(m(i-1,w), m(i-1,w-w_{i})+w_{i}) &\\text{$w_{i} \\le w$ }\n\\end{cases}\n\nThe solution can then be found by calculating m(n,W). To do this efficiently, we can use a table to store previous computations. This solution will therefore run in O(nW) time and O(nW) space.",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#approximation-algorithms",
    "href": "docs/applications/knapsack_problem.html#approximation-algorithms",
    "title": "1 Introduction",
    "section": "2.2 Approximation algorithms",
    "text": "2.2 Approximation algorithms\n\n2.2.1 Greedy algorithm\nTo solve the problem by the greedy algorithm, it is necessary to sort things by their specific value (that is, the ratio of the value of an item to its weight), and put items with the highest specific value in a backpack. The running time of this algorithm is the sum of the sorting time and stacking time. The difficulty in sorting items is O(N \\ log (N)). Next, the calculation of how many items fit in a backpack for the total time O(N). Total complexity O(N \\ log (N)) if necessary sorting and O(N) if already sorted data. It should be understood that a greedy algorithm can lead to an answer arbitrarily far from optimal. For example, if one item has a weight of 1 and a cost of 2, and another has a weight of W and a cost of W, then the greedy algorithm will pick up the final cost of 2 with the optimal answer W.\n\n\n2.2.2 Probabilistic algorithm\nIt is a modification of greedy algorithm. In this algorithm decision to include an item with index j in the knapsack is taken based on th probability of \\frac{\\lambda_j}{\\sum_{i=1}^n\\lambda_i}, where \\lambda_i is the ratio of the value of an item to its weight. This algorithm is run several times and the best solution is selected. If Algorithm starts as many times as you like, then the probability of getting it as a result the work of the optimal solution tends to 1. Total complexity is equal to O(mN log(N)) operations. Calculating experiment The above algorithms were implemented in the python programming language, their source codes are available at the link below. Initial data for the task, namely the values (v_{i}) and weights (w_{i}) of items were randomly generated. The following ranges of values have been selected v_{i}\\in [0, 100] and w_{i}\\in [100, 200]. Maximum weight capacity W generated randomly from interval W \\in [0.5\\sum w_i, 0.75\\sum w_i].",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/ellipsoid.html",
    "href": "docs/applications/ellipsoid.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\n\n\n\nIllustration\n\n\nLet x_1, \\ldots, x_n be the points in \\mathbb{R}^2. Given these points we need to find an ellipsoid, that contains all points with the minimum volume (in 2d case volume of an ellipsoid is just the square).\nAn invertible linear transformation applied to a unit sphere produces an ellipsoid with the square, that is \\det A^{-1} times bigger, than the unit sphere square, that’s why we parametrize the interior of ellipsoid in the following way:\n\nS = \\{x \\in \\mathbb{R}^2 \\; | \\; u = Ax + b, \\|u\\|_2^2 \\leq 1\\}\n\nSadly, the determinant is the function, which is relatively hard to minimize explicitly. However, the function \\log \\det A^{-1} = -\\log \\det A is actually convex, which provides a great opportunity to work with it. As soon as we need to cover all the points with ellipsoid of minimum volume, we pose an optimization problem on the convex function with convex restrictions:\n\n\\begin{align*}\n& \\min_{A \\in \\mathbb{R}^{2 \\times 2}, b \\in \\mathbb{R}^{2}} -\\log\\det(A)\\\\\n\\text{s.t. } & \\|Ax_i + b\\| \\leq 1, i = 1, \\ldots, n\\\\\n& A \\succ 0\n\\end{align*}\n\n\n\n\nIllustration\n\n\n\n\n2 Code\nOpen In Colab{: .btn }\n\n\n3 References\n\nJupyter notebook by A. Katrutsa\nhttps://cvxopt.org/examples/book/ellipsoids.html",
    "crumbs": [
      "Applications",
      "Minimum volume ellipsoid"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html",
    "href": "docs/applications/Neural_Lipschitz_constant.html",
    "title": "1 Lipschitz constant of a convolutional layer in neural network",
    "section": "",
    "text": "1 Lipschitz constant of a convolutional layer in neural network\nIt was observed, that small perturbation in Neural Network input could lead to significant errors, i.e. misclassifications.\n\n\n\nhttps://escholarship.org/content/qt3k2780bg/qt3k2780bg_noSplash_e0803cb722032c480ec3468d84e60e2a.pdf?t=qqf3iz\n\n\nLipschitz constant bounds the magnitude of the output of a function, so it cannot change drastically with a slight change in the input\n\n\\|NN(image) - NN(image+\\varepsilon)\\| \\leq L\\|\\varepsilon\\|\n\nIn this notebook we will try to estimate Lipschitz constant of some convolutional layer of a Neural Network.\n\n\n2 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html",
    "href": "docs/applications/MLE.html",
    "title": "1 Problem",
    "section": "",
    "text": "We need to estimate probability density p(x) of a random variable from observed values.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html#linear-measurements-with-i.i.d.-noise",
    "href": "docs/applications/MLE.html#linear-measurements-with-i.i.d.-noise",
    "title": "1 Problem",
    "section": "2.1 Linear measurements with i.i.d. noise",
    "text": "2.1 Linear measurements with i.i.d. noise\nSuppose, we are given the set of observations:\n\nx_i = \\theta^\\top a_i + \\xi_i, \\quad i = [1,m],\n\nwhere\n\n\\theta \\in \\mathbb{R}^n - unknown vector of parameters\n\\xi_i are IID noise random variables with density p(z)\nx_i - measurements, x \\in \\mathbb{R}^m\n\nWhich implies the following optimization problem:\n\n\\max\\limits_{\\theta} \\log p(x) = \\max_\\theta \\sum\\limits_{i=1}^m \\log p (x_i - \\theta^\\top a_i) = \\max_\\theta L(\\theta)\n\nWhere the sum goes from the fact, that all observation are independent, which leads to the fact, that p(\\xi) = \\prod\\limits_{i=1}^m p(\\xi_i). The target function is called log-likelihood function L(\\theta).\n\n2.1.1 Gaussian noise\n\np(z) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{z^2}{2 \\sigma^2}}\n\n\n\\log p(z) = - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{z^2}{2 \\sigma^2}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{(x_i - \\theta^\\top a_i)^2}{2 \\sigma^2} \\right] \\\\\n&= - \\dfrac{m}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{1}{2 \\sigma^2} \\sum\\limits_{i=1}^m (x_i - \\theta^\\top a_i)^2\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of gaussian noise is a least squares solution.\n\n\n2.1.2 Laplacian noise\n\np(z) = \\dfrac{1}{2a} e^{-\\frac{|z|}{a}}\n\n\n\\log p(z) = -  \\log (2a) - -\\dfrac{|z|}{a}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\log (2a) - -\\dfrac{|(x_i - \\theta^\\top a_i)|}{a} \\right] \\\\\n&= - m \\log (2 a) - \\dfrac{1}{a} \\sum\\limits_{i=1}^m |x_i - \\theta^\\top a_i|\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of Laplacian noise is a l_1-norm solution.\n\n\n2.1.3 Uniform noise\n\np(z) = \\begin{cases}\n  \\frac{1}{2a}, & -a \\leq z \\leq a, \\\\\n  0, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n\n\\log p(z) =  \\begin{cases}\n  - \\log(2a), & -a \\leq z \\leq a, \\\\\n  -\\infty, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n$$\nL() =\n\\begin{cases}\n  - m\\log(2a), & |x_i - \\theta^\\top a_i| \\leq a, \\\\\n  -\\infty, &  \\text{ otherwise }\n  \\end{cases}\n$$\nWhich means, the maximum likelihood estimation in case of uniform noise is any vector \\theta, which satisfies \\vert x_i - \\theta^\\top a_i \\vert \\leq a.",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html#binary-logistic-regression",
    "href": "docs/applications/MLE.html#binary-logistic-regression",
    "title": "1 Problem",
    "section": "2.2 Binary logistic regression",
    "text": "2.2 Binary logistic regression\nSuppose, we are given a set of binary random variables y_i \\in \\{0,1\\}. Let us parametrize the distribution function as a sigmoid, using linear transformation of the input as an argument of a sigmoid.\n\n\n\nPicture from Wikipedia\n\n\n\n\\begin{split}\np(y_i = 1) &= \\dfrac{\\text{exp}(\\theta_0^\\top x_i + \\theta_1)}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)} \\\\\np(y_i = 0) &= \\dfrac{1}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)}\n\\end{split}\n\nLet’s assume, that first k observations are ones: y_1, \\ldots, y_k =1, y_{k+1}, \\ldots, y_m = 0. Then, log-likelihood function will be written as follows:\n\nL(\\theta_0, \\theta_1) = \\sum\\limits_{i=1}^k (\\theta_0^\\top x_i + \\theta_1) - \\sum\\limits_{i=1}^m \\log(1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1))",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html",
    "href": "docs/applications/A-Star.html",
    "title": "1 Problem",
    "section": "",
    "text": "The graph is one of the most significant structures in the algorithms, because this structure can represent many real life cases, from streets to networks.\nAnd one is the most popular problem is: Find the least sum of graph edges for given start and end points\nGenerally, we need determine input and output data:\n- Input data: graph map and end or start point/node (or both for certain path) - Output data: paths (or intermediate points/nodes) with the least sum of graph edges as result",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#breadth-first-search",
    "href": "docs/applications/A-Star.html#breadth-first-search",
    "title": "1 Problem",
    "section": "2.1 Breadth First Search",
    "text": "2.1 Breadth First Search\nThis is the simplest algorithm for graph traversing. It starts at the tree root (it may be start/end node) and explores all the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n\n\n\n\n\n\n\n\nOrigin Graph\nResult Tree\nAnimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously this algorithm has low performance: O(\\vert V \\vert + \\vert E\\vert) = O(b^d), where b is branch factor (average quantity of children nodes in tree, e.g. for binary tree b=2) and d is depth/distance from root.",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#dijkstras-algorithm",
    "href": "docs/applications/A-Star.html#dijkstras-algorithm",
    "title": "1 Problem",
    "section": "2.2 Dijkstra’s algorithm",
    "text": "2.2 Dijkstra’s algorithm\n\n\n\nDescription\nAnimation\n\n\n\n\nDijkstra’s Algorithm (also called Uniform Cost Search) lets us prioritize which paths to explore. Instead of exploring all possible paths equally (like in Breadth First Search), it favors lower cost paths.\n_________________________",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#greedy-best-first-search",
    "href": "docs/applications/A-Star.html#greedy-best-first-search",
    "title": "1 Problem",
    "section": "2.3 Greedy Best-First-Search",
    "text": "2.3 Greedy Best-First-Search\nWith Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. Let’s make the frontier expand towards the goal more than it expands in other directions. First, we’ll define a heuristic function that tells us how close we are to the goal. E.g. on flat map we can use function like H(A, B) = |A.x - B.x| + |A.y - B.y| , where A and B are nodes with coordinates {x, y}. Let’s consider not only shortest edges, but also use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first.\n\n\n\n\n\n\n\nResult of Heuristic function\nAnimation\n\n\n\n\nWe can see that firstly nodes, that are closer to target are considered at first. But when algorithm finds a barrier, then it tries to find the path to walk around, but this path is best from the corner, not from the start position, so the result path is not the shortest. This is a result of the heuristic function. To solve this problem let’s consider next algorithm\n_________________________",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#a-star-algorithm",
    "href": "docs/applications/A-Star.html#a-star-algorithm",
    "title": "1 Problem",
    "section": "2.4 A-Star Algorithm",
    "text": "2.4 A-Star Algorithm\nDijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal.\n\n\n\n\n\n\n\nResult of Cost and Heuristic function\nAnimation\n\n\n\n\nBecause of considering both cost and result of heuristic functuion as result metric for Dijkstra’s algorithm, we can find the shortest path faster, than raw Dijkstra’s algorithm, and precisely, than Greedy Best-First-Search\n_________________________",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#a-star-implementation",
    "href": "docs/applications/A-Star.html#a-star-implementation",
    "title": "1 Problem",
    "section": "2.5 A-Star Implementation",
    "text": "2.5 A-Star Implementation\nLet’s take a closer look at this algorithm and analyze it with code example. First of all you need to create a Priority Queue because you should consider points, which are closer to destination from start position. Priority does not equal cost. This Queue contains possible points, that are to be considered as possible shortest way to destination.\n# Only main methods\nclass PriorityQueue:\n    # Puts item in collection, sorted by priority.\n    def put(self, item, priority):\n    # Returns the most priority item.\n    def get(self):\nAlso you need a class, that describes your Graph Model with 2 methods. First finds neighbors of current node, and second returns cost between current node and next. This methods allows to implement any structure, be neither grid, hexagonal map or graph.\n# Only main methods\nclass SquareGrid:\n    # Returns neigbours of 'id' cell\n    # according to map and 'walls'.\n    def neighbors(self, id):\n    # Returns cost (or distance) between 2 cells.\n    # Applicable for neighbors only.\n    def cost(self, current, next):\nAlso you need to add your heuristic function too. Because, e.i. on a grid, a cost is always equals to 1 (if you don’t use diagonals), so it would be like a Breadth First Search, but you know a destination point, so you can use direction.\ndef heuristic(a, b):\n    (x1, y1) = a\n    (x2, y2) = b\n    return ((x1 - x2)**2 + (y1 - y2)**2)**.5\nNow we can implement our A-Star algorithm. First of all we need to init our algorithm: frontier stores points according to priority. We will store information in dictionaries:\n\ncame_from like pair &lt;TO point : FROM point&gt;\ncost_so_far like pair &lt;Point : Distance from start&gt;\n\nFirstly add our start point to them. Than, for each point (temporary as origin we find its neighbors and for each calculate the cost as: cost from origin to neighbor. If there is no information about this node in Queue or the cost is less than before, that add this point to queue with priority = cost + heuristic. Last step allows to consider more closer point to destination at first.\ndef  a_star_search(graph, start, goal):\n    ## Create a queue and add start point\n    frontier = PriorityQueue()\n    frontier.put(start,  0)\n    # Dictionaries with init for start point\n    came_from = {}\n    cost_so_far = {}\n    came_from[start] = None\n    cost_so_far[start] = 0\n    # Not all neighbors are visited\n    while  not frontier.empty():\n        # Get next node (firstly it is start one) \n        current = frontier.get()\n        if current == goal:\n            break\n        # Find all neighbor nodes\n        for  next  in graph.neighbors(current):\n            new_cost = cost_so_far[current] + graph.cost(current,  next)\n            # Not visited or cost to it is less\n            if  next  not  in cost_so_far or new_cost &lt; cost_so_far[next]:\n                cost_so_far[next] = new_cost\n                priority = new_cost + heuristic(goal,  next)\n                frontier.put(next, priority)\n                came_from[next] = current\n    return came_from, cost_so_far",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html",
    "href": "docs/applications/NN_Loss_Surface.html",
    "title": "1 Scalar Projection",
    "section": "",
    "text": "1 Scalar Projection\nLet’s consider the training of our neural network by solving the following optimization problem:\n\n\\mathcal{L} (\\theta) \\to \\min_{\\theta \\in \\mathbb{R}^p}\n\nWe denote the initial point as \\theta_0, representing the weights of the neural network at initialization. The weights after training are denoted as \\hat{\\theta}.\nIn the given example, we have p = 105,866, which implies that we are seeking a minimum in a 105,866-dimensional space. Exploring this space is intriguing, and the underlying concept is as follows.\nInitially, we generate a random Gaussian direction w_1 \\in \\mathbb{R}^p, which inherits the magnitude of the original neural network weights for each parameter group. Subsequently, we sample the training and testing loss surfaces at points along the direction w_1, situated close to either \\theta_0 or \\hat{\\theta}.\nMathematically, this involves evaluating:\n\n\\mathcal{L} (\\alpha) = \\mathcal{L} (\\theta_0 + \\alpha w_1), \\text{ where } \\alpha \\in [-b, b].\n\nHere, \\alpha plays the role of a coordinate along the w_1 direction, and b stands for the bounds of interpolation. Visualizing \\mathcal{L} (\\alpha) enables us to project the p-dimensional surface onto a one-dimensional axis.\nIt is important to note that the characteristics of the resulting graph heavily rely on the chosen projection direction. It’s not feasible to maintain the entirety of the informationWhen transforming a space with 100,000 dimensions into a one-dimensional line through projection. However, certain properties can still be established. For instance, if \\mathcal{L} (\\alpha) \\mid_{\\alpha=0} is decreasing, this indicates that the point lies on a slope. Additionally, if the projection is non-convex, it implies that the original surface was not convex.\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\n\n\n2 Two dimensional projection\nWe can explore this idea further and draw the projection of the loss surface to the plane, which is defined by 2 random vectors. Note, that with 2 random gaussian vectors in the huge dimensional space are almost certainly orthogonal.\nSo, as previously, we generate random normalized gaussian vectors w_1, w_2 \\in \\mathbb{R}^p and evaluate the loss function\n\n\\mathcal{L} (\\alpha, \\beta) = \\mathcal{L} (\\theta_0 + \\alpha w_1 + \\beta w_2), \\text{ where } \\alpha, \\beta \\in [-b, b]^2.\n\nwhich immediately leads us to the following nice pictures:\n                        \n                                            \n                        \n                                            \n\n\n3 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Neural Network Loss Surface Visualization"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html",
    "href": "docs/applications/deep_learning.html",
    "title": "1 Problem",
    "section": "",
    "text": "Illustration\n\n\nA lot of practical tasks nowadays are being solved using the deep learning approach, which is usually implies finding local minimum of a non-convex function, that generalizes well (enough 😉). The goal of this short text is to show you the importance of the optimization behind neural network training.\n\n\nOne of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html#cross-entropy",
    "href": "docs/applications/deep_learning.html#cross-entropy",
    "title": "1 Problem",
    "section": "",
    "text": "One of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/index.html",
    "href": "docs/applications/index.html",
    "title": "",
    "section": "",
    "text": "This section contains self-sufficient examples of numerical applications with Python code. Most of the examples are covered in the Boyd book. There are also some very useful link about applications with code:\n\nCVXOPT examples\nCVXPY examples\nAlexandr Katrutsa demos\n\n\n\n\n\n\n\n\n\nA^* algorithm for path finding\n\n\n\n\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nKnapsack problem\n\n\n\n\n\n\n\n\n\n\n\nLinear least squares\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood estimation\n\n\n\n\n\n\n\n\n\n\n\nMinimum volume ellipsoid\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Loss Surface Visualization\n\n\n\n\n\n\n\n\n\n\n\nNeural network Lipschitz constant\n\n\n\n\n\n\n\n\n\n\n\nPrincipal component analysis\n\n\n\n\n\n\n\n\n\n\n\nRendezvous problem\n\n\n\n\n\n\n\n\n\n\n\nTotal variation in-painting\n\n\n\n\n\n\n\n\n\n\n\nTravelling salesman problem\n\n\n\n\n\n\n\n\n\n\n\nTwo way partitioning problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html",
    "href": "docs/applications/least_squares.html",
    "title": "1 Problem",
    "section": "",
    "text": "Illustration\n\n\nIn a least-squares, or linear regression, problem, we have measurements X \\in \\mathbb{R}^{m \\times n} and y \\in \\mathbb{R}^{m} and seek a vector \\theta \\in \\mathbb{R}^{n} such that $X $ is close to y. Closeness is defined as the sum of the squared differences:\n\n\\sum\\limits_{i=1}^m (x_i^\\top \\theta - y_i)^2\n\nalso known as the l_2-norm squared, \\|X \\theta - y\\|^2_2\nFor example, we might have a dataset of m users, each represented by n features. Each row x_i^\\top of X is the features for user i, while the corresponding entry y_i of y is the measurement we want to predict from x_i^\\top, such as ad spending. The prediction is given by x_i^\\top \\theta.\nWe find the optimal \\theta by solving the optimization problem\n\n\\|X \\theta - y\\|^2_2 \\to \\min_{\\theta \\in \\mathbb{R}^{n}}\n\nLet \\theta^* denote the optimal $ $. The quantity $ r=X ^* - y $ is known as the residual. If $ |r|_2 = 0 $, we have a perfect fit.\nNote, that the function needn’t be linear in the argument x but only in the parameters \\theta that are to be determined in the best fit.",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#moorepenrose-inverse",
    "href": "docs/applications/least_squares.html#moorepenrose-inverse",
    "title": "1 Problem",
    "section": "2.1 Moore–Penrose inverse",
    "text": "2.1 Moore–Penrose inverse\nIf the matrix X is relatively small, we can write down and calculate exact solution:\n\n\\theta^* = (X^\\top X)^{-1} X^\\top y = X^\\dagger y,\n\nwhere X^\\dagger is called pseudo-inverse matrix. However, this approach squares the condition number of the problem, which could be an obstacle in case of ill-conditioned huge scale problem.",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#qr-decomposition",
    "href": "docs/applications/least_squares.html#qr-decomposition",
    "title": "1 Problem",
    "section": "2.2 QR decomposition",
    "text": "2.2 QR decomposition\nFor any matrix X \\in \\mathbb{R}^{m \\times n} there is exists QR decomposition:\n\nX = Q \\cdot R,\n\nwhere Q is an orthogonal matrix (its columns are orthogonal unit vectors meaning Q^\\top Q=QQ^\\top=I and R is an upper triangular matrix. It is important to notice, that since Q^{-1} = Q^\\top, we have:\n\nQR\\theta = y \\quad \\longrightarrow \\quad R \\theta = Q^\\top y\n\nNow, process of finding theta consists of two steps:\n\nFind the QR decomposition of X.\nSolve triangular system R \\theta = Q^\\top y, which is triangular and, therefore, easy to solve.",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#cholesky-decomposition",
    "href": "docs/applications/least_squares.html#cholesky-decomposition",
    "title": "1 Problem",
    "section": "2.3 Cholesky decomposition",
    "text": "2.3 Cholesky decomposition\nFor any positive definite matrix A \\in \\mathbb{R}^{n \\times n} there is exists Cholesky decomposition:\n\nX^\\top X = A = L^\\top \\cdot L,\n\nwhere L is an lower triangular matrix. We have:\n\nL^\\top L\\theta = y \\quad \\longrightarrow \\quad L^\\top z_\\theta = y\n\nNow, process of finding theta consists of two steps:\n\nFind the Cholesky decomposition of X^\\top X.\nFind the z_\\theta = L\\theta by solving triangular system L^\\top z_\\theta = y\nFind the \\theta by solving triangular system L\\theta = z_\\theta\n\nNote, that in this case the error stil proportional to the squared condition number.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/rendezvous.html",
    "href": "docs/applications/rendezvous.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\n\n\n\nIllustration\n\n\nWe have two bodies in discrete time: the first is described by its coordinate x_i and its speed v_i, the second has coordinate z_i and speed u_i. Each body has its own dynamics, which we denote as linear systems with matrices A, B, C, D:\n\n\\begin{align*}\nx_{i+1} = Ax_i + Bu_i \\\\\nz_{i+1} = Cz_i + Dv_i\n\\end{align*}\n\nWe want these bodies to meet in future at some point T in such a way, that preserve minimum energy through the path. We will consider only kinetic energy, which is proportional to the squared speed at each point of time, that’s why optimization problem takes the following form:\n\n\\begin{align*}\n& \\min \\sum_{i=1}^T \\|u_i\\|_2^2 + \\|v_i\\|_2^2 \\\\\n\\text{s.t. } & x_{t+1} = Ax_t + Bu_t, \\; t = 1,\\ldots,T-1\\\\\n& z_{t+1} = Cz_t + Dv_t, \\; t = 1,\\ldots,T-1\\\\\n& x_T = z_T\n\\end{align*}\n\nProblem of this type arise in space engineering - just imagine, that the first body is the spaceship, while the second, say, Mars.\n\n\n2 Code\nOpen In Colab{: .btn }\n\n\n3 References\n\nJupyter notebook by A. Katrutsa",
    "crumbs": [
      "Applications",
      "Rendezvous problem"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html",
    "href": "docs/applications/total_variation_inpainting.html",
    "title": "1 Problem",
    "section": "",
    "text": "Illustration\n\n\n\n\nA grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:\n\n\n\nIllustration\n\n\n\n\n\nFor the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIt is not that easy, right?\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nOnly 5% of all pixels are left:\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nWhat about 1% of all pixels?\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#grayscale-image",
    "href": "docs/applications/total_variation_inpainting.html#grayscale-image",
    "title": "1 Problem",
    "section": "",
    "text": "A grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#color-image",
    "href": "docs/applications/total_variation_inpainting.html#color-image",
    "title": "1 Problem",
    "section": "",
    "text": "For the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIt is not that easy, right?\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nOnly 5% of all pixels are left:\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nWhat about 1% of all pixels?\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "href": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "title": "",
    "section": "",
    "text": "This chapter is WIP. We will make interactive graphs with benchmarx library, which allows to benchmark different optimizers in a convenient, reproducible way.",
    "crumbs": [
      "Benchmarks",
      "CNN on FashionMNIST"
    ]
  },
  {
    "objectID": "docs/benchmarks/linear_least_squares.html",
    "href": "docs/benchmarks/linear_least_squares.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\nIn a least-squares, or linear regression, problem, we have measurements A \\in \\mathbb{R}^{m \\times n} and b \\in \\mathbb{R}^{m} and seek a vector x \\in \\mathbb{R}^{n} such that A x is close to b. Closeness is defined as the sum of the squared differences:\n\nf(x) = \\|Ax - b\\|_2^2 \\to \\min_{x \\in \\mathbb{R^n}}",
    "crumbs": [
      "Benchmarks",
      "Linear Least Squares"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_functions.html",
    "href": "docs/exercises/conjugate_functions.html",
    "title": "Conjugate functions",
    "section": "",
    "text": "Conjugate functions\n\nFind f^*(y), if f(x) = ax + b\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = e^x\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, \\;\\;\\; f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) =\\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n\nFind f^*(y), if f(x) = -\\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = -0,5 - \\log x, \\;\\; x&gt;0\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nFind f^*(y), if f(x) = - (a^2 - x^2)^{1/2}, \\;\\;\\; \\vert x\\vert \\le a, \\;\\;\\; a&gt;0\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) = \\|x\\|\nFind f^*(y), if f(x) = \\dfrac{1}{2}\\|x\\|^2\nName any 3 non-trivial facts about conjugate function.\nFind conjugate function to the f(x) = \\dfrac{1}{x}, \\;\\; x \\in \\mathbb{R}_{++}\nFind conjugate function to the f(x) = x^p, \\;\\; x \\in \\mathbb{R}_{++}, \\;\\; p&gt;1\nProve, that if f(x_1, x_2) = g_1(x_1) + g_2(x_2), then f^*(y_1, y_2) = g_1^*(y_1) + g_2^*(y_2)\nProve, that if f(x) = g(x-b), then f^*(y) = b^\\top y + g^*(y)\nProve, that if f(x) = \\alpha g(x) and $ &gt; 0 $, then f^*(y) = \\alpha g^*(y/\\alpha)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y)",
    "crumbs": [
      "Exercises",
      "Conjugate functions"
    ]
  },
  {
    "objectID": "docs/exercises/convergence.html",
    "href": "docs/exercises/convergence.html",
    "title": "Rates of convergence",
    "section": "",
    "text": "Rates of convergence\n\nShow with the definition that the sequence \\left\\{ \\dfrac{1}{k} \\right\\}_{k=1}^\\infty does not have a linear convergence rate (but it converges to zero).\nShow with the definition that the sequence \\left\\{ \\dfrac{1}{k^k} \\right\\}_{k=1}^\\infty does not have a quadratic convergence rate (but it converges to zero).\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^k.\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^{2^k}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k^2}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k!}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^2}, & \\text{if } k\\text{ is odd} \\end{cases}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^{2k}}, & \\text{if } k\\text{ is odd} \\end{cases}.\nShow that the sequence x_k = 1 + (0.5)^{2^k} is quadratically converged to 1.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\left(\\frac{1}{4}\\right)^{2^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{x_{k-1}}{k}, & \\text{if } k\\text{ is odd} \\end{cases}.\nLet \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\nDetermine the convergence type of a given sequence r_k =\\begin{cases} \\frac{1}{2^k}, & \\text{if } k\\text{ is odd} \\\\ \\frac{1}{3^{2k}}, & \\text{if } k\\text{ is even} \\end{cases}.",
    "crumbs": [
      "Exercises",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/exercises/convex_sets.html",
    "href": "docs/exercises/convex_sets.html",
    "title": "Convex sets",
    "section": "",
    "text": "Convex sets\n\nShow that the set is convex if and only if its intersection with any line is convex.\nShow that the convex hull of the S set is the intersection of all convex sets containing S.\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\alpha &lt; \\mathbb{E} f(x) &lt; \\beta, where \\mathbb{E}f(x) stands for expected value of f(x): \\mathbb{R} \\rightarrow \\mathbb{R}, i.e. \\mathbb{E}f(x) = \\sum\\limits_{i=1}^n p_i f(a_i)\n\\mathbb{E}x^2 \\le \\alpha\n\\mathbb{V}x \\le \\alpha\n\nProve that if the set is convex, its interior is also convex. Is the opposite true?\nProve that if the set is convex, its closure is also convex. Is the opposite true?\nProve that the set of square symmetric positive definite matrices is convex.\nShow that the set of S is convex if and only if\n\n\\forall \\lambda_1, \\lambda_2 \\geq 0, \\quad (\\lambda_1, \\lambda_2) \\neq (0, 0):  \\lambda_1 S + \\lambda_2 S = (\\lambda_1 + \\lambda_2)S\n\nCalculate the Minkowski sum of the line segment and the square on the plane, the line segment and the triangle, the line segment and the circle, the line segment and the disk.\nFind the minimum value of k, at which the set of \\{x \\in \\mathbb{R}^2 \\mid (x_1^2 + 1) x_2\\le 2, x_2 \\ge k\\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nGive an example of two closed convex sets, the sum of which is not closed\nFind convex and conical hulls of the following sets: \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2, x_1 \\ge 0 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1 x_2 = 1 \\}\nShow that the set of directions of the strict local descending of the differentiable function in a point is a convex cone.\nProve that K:\n\nK = \\{ x \\in \\mathbb{R}^3 \\mid x_1^2 - 2x_1x_3 + x_2^2 \\leq 0, x_3 \\geq 0 \\}\n\nis a convex cone.\nFind the convex hulls of the following sets:\n\nx^2 + y^2 \\leq 1, xy = 0\nx^2 + y^2 = 1, x - y = 0\nx^2 + y^2 = 1, \\|x\\| \\leq 1, \\|y\\|\ny \\leq e^x, y \\geq \\|x\\|\n\nFor an arbitrary set of S, let’s say \\tilde{S} consists of all segments of [a,b] with the ends of a,b \\in S. Is it true that \\tilde{S} = \\text{conv}(S) ?\nIs the given set a convex polyhedron (could be written in the form of Ax \\preceq b, Cx = d):\n\nS = \\{ y_1a_1 + y_2a_2 \\mid -1 \\leq y_1, y_2 \\leq 1 \\}; a_1, a_2 \\in \\mathbb{R}^n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, \\mathbf{1}^\\top x = 1, \\sum\\limits_{i=1}^n x_ia_i = b_1, \\sum\\limits_{i=1}^n x_ia_i^2 = b_2 \\}; a_1, \\ldots a_n, b_1, b_2 \\in \\mathbb{R}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\|y\\|_2 = 1 \\}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\sum\\limits_{i=1}^n \\|y_i\\| = 1 \\}\n\nLet S \\subseteq \\mathbb{R}^n is a set of solutions to the quadratic inequality:\n\nS = \\{x \\in \\mathbb{R}^n \\mid x^\\top A x + b^\\top x + c \\leq 0 \\}; A \\in \\mathbb{S}^n, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}\n\n\nShow that if A \\succeq 0, S is convex. Is the opposite true?\nShow that the intersection of S with the hyperplane defined by the g^\\top x + h = 0, g \\neq 0 is convex if A + \\lambda gg^\\top \\succeq 0 for some real \\lambda \\in \\mathbb{R}. Is the opposite true?\n\nShow that the hyperbolic set of \\{x \\in \\mathbb{R}^n_+ | \\prod\\limits_{i=1}^n x_i \\geq 1 \\} is convex. Hint: For 0 \\leq \\theta \\leq 1 it is valid, that a^\\theta b^{1 - \\theta} \\leq \\theta a + (1-\\theta)b with non-negative a,b.\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\|_2 \\leq \\|x-y\\|_2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\|x - a\\|_2 \\leq \\theta\\|xb\\|_2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}\n\nFind the conic hull of the set of rank k matrix products \\{XX^\\top \\mid X \\in \\mathbb{R}^{n \\times k}, \\mathbf{rank} X = k \\}?\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nLet S be such that \\forall x,y \\in S \\to \\frac{1}{2}(x+y) \\in S. Is this set convex?\nFind the conic hull of the following sets in \\mathbb{R}^2:\n\ny = x^2\ny = x^2, x \\geq 0\ny = x^2 + x, x \\geq 0\nxy=1, x &gt; 0\ny = \\sin x, 0 \\leq x \\leq \\pi\ny = e^x\n\nLet S_1 = \\{x^2 + y^2 \\leq 1 \\} is a disk of \\mathbb{R^3} and S_2 is a segment of \\left[(0,0,-1), (0,0,1)\\right]. How their convex combination with \\alpha, \\beta looks like.\nIs the next set convex?\n\n\\{a \\in \\mathbb{R}^k \\mid p(0) = 1, \\|p(t)\\| \\leq 1 \\;\\; \\forall \\alpha \\leq t \\leq \\beta, \\;\\; p(t) = a_1 + a_2t + \\ldots + a_kt^{k-1} \\}\n\nProve that in order for K \\subseteq \\mathbb{R}^n to be a convex cone, it is enough that K contains all possible non-negative combinations of its points.\nProve that in order for S \\subseteq \\mathbb{R}^n to be an affine set it is necessary and sufficient that S contains all possible affine combinations of its points.\nПусть S_1, \\ldots, S_k - произвольные непустые множества в \\mathbb{R}^n. Докажите, что:\n\n\\mathbf{cone} \\left( \\bigcup\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{cone} \\left( S_i\\right)\n\\mathbf{conv} \\left( \\sum\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{conv} \\left( S_i\\right)\n\nProve, that the set S \\subseteq \\mathbb{R}^n is convex if and only if (\\alpha + \\beta)S = \\alpha S + \\beta S for all non-negative \\alpha and \\beta\\quad (\\alpha, \\beta) \\neq (0, 0)\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\n\\mathbb{V}x \\ge \\alpha\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\| \\mathbf{x} - \\mathbf{x}_c \\| \\leq r \\}) - is convex.\nProve, that if S is convex, then S+S = 2S. Give an counterexample in case, when S - is not convex.\nWhich of the following operations does not preserve convexity if X,Y \\subseteq \\mathbb{R}^n are convex sets?\n\nX \\cup Y\nX \\times Y = \\left\\{ (x,y) \\; \\mid \\; x \\in X, y \\in Y \\right\\}\n\\alpha X + \\beta Y = \\{ \\alpha x + \\beta y \\; \\mid \\; x \\in X, \\; y \\in Y, \\; \\alpha, \\beta \\in \\mathbb{R} \\}\n\\alpha X = \\{ \\alpha x \\; \\mid \\; x \\in X, \\; \\alpha \\in \\mathbb{R_{-}} \\}\nX^{c} = \\{x \\in \\mathbb{R}^n \\; \\mid \\; x \\notin X\\}\n\nShow, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.",
    "crumbs": [
      "Exercises",
      "Convex sets"
    ]
  },
  {
    "objectID": "docs/exercises/duality.html",
    "href": "docs/exercises/duality.html",
    "title": "Duality",
    "section": "",
    "text": "Duality\n\nToy example\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x = 0 \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate.Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& Cx = d \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f_0(x) = \\sum\\limits_{i=1}^n f_i(x_i) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b \\\\\n& f_i(x) - \\text{ differentiable and strictly convex}\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nNew variables. Consider an unconstrained problem of the form:\n\nf_0(Ax + b) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nAnd its equivalent reformulation:\n\n\\begin{split}\n& f_0(y) \\to \\min\\limits_{y \\in \\mathbb{R}^{m} }\\\\\n\\text{s.t. } & y = Ax + b \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problems\nFind the dual functions\nWrite down the dual problems\n\nThe weak duality inequality, d^* ≤ p^* , clearly holds when d^* = -\\infty or p^* = \\infty. Show that it holds in the other two cases as well: If p^* = −\\infty, then we must have d^* = −\\infty, and also, if d^* = \\infty, then we must have p^* = \\infty.\nExpress the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nLeast Squares. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nStandard form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nTwo-way partitioning problem. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top W x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x_i^2 = 1, i = 1, \\ldots, n \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\nCan you reduce this problem to the eigenvalue problem? 🐱\n\nEntropy maximization. Let we have the primal problem:\n\n\\begin{split}\n& \\sum_i x_i \\ln x_i \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& 1^\\top x = 1 \\\\\n& x \\succ 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nMinimum volume covering ellipsoid. Let we have the primal problem:\n\n\\begin{split}\n& \\ln \\text{det} X^{-1} \\to \\min\\limits_{X \\in \\mathbb{S}^{n}_{++} }\\\\\n\\text{s.t. } & a_i^\\top X a_i \\leq 1 , i = 1, \\ldots, m\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nEquality constrained norm minimization. Let we have the primal problem:\n\n\\begin{split}\n& \\|x\\| \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nInequality form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nNonconvex strong duality Let we have the primal problem:\n\n\\begin{split}\n& x^\\top Ax +2b^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1 \\\\\n& A \\in \\mathbb{S}^n, A \\nsucceq 0, b \\in \\mathbb{R}^n\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nA penalty method for equality constraints. We consider the problem minimize\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere $f_0(x): ^n $ is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\|Ax - b\\|_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [ 1,m ] \\}. First introduce new variables y_i and equality constraints y_i = b_i − a^\\top_i x. (The solution of this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [ 1,m ]. Analytic centers have geometric applications, and play an important role in barrier methods.)",
    "crumbs": [
      "Exercises",
      "Duality"
    ]
  },
  {
    "objectID": "docs/exercises/gop.html",
    "href": "docs/exercises/gop.html",
    "title": "General optimization problems",
    "section": "",
    "text": "General optimization problems\n\nLinear Least squares Write down exact solution of the linear least squares problem:\n\n\\|Ax-b\\|^2 \\to \\min_{x \\in \\mathbb{R}^n}, A \\in \\mathbb{R}^{m \\times n}\n\nConsider three cases:\n\nm &lt; n\nm = n\nm &gt; n\n\nTo successfully write a test on optimization methods, a student must spend at least \\mathrm{K} kilocalories. The evening before the test, he goes to the store to buy food for dinner. There are m items in the store, the unit price of each item is p_i, i = 1, \\ldots , m. It is also known that each item’s i-th unit gives the student energy equal to k_i kilocalories. Formulate the problem of determining the contents of a minimum value basket for the successful writing of a test. Is this a convex problem? Why?\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & a^\\top x ≤ b,\n\\end{split}\n\nwhere a \\neq 0\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & l \\preceq x \\preceq u,\n\\end{split}\n\nwhere l \\preceq u\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered as a simplest portfolio optimization problem.\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = \\alpha, \\\\\n& 0 \\preceq x \\preceq 1,\n\\end{split}\n\nwhere \\alpha is an integer between 0 and n. What happens if \\alpha is not an integer (but satisfies 0 \\leq \\alpha \\leq n)? What if we change the equality to an inequality 1^\\top x \\leq \\alpha?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0. What is the solution if the problem is not convex (A \\notin \\mathbb{S}^n_{++}) (Hint: consider eigendecomposition of the matrix: A = Q \\mathbf{diag}(\\lambda)Q^\\top = \\sum\\limits_{i=1}^n \\lambda_i q_i q_i^\\top) and different cases of \\lambda &gt;0, \\lambda=0, \\lambda&lt;0?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& x^\\top Bx \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, B \\in \\mathbb{S}^n_{+}.\nConsider the equality constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{C}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nLet X \\in \\mathbb{R}^{m \\times n} with \\text{rk} X = n, \\Omega \\in \\mathbb{S}_{++}^n, and W \\in \\mathbb{R}^{k \\times n}. Find matrix G \\in \\mathbb{R}^{k \\times m}, which solves the following optimization problem:\n\nf(G) = \\text{tr} \\left(G \\Omega G^\\top \\right) \\to \\min\\limits_{GX = W}\n 1.Consider the problem of projection some point y \\in \\mathbb{R}^n, y \\notin \\Delta^n onto the probability simplex \\Delta^n. Find 2 ways to solve the problem numerically and compare them in terms of the total computational time, memory requirements and iteration number for n = 10, 100, 1000.\n\n\\begin{split}\n& \\|x - y \\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}",
    "crumbs": [
      "Exercises",
      "General optimization problems"
    ]
  },
  {
    "objectID": "docs/exercises/line_search.html",
    "href": "docs/exercises/line_search.html",
    "title": "Line search",
    "section": "",
    "text": "Line search\n\nWhich function is called unimodal?\nDerive the convergence speed for a dichotomy method for a unimodal function. What type of convergence does this method have?\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0].\n\n\n\nIllustration\n\n\nConsider the following modification of solution localization method, in which the interval [a,b] is divided into 2 parts in a fixed proportion of t: x_t = a + t*(b-a) (maximum twice at iteration - as in the dichotomy method). Experiment with different values of t \\in [0,1] and plot the dependence of N (t) - the number of iterations needed to achieve \\varepsilon - accuracy from the t parameter. Consider \\varepsilon = 10^{-7}. Note that with t = 0.5 this method is exactly the same as the dichotomy method.\nDescribe the idea of successive parabolic interpolation. What type of convergence does this method have?\nWrite down Armijo–Goldstein condition.\nShow that if 0 &lt; c_2 &lt; c_1 &lt; 1, there may be no step lengths that satisfy the Wolfe conditions (sufficient decrease and curvature condition).\nShow that the one-dimensional minimizer of a strongly convex quadratic function always satisfies the Goldstein conditions.\nConsider the Rosenbrock function:\n\nf(x_1, x_2) =  10(x_2 − x_1^2)^2 + (x_1 − 1)^22\n\nYou are given the starting point x_0 = (-1, 2)^\\top. Implement the gradient descent algorithm:\n\nx^{k+1} = x^k - \\alpha^k \\nabla f(x^k),\n\nwhere the stepsize is choosen at each iteration via solution of the following line search problem\n\n\\alpha^k = \\arg\\min\\limits_{\\alpha \\in \\mathbb{R}^+}{f(x^k - \\alpha \\nabla f(x^k))}.\n\nImplement any line search method in this problem and plot 2 graphs: function value from iteration number and function value from the number of function calls (calculate only the function calls, don’t include the gradient calls).\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0] \n\nImplement golden search and binary search methods for this function.\nMinimize the function with these two methods and add Brent method from scipy. Compare 3 methods in terms of iterations, time, number of oracle calls.",
    "crumbs": [
      "Exercises",
      "Line search"
    ]
  },
  {
    "objectID": "docs/exercises/projection.html",
    "href": "docs/exercises/projection.html",
    "title": "Projection",
    "section": "",
    "text": "Projection\n\nLet us have two different points a, b \\in \\mathbb{R}^n. Prove that the set of points which in the Euclidean norm are closer to the point a than to b make up a half-space. Is this true for another norm?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S\nIllustrate the geometric inequality that connects \\pi_S(y), y \\notin S, x \\in S, from which it follows that \\pi_S(y) is a projection of the y point onto a convex set of S.\nFor which sets does the projection of the point outside this set exist? Unique?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x \\ge b \\}\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid x = x_0 + X \\alpha, X \\in \\mathbb{R}^{n \\times m}, \\alpha \\in \\mathbb{R}^{m}\\}, y \\in S\nLet S \\subseteq \\mathbb{R}^n be a closed set, and x \\in \\mathbb{R}^n be a point not lying in it. Show that the projection in l_2 norm will be unique, while in l_\\infty norm this statement is not valid.\nFind the projection of the matrix X on a set of matrices of rank k, \\;\\;\\; X \\in \\mathbb{R}^{m \\times n}, k \\leq n \\leq m. In Frobenius norm and spectral norm.\nFind a projection of the X matrix on a set of symmetrical positive semi-definite matrices of X \\in \\mathbb{R}^{n \\times n}. In Frobenius norm and the scalar product associated with it.\nFind the projection \\pi_S(y) of point y onto the set S = \\{x_1, x_2 \\in \\mathbb{R}^2 \\mid \\mid \\vert x_1\\vert + \\vert x_2\\vert = 1 \\} in \\| \\cdot \\|_1 norm. Consider the different positions of y.\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\le x_i \\le \\beta_i, i = 1, \\ldots, n \\}.\nProve that projection is a nonexpansive operator, i.e. prove, that if S \\in \\mathbb{R}^{n} is nonempty, closed and convex set, then for any (x_{1}, x_{2}) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}\n\n\\lVert \\pi_{S}(x_{2}) - \\pi_{S}(x_{1}) \\rVert_{2} \\leq \\lVert x_{2} - x_{1} \\rVert_{2}",
    "crumbs": [
      "Exercises",
      "Projection"
    ]
  },
  {
    "objectID": "docs/exercises/subgradient.html",
    "href": "docs/exercises/subgradient.html",
    "title": "Subgradient and subdifferential",
    "section": "",
    "text": "Subgradient and subdifferential\n\nProve, that x_0 - is the minimum point of a convex function f(x) if and only if 0 \\in \\partial f(x_0)\nFind \\partial f(x), if f(x) = \\text{ReLU}(x) = \\max \\{0, x\\}\nFind \\partial f(x), if f(x) = \\text{Leaky ReLU}(x) = \\begin{cases}  x & \\text{if } x &gt; 0, \\\\  0.01x & \\text{otherwise}. \\end{cases}\nFind \\partial f(x), if f(x) = \\|x\\|_p при p = 1,2, \\infty\nFind \\partial f(x), if f(x) = \\|Ax - b\\|_1^2\nFind \\partial f(x), if f(x) = e^{\\|x\\|}. Try do the task for an arbitrary norm. At least, try \\|\\cdot\\| = \\|\\cdot\\|_{\\{2,1,\\infty\\}}.\nDescribe the connection between subgradient of a scalar function f: \\mathbb{R} \\to \\mathbb{R} and global linear lower bound, which support (tangent) the graph of the function at a point.\nWhat can we say about subdifferential of a convex function in those points, where the function is differentiable?\nDoes the subgradient coincide with the gradient of a function if the function is differentiable? Under which condition it holds?\nIf the function is convex on S, whether \\partial f(x) \\neq \\emptyset \\;\\;\\; \\forall x \\in S always holds or not?\nFind \\partial f(x), if f(x) = x^3\nFind f(x) = \\lambda_{max} (A(x)) = \\sup\\limits_{\\|y\\|_2 = 1} y^T A(x)y, где A(x) = A_0 + x_1A_1 + \\ldots + x_nA_n, all the matrices A_i \\in \\mathbb{S}^k are symmetric and defined.\nFind subdifferential of a function f(x,y) = x^2 + xy + y^2 + 3\\vert x + y − 2\\vert at points (1,0) and (1,1).\nFind subdifferential of a function f(x) = \\sin x on the set X = [0, \\frac32 \\pi].\nFind subdifferential of a function f(x) = \\vert c^{\\top}x\\vert, \\; x \\in \\mathbb{R}^n.\nFind subdifferential of a function f(x) = \\|x\\|_1, \\; x \\in \\mathbb{R}^n.\nSuppose, that if f(x) = \\|x\\|_\\infty. Prove that \n\\partial f(0) = \\textbf{conv}\\{\\pm e_1, \\ldots , \\pm e_n\\},\n where e_i is i-th canonical basis vector (column of identity matrix).",
    "crumbs": [
      "Exercises",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/exercises/zom.html",
    "href": "docs/exercises/zom.html",
    "title": "Zero order methods",
    "section": "",
    "text": "Zero order methods\n\nImplement Rastrigin function f: \\mathbb{R}^d \\to \\mathbb{R} for d = 10. link\n\nf(\\mathbf{x})=10 d+\\sum_{i=1}^{d}\\left[x_{i}^{2}-10 \\cos \\left(2 \\pi x_{i}\\right)\\right]\n\n\nConsider global optimization from here.\nPlot 4 graphs for different d from {10, 100, 1000, 10000}. On each graph you are to plot f from N_{fev} for 5 methods: basinhopping, brute, differential_evolution, shgo, dual_annealing from scipy, where N_{fev} - the number of function evaluations. This information is usually available from specific_optimizer.nfev. If you will need bounds for the optimizer, use x_i \\in [-5, 5].\nNote, that it is crucial to fix seed and to use the same starting point for fair comparison.\n\nMachine learning models often have hyperparameters. To choose optimal one between them one can use GridSearch or RandomSearch. But these algorithms computationally uneffective and don’t use any sort of information about type of optimized function. To overcome this problem one can use bayesian optimization. Using this method we optimize our model by sequentially chosing points based on prior information about function.\n\n\n\nImage\n\n\nIn this task you will use optuna package for hyperparameter optimization RandomForestClassifier. Your task is to find best Random Forest model varying at least 3 hyperparameters on iris dataset. Examples can be find here or here\n!pip install optuna\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\nimport sklearn.svm\n\nimport optuna\n\niris = sklearn.datasets.load_iris()\nx, y = iris.data, iris.target\nTry to perform hyperparameter optimization in context of any metric for imbalanced classification problem with optuna and keras. Open In Colab{: .btn }\nLet’s arrange the base stations of the wireless network optimally! Suppose you have N_{obj} = 10 clusters of 10 subscribers each. Let us use a genetic algorithm to gradually search for the optimal number and location of base stations in order to minimize the cost of arranging such stations.\nBelow is one possible implementation of the genetic algorithm.\nPopulation\nThis is a list of arrays of size [N_stations x 2]. Each individual in this case is a set of station coordinates on the plane. Generation of a random\nMutation\nDefined by the function mutation(). A mutation_rate part is selected from all individuals and a random Gaussian noise is added to the mutation_rate part of its stations. An individual with a random number of stations with random coordinates is then added to the population.\nCrossing\nDefined by children_creation() and breed(). Two sets of stations are matched with a third station, from which the even stations of one parent and the odd stations of the other are taken.\nEstimation of the value of an individual\nDefined by evaluate_generation(). The total cost corresponding to a particular individual is made up of the cost of building base stations (each cost station_cost) minus the profit from each client. The profit from each client is inversely proportional to the distance to “his” base station. Each customer joins only one (closest) base station using find_nearest_station(). In addition, the profit from each subscriber is inversely proportional to the number of subscribers at a given base station (each station has the number of subscribers stations_load connected to it). Note also that, starting from a certain proximity to the subscriber to the base station, the client’s profit ceases to grow (in our algorithm, it is the same in the radius of 0.1 from the base station, then linearly decreases).\nYour task is to come up with any modifications to the proposed procedures within the genetic algorithm so that the final quality of the algorithm is better. Suggest, describe, and test ideas for improving the algorithm.\n%matplotlib notebook\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom random import shuffle, sample\nfrom copy import deepcopy\nimport random\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\ndef generate_problem(N_obj, N_abon_per_cluster):\n    abonents = np.zeros((N_obj*N_abon_per_cluster,2))\n    for i_obj in range(N_obj):\n        center = np.random.random(2)\n        cov    = np.random.random((2,2))*0.1\n        cov    = cov @ cov.T\n        xs, ys = np.random.multivariate_normal(center, cov, N_abon_per_cluster).T\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 0] = xs\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 1] = ys\n    return abonents\n\ndef plot_problem(abonents):\n    plt.figure(figsize=(10,6))\n    plt.plot(abonents[:,0], abonents[:,1], 'go')\n    plt.title('The village')\n#     plt.savefig('bs_problem.svg')\n    plt.show()\n\ndef random_solution(abonents, N_solutions = 100):\n    x_min, x_max = abonents[:,0].min(), abonents[:,0].max()\n    y_min, y_max = abonents[:,1].min(), abonents[:,1].max()\n    population = []\n\n    for i_sol in range(N_solutions):\n        N_stations = int(np.random.random(1)[0]*10)+1\n        stations = np.zeros((N_stations,2))\n        stations[:,0], stations[:,1] = np.random.random(N_stations)*(x_max - x_min), np.random.random(N_stations)*(y_max - y_min)\n        population.append(stations)\n    return population\n\ndef find_nearest_station(dist_matrix):\n    return np.argmin(dist_matrix, axis=1)\n\ndef pairwise_distance(abonents, stations):\n    return cdist(abonents, stations)\n\ndef evaluate_generation(abonents, population, station_cost = 1, abonent_profit_base = 1):  \n    costs = []\n    for creature in population:\n        N_stations, N_users = len(creature), len(abonents)\n        total_cost          = N_stations*station_cost\n        dist_matrix         = pairwise_distance(abonents, creature)\n        stations_assignment = find_nearest_station(dist_matrix)\n        stations_load       = np.ones(N_stations)\n        stations_load       = np.array([1/(sum(stations_assignment == i_st)+1) for i_st, st in enumerate(stations_load)])\n\n        for i_ab, abonent in enumerate(abonents):\n            dist_to_base = dist_matrix[i_ab, stations_assignment[i_ab]]\n            total_cost  -= stations_load[stations_assignment[i_ab]]*abonent_profit_base/(max(0.1, dist_to_base))\n\n        costs.append(total_cost)\n    return np.array(costs)\n\ndef mutation(population, mutation_rate = 0.3):\n    N_creatures = len(population)\n    x_min, x_max = -1, 1\n    y_min, y_max = -1, 1\n    mutated_creatures = sample(range(N_creatures), int(mutation_rate*N_creatures))\n    for i_mut in mutated_creatures:\n        N_stations = len(population[i_mut])\n        mutated_stations = sample(range(N_stations), int(mutation_rate*N_stations))\n        for i_st_mut in mutated_stations:\n            population[i_mut][i_st_mut] += np.random.normal(0, 0.01, 2)\n\n    N_new_stations = max(1, int(random.random()*mutation_rate*N_creatures))\n    for i in range(N_new_stations):\n        new_stations = np.zeros((N_new_stations,2))\n        new_stations[:,0], new_stations[:,1] = np.random.random(N_new_stations)*(x_max - x_min), np.random.random(N_new_stations)*(y_max - y_min)\n        population.append(new_stations)\n    return population\n\ndef children_creation(parent1, parent2):\n    # whoisbatya\n    batya = random.random() &gt; 0.5\n    if batya:\n        child = np.concatenate((parent1[::2], parent2[1::2]))\n    else:\n        child = np.concatenate((parent1[1::2], parent2[::2]))\n    return np.array(child)\n\ndef breed(population):\n    new_population = deepcopy(population)\n    random.shuffle(new_population)\n    N_creatures = len(population)\n    for i in range(N_creatures//2):\n        children = children_creation(population[i], population[i+1])\n        new_population.append(children)\n    return new_population\n\ndef selection(abonents, population, offsprings = 10):\n    scores = evaluate_generation(abonents, population)\n    best = np.array(scores).argsort()[:offsprings].tolist()\n    return [population[i_b] for i_b in best], population[best[0]] \n\n\ndef let_eat_bee(N_creatures, N_generations, N_obj = 10, N_abon_per_cluster = 10):\n    abonents = generate_problem(N_obj, N_abon_per_cluster)\n\n    costs_evolution = np.zeros((N_generations, N_creatures))\n    population = random_solution(abonents, N_creatures)\n    best_creatures = []\n    for generation in range(N_generations):\n        population                = mutation(population)\n        population                = breed(population)\n        population, best_creature = selection(abonents, population, N_creatures)\n        best_creatures.append(best_creature)\n\n        costs_evolution[generation, :] = evaluate_generation(abonents, population)\n\n        # Plotting\n        x_min, x_max = 0, 1\n        y_min, y_max = 0,1\n        cost_min  = [np.min(costs_evolution[i])  for i in range(generation)]\n        cost_max  = [np.max(costs_evolution[i])  for i in range(generation)]\n        cost_mean = [np.mean(costs_evolution[i]) for i in range(generation)]\n\n        fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Topology of the best solution\", \"Cost function\"))\n        fig.update_xaxes(title_text=\"x\", range = [x_min,x_max],  row=1, col=1)\n        fig.update_yaxes(title_text=\"y\", range = [y_min,y_max], row=1, col=1)\n        fig.update_yaxes(title_text=\"Total cost\", row=1, col=2)\n        fig.update_xaxes(title_text=\"Generation\", row=1, col=2)\n\n        fig.add_trace(\n            go.Scatter(x=abonents[:, 0], y=abonents[:, 1], mode='markers', name='abonents',  marker=dict(size=5)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x=best_creatures[generation][:, 0], y=best_creatures[generation][:, 1], mode='markers', name='stations', marker=dict(size=15)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_min, name='best'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_max, name='worst'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_mean, name='mean'),\n            row=1, col=2\n        )\n\n        clear_output(wait=True)\n        fig.show()\n\n    fig.write_html(\"test.html\")    \n    return costs_evolution, abonents, best_creatures\n\n\ncosts_evolution, abonents, best_creatures = let_eat_bee(200, 200)",
    "crumbs": [
      "Exercises",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html",
    "href": "docs/materials/tutorials/Colab tutorial.html",
    "title": "1 Github features",
    "section": "",
    "text": "This tutorial will highlight some\nThis tool perfectly fits 90% of your tasks. * Fast prototyping * Sharing and simultaneous work",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#running-any-jupyter-notebook-from-github",
    "href": "docs/materials/tutorials/Colab tutorial.html#running-any-jupyter-notebook-from-github",
    "title": "1 Github features",
    "section": "1.1 Running any jupyter notebook from github",
    "text": "1.1 Running any jupyter notebook from github\nIf you want to open any notebook, that is already stored in any public github repository, it is enough to paste user/repo name in the field below and just open it. Modified notebook could be downloaded locally or saved to your google drive storage.",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#commiting-to-github-from-colab",
    "href": "docs/materials/tutorials/Colab tutorial.html#commiting-to-github-from-colab",
    "title": "1 Github features",
    "section": "1.2 Commiting to github from colab",
    "text": "1.2 Commiting to github from colab\nHowever, you can save any changes directly to the github through the commits. In order to do this, you’ll need to authorize colab to work with your github account. It’s up to you to provide this access or not. All these things work even with private repos.",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#dark-theme",
    "href": "docs/materials/tutorials/Colab tutorial.html#dark-theme",
    "title": "1 Github features",
    "section": "3.1 Dark theme",
    "text": "3.1 Dark theme\nTools -&gt; Settings -&gt; Site -&gt; Theme -&gt; Dark",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#monokai",
    "href": "docs/materials/tutorials/Colab tutorial.html#monokai",
    "title": "1 Github features",
    "section": "3.2 Monokai",
    "text": "3.2 Monokai\nTools -&gt; Settings -&gt; Site -&gt; Editor -&gt; Editor colorization",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#bonus",
    "href": "docs/materials/tutorials/Colab tutorial.html#bonus",
    "title": "1 Github features",
    "section": "3.3 Bonus",
    "text": "3.3 Bonus\nTools -&gt; Settings -&gt; Miscellaneous -&gt; Kitty mode",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html",
    "href": "docs/methods/Simplex.html",
    "title": "1 What is LP",
    "section": "",
    "text": "Generally speaking, all problems with linear objective and linear equalitiesconstraints could be considered as Linear Programming. However, there are some widely accepted formulations.\n\n\\tag{LP.Basic}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n\\end{align*}\n\n\n\n\nIllustration\n\n\nfor some vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}. Where the inequalities are interpreted component-wise.\n\n\nThis form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\n\n\\tag{LP.Canonical}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\n\n\nImagine, that you have to construct a diet plan from some set of products: 🍌🍰🍗🥚🐟. Each of the products has its own vector of nutrients. Thus, all the food information could be processed through the matrix W. Let also assume, that we have the vector of requirements for each of nutrients r \\in \\mathbb{R}^n. We need to find the cheapest configuration of the diet, which meets all the requirements:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^p} c^{\\top}x \\\\\n\\text{s.t. } & Wx \\geq r\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#standard-form",
    "href": "docs/methods/Simplex.html#standard-form",
    "title": "1 What is LP",
    "section": "",
    "text": "This form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#canonical-form",
    "href": "docs/methods/Simplex.html#canonical-form",
    "title": "1 What is LP",
    "section": "",
    "text": "\\tag{LP.Canonical}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#real-world-problems",
    "href": "docs/methods/Simplex.html#real-world-problems",
    "title": "1 What is LP",
    "section": "",
    "text": "Imagine, that you have to construct a diet plan from some set of products: 🍌🍰🍗🥚🐟. Each of the products has its own vector of nutrients. Thus, all the food information could be processed through the matrix W. Let also assume, that we have the vector of requirements for each of nutrients r \\in \\mathbb{R}^n. We need to find the cheapest configuration of the diet, which meets all the requirements:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^p} c^{\\top}x \\\\\n\\text{s.t. } & Wx \\geq r\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#basic-transformations",
    "href": "docs/methods/Simplex.html#basic-transformations",
    "title": "1 What is LP",
    "section": "2.1 Basic transformations",
    "text": "2.1 Basic transformations\nInequality to equality by increasing the dimension of the problem by m.\n\nAx \\leq b \\leftrightarrow\n\\begin{cases}\nAx + z =  b\\\\\nz \\geq 0\n\\end{cases}\n\nunsigned variables to nonnegative variables.\n\nx \\leftrightarrow\n\\begin{cases}\nx = x_+ - x_-\\\\\nx_+ \\geq 0 \\\\\nx_- \\geq 0\n\\end{cases}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#chebyshev-approximation-problem",
    "href": "docs/methods/Simplex.html#chebyshev-approximation-problem",
    "title": "1 What is LP",
    "section": "2.2 Chebyshev approximation problem",
    "text": "2.2 Chebyshev approximation problem\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_\\infty \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\max_{i} |a_i^\\top x - b_i|\n\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}, x \\in \\mathbb{R}^n} t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#l_1-approximation-problem",
    "href": "docs/methods/Simplex.html#l_1-approximation-problem",
    "title": "1 What is LP",
    "section": "2.3 l_1 approximation problem",
    "text": "2.3 l_1 approximation problem\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_1 \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^n |a_i^\\top x - b_i|\n\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}^n, x \\in \\mathbb{R}^n} \\mathbf{1}^\\top t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t_i, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t_i, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#main-lemma",
    "href": "docs/methods/Simplex.html#main-lemma",
    "title": "1 What is LP",
    "section": "3.1 Main lemma",
    "text": "3.1 Main lemma\nIf all components of \\lambda_B are non-positive and B is feasible, then B is optimal.\nProof:\n\n\\begin{align*}\n\\exists x^*: Ax^* &\\leq b, c^\\top x^* &lt; c^\\top x_B \\\\\nA_B x^* &\\leq b_B \\\\\n\\lambda_B^\\top A_B x^* &\\geq \\lambda_B^\\top b_B \\\\\nc^\\top x^* & \\geq \\lambda_B^\\top A_B x_B \\\\\nc^\\top x^* & \\geq c^\\top  x_B \\\\\n\\end{align*}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#changing-basis",
    "href": "docs/methods/Simplex.html#changing-basis",
    "title": "1 What is LP",
    "section": "3.2 Changing basis",
    "text": "3.2 Changing basis\nSuppose, some of the coefficients of \\lambda_B are positive. Then we need to go through the edge of the polytope to the new vertex (i.e., switch the basis)\n\n\n\nIllustration\n\n\n\nx_{B'} = x_B + \\mu d = A^{-1}_{B'} b_{B'}",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#finding-an-initial-basic-feasible-solution",
    "href": "docs/methods/Simplex.html#finding-an-initial-basic-feasible-solution",
    "title": "1 What is LP",
    "section": "3.3 Finding an initial basic feasible solution",
    "text": "3.3 Finding an initial basic feasible solution\nLet us consider \\text{LP.Canonical}.\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\nThe proposed algorithm requires an initial basic feasible solution and corresponding basis. To compute this solution and basis, we start by multiplying by −1 any row i of Ax = b such that b_i &lt; 0. This ensures that b \\geq 0. We then introduce artificial variables z \\in \\mathbb{R}^m and consider the following LP:\n\n\\tag{LP.Phase 1}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}^m} 1^{\\top}z \\\\\n\\text{s.t. } & Ax + Iz = b\\\\\n& x_i, z_j \\geq 0, \\; i = 1,\\dots, n \\; j = 1,\\dots, m\n\\end{align*}\n\nwhich can be written in canonical form \\min\\{\\tilde{c}^\\top \\tilde{x} \\mid \\tilde{A}\\tilde{x} = \\tilde{b}, \\tilde{x} \\geq 0\\} by setting\n\n\\tilde{x} = \\begin{bmatrix}x\\\\z\\end{bmatrix}, \\quad \\tilde{A} = [A \\; I], \\quad \\tilde{b} = b, \\quad \\tilde{c} = \\begin{bmatrix}0_n\\\\1_m\\end{bmatrix}\n\nAn initial basis for \\text{LP.Phase 1} is \\tilde{A}_B = I, \\tilde{A}_N = A with corresponding basic feasible solution \\tilde{x}_N = 0, \\tilde{x}_B = \\tilde{A}^{-1}_B \\tilde{b} = \\tilde{b} \\geq 0. We can therefore run the simplex method on \\text{LP.Phase 1}, which will converge to an optimum \\tilde{x}^*. \\tilde{x} = (\\tilde{x}_N \\; \\tilde{x}_B). There are several possible outcomes:\n\n\\tilde{c}^\\top \\tilde{x} &gt; 0. Original primal is infeasible.\n\\tilde{c}^\\top \\tilde{x} = 0 \\to 1^\\top z^* = 0. The obtained solution is a start point for the original problem (probably with slight modification).",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#klee-minty-example",
    "href": "docs/methods/Simplex.html#klee-minty-example",
    "title": "1 What is LP",
    "section": "4.1 Klee Minty example",
    "text": "4.1 Klee Minty example\nIn the following problem simplex algorithm needs to check 2^n - 1 vertexes with x_0 = 0.\n\n\\begin{align*} & \\max_{x \\in \\mathbb{R}^n} 2^{n-1}x_1 + 2^{n-2}x_2 + \\dots + 2x_{n-1} + x_n\\\\\n\\text{s.t. } & x_1 \\leq 5\\\\\n& 4x_1 + x_2 \\leq 25\\\\\n& 8x_1 + 4x_2 + x_3 \\leq 125\\\\\n& \\ldots\\\\\n& 2^n x_1 + 2^{n-1}x_2 + 2^{n-2}x_3 + \\ldots + x_n \\leq 5^n\\ & x \\geq 0\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#strong-duality",
    "href": "docs/methods/Simplex.html#strong-duality",
    "title": "1 What is LP",
    "section": "4.2 Strong duality",
    "text": "4.2 Strong duality\nThere are four possibilities:\n\nBoth the primal and the dual are infeasible.\nThe primal is infeasible and the dual is unbounded.\nThe primal is unbounded and the dual is infeasible.\nBoth the primal and the dual are feasible and their optimal values are equal.",
    "crumbs": [
      "Methods",
      "LP and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "title": "1 Intuition",
    "section": "",
    "text": "1 Intuition\nLet’s consider illustrative example of a simple function of 2 variables:\n\nf(x_1, x_2) = 2x_1 + \\frac{1}{3}x_2, \\quad \\nabla_x f = \\begin{pmatrix} 2\\\\ \\frac{1}{3} \\end{pmatrix}\n\nNow, let’s introduce new variables $(y_1, y_2) = (2x_1, x_2) $ or y = Bx, where B = \\begin{pmatrix} 2 & 0\\\\ 0 & \\frac{1}{3} \\end{pmatrix}. The same function, written in the new coordinates, is\n\nf(y_1, y_2) = y_1 + y_2, \\quad \\nabla_y f = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}\n\nLet’s summarize what happened:\n\nWe have a transformation of a vector space described by a coordinate transformation matrix B.\nCoordinate vectors transforms as y = Bx.\nHowever, the partial gradient of a function w.r.t. the coordinates transforms as \\frac{\\partial f}{\\partial y} = B^{-\\top} \\frac{\\partial f}{\\partial x}.\nTherefore, there seems to exist one type of mathematical objects (e.g. coordinate vectors) which transform with B, and a second type of mathematical objects (e.g. the partial gradient of a function w.r.t. coordinates) which transform with B^{-\\top}.\n\nThese two types are called contra-variant and co-variant. This should at least tell us that indeed the so-called “gradient-vector” is somewhat different to a “normal vector”: it behaves inversely under coordinate transformations.\nNice thing here is that steepest descent direction A_x^{-1}\\nabla_x f on a sphere transforms as a covariant vector, since A_y = B^{-\\top} A_x B^{-1}:\n\n\\begin{split}\nA_y^{-1}\\nabla_y f = \\\\\n(B^{-\\top} A_x B^{-1})^{-1} B^{-\\top} \\nabla_x f = \\\\\nB A_x^{-1} B^\\top B^{-\\top} \\nabla_x f = \\\\\nB (A_x^{-1} \\nabla_x f)\n\\end{split}\n\n\n\n2 Steepest descent in distribution space\nSuppose, we have a probabilistic model represented by its likelihood $p(x ) $. We want to maximize this likelihood function to find the most likely parameter \\theta with given observations. Equivalent formulation would be to minimize the loss function \\mathcal{L}(\\theta), which is the negative logarithm of likelihood function.\n\n\n3 Example\n\n\n4 References\n\nSome notes on gradient descent\nNatural Gradient Descent\n\n\n\n5 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "title": "1 Intuition",
    "section": "",
    "text": "1 Intuition\nFor the classic task of unconditional optimization f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} the general scheme of iteration method is written as:\n\nx_{k+1} = x_k + \\alpha_k s_k\n\nIn the Newton method, the s_k direction (Newton’s direction) is set by the linear system solution at each step:\n\ns_k = - B_k\\nabla f(x_k), \\;\\;\\; B_k = f_{xx}^{-1}(x_k)\n\ni.e. at each iteration it is necessary to compensate hessian and gradient and resolve linear system.\nNote here that if we take a single matrix of B_k = I_n as B_k at each step, we will exactly get the gradient descent method.\nThe general scheme of quasi-Newton methods is based on the selection of the B_k matrix so that it tends in some sense at k \\to \\infty to the true value of inverted Hessian in the local optimum f_{xx}^{-1}(x_*). Let’s consider several schemes using iterative updating of B_k matrix in the following way:\n\nB_{k+1} = B_k + \\Delta B_k\n\nThen if we use Taylor’s approximation for the first order gradient, we get it:\n\n\\nabla f(x_k) - \\nabla f(x_{k+1}) \\approx f_{xx}(x_{k+1}) (x_k - x_{k+1}).\n\nNow let’s formulate our method as:\n\n\\Delta x_k = B_{k+1} \\Delta y_k, \\text{ where } \\;\\; \\Delta y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\n\nin case you set the task of finding an update \\Delta B_k:\n\n\\Delta B_k \\Delta y_k = \\Delta x_k - B_k \\Delta y_k\n\n\n\n2 Broyden method\nThe simplest option is when the amendment \\Delta B_k has a rank equal to one. Then you can look for an amendment in the form\n\n\\Delta B_k = \\mu_k q_k q_k^\\top.\n\nwhere \\mu_k is a scalar and q_k is a non-zero vector. Then mark the right side of the equation to find \\Delta B_k for \\Delta z_k:\n\n\\Delta z_k = \\Delta x_k - B_k \\Delta y_k\n\nWe get it:\n\n\\mu_k q_k q_k^\\top \\Delta y_k = \\Delta z_k\n\n\n\\left(\\mu_k \\cdot q_k^\\top \\Delta y_k\\right) q_k = \\Delta z_k\n\nA possible solution is: q_k = \\Delta z_k, \\mu_k = \\left(q_k^\\top \\Delta y_k\\right)^{-1}.\nThen an iterative amendment to Hessian’s evaluation at each iteration:\n\n\\Delta B_k = \\dfrac{(\\Delta x_k - B_k \\Delta y_k)(\\Delta x_k - B_k \\Delta y_k)^\\top}{\\langle \\Delta x_k - B_k \\Delta y_k , \\Delta y_k\\rangle}.\n\n\n\n3 Davidon–Fletcher–Powell method\n\n\\Delta B_k = \\mu_1 \\Delta x_k (\\Delta x_k)^\\top + \\mu_2 B_k \\Delta y_k (B_k \\Delta y_k)^\\top.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle}.\n\n\n\n4 Broyden–Fletcher–Goldfarb–Shanno method\n\n\\Delta B_k = Q U Q^\\top, \\quad Q = [q_1, q_2], \\quad q_1, q_2 \\in \\mathbb{R}^n, \\quad U = \\begin{pmatrix} a & c\\\\ c & b \\end{pmatrix}.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle} + p_k p_k^\\top.\n\n\n\n5 Code\n\nOpen In Colab\nComparison of quasi Newton methods",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/fom/ADAM.html",
    "href": "docs/methods/fom/ADAM.html",
    "title": "",
    "section": "",
    "text": "Adam is the stochastic first order optimization algorithm, that uses historical information about stochastic gradients and incorporates it in attempt to estimate second order moment of stochastic gradients.\n\n\\tag{ADAM}\n\\begin{align*}\nx_{k+1} &= x_k - \\alpha_k \\dfrac{\\widehat{m_k}}{\\sqrt{\\widehat{v_k}} + \\epsilon} \\\\\n\\tag{First moment estimation}\n\\widehat{m_k} &= \\dfrac{m_k}{1 - \\beta_1^k} \\\\\nm_k &= \\beta_1 m_{k-1} + (1 - \\beta_1) g_k \\\\\n\\tag{Second moment estimation}\n\\widehat{v_k} &= \\dfrac{v_k}{1 - \\beta_2^k} \\\\\nv_k &= \\beta_2 v_{k-1} + (1 - \\beta_2)g_k^2 \\\\\n\\end{align*}\n\nAll vector operations are element-wise. \\alpha = 0.001, \\beta_1 = 0.9, \\beta_2 = 0.999 - the default values for hyperparameters (\\epsilon here is needed for avoiding zero division problems) and g_k = \\nabla f(x_k, \\xi_k) is the sample of stochastic gradient.\n\nWe can consider this approach as normalization of each parameter by using individual learning rates on $ (0,1)$, since \\mathbb{E}\\_{\\xi_k}[g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{m_k}] and \\mathbb{E}\\_{\\xi_k}[g_k \\odot g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{v_k}].\nThere are some issues with Adam effectiveness and some works, stated, that adaptive metrics methods could lead to worse generalization.\nThe name came from “Adaptive Moment estimation”.",
    "crumbs": [
      "Methods",
      "First order methods",
      "ADAM: A Method for Stochastic Optimization"
    ]
  },
  {
    "objectID": "docs/methods/fom/ADAM.html#summary",
    "href": "docs/methods/fom/ADAM.html#summary",
    "title": "",
    "section": "",
    "text": "Adam is the stochastic first order optimization algorithm, that uses historical information about stochastic gradients and incorporates it in attempt to estimate second order moment of stochastic gradients.\n\n\\tag{ADAM}\n\\begin{align*}\nx_{k+1} &= x_k - \\alpha_k \\dfrac{\\widehat{m_k}}{\\sqrt{\\widehat{v_k}} + \\epsilon} \\\\\n\\tag{First moment estimation}\n\\widehat{m_k} &= \\dfrac{m_k}{1 - \\beta_1^k} \\\\\nm_k &= \\beta_1 m_{k-1} + (1 - \\beta_1) g_k \\\\\n\\tag{Second moment estimation}\n\\widehat{v_k} &= \\dfrac{v_k}{1 - \\beta_2^k} \\\\\nv_k &= \\beta_2 v_{k-1} + (1 - \\beta_2)g_k^2 \\\\\n\\end{align*}\n\nAll vector operations are element-wise. \\alpha = 0.001, \\beta_1 = 0.9, \\beta_2 = 0.999 - the default values for hyperparameters (\\epsilon here is needed for avoiding zero division problems) and g_k = \\nabla f(x_k, \\xi_k) is the sample of stochastic gradient.\n\nWe can consider this approach as normalization of each parameter by using individual learning rates on $ (0,1)$, since \\mathbb{E}\\_{\\xi_k}[g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{m_k}] and \\mathbb{E}\\_{\\xi_k}[g_k \\odot g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{v_k}].\nThere are some issues with Adam effectiveness and some works, stated, that adaptive metrics methods could lead to worse generalization.\nThe name came from “Adaptive Moment estimation”.",
    "crumbs": [
      "Methods",
      "First order methods",
      "ADAM: A Method for Stochastic Optimization"
    ]
  },
  {
    "objectID": "docs/methods/fom/ADAM.html#bounds",
    "href": "docs/methods/fom/ADAM.html#bounds",
    "title": "",
    "section": "2 Bounds",
    "text": "2 Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert \\mathbb{E} [f(x_k)] - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert \\mathbb{E}[x_k] - x^* \\Vert \\leq\n\n\n\n\nConvex\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right)\nSublinear\n\n\n\n\nVersion of Adam for a strongly convex functions is considered in this work. The obtained rate is \\mathcal{O}\\left(\\dfrac{\\log k}{\\sqrt{k}} \\right), while the version for truly linear rate remains undiscovered.",
    "crumbs": [
      "Methods",
      "First order methods",
      "ADAM: A Method for Stochastic Optimization"
    ]
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html",
    "href": "docs/methods/fom/Lookahead.html",
    "title": "1 Summary",
    "section": "",
    "text": "1 Summary\nThe lookahead method provides an interesting way to accelerate and stabilize algorithms of stochastic gradient descent family. The main idea is quite simple:\n\nSet some number k. Take initial parameter weights x_0 = \\hat{x}_0\nDo k steps with your favorite optimization algorithm: \\hat{x}_1, \\ldots, \\hat{x}_k\nTake some value between initial x_0 and \\hat{x}_k:\n\n  x_{t+1} = (1 - \\alpha)x_{t} + \\alpha\\hat{x_k}\n  \nUpdate \\hat{x_0} with the last output of the algorithm.\nRepeat profit\n\nAuthors introduced separation on the fast weights and slow weights, which naturally arise in the described procedure. The paper contains proof for optimal step-size of the quadratic loss function and provides understanding why this technique could reduce variance of {% include link.html title=“Stochastic gradient descent” %} in the noisy quadratic case. Moreover, this work compares the convergence rate in dependency of condition number of the squared system.\nIt is worth to say, that author claims significant improvement in practical huge scale settings (ImageNet, CIFAR10,CIFAR100)\n \n\n\n2 Pros\n\nInteresting idea, costs almost nothing, why not to try?\nWorks with any SGD-like optimizer (SGD, Adam, RmsProp)\nAnalytical approach to quadratic case.\nWide set of empirical tests (Image classification, Neural Translation, LSTM training)\n\n\n\n3 Cons\n\nLack of test loss pictures, the majority of them obtained for the train loss/accuracy\nLack of pictures with different batch sizes\nDifficult to analyze the method analytically",
    "crumbs": [
      "Methods",
      "First order methods",
      "Lookahead Optimizer: $k$ steps forward, $1$ step back"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html",
    "href": "docs/methods/fom/Projected_subgradient_descent.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#recap",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#recap",
    "title": "1 Intuition",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#introduction",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#introduction",
    "title": "1 Intuition",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nВ этом разделе мы будем рассматривать работу в рамках какого-то выпуклого множества S \\in \\mathbb{R}^n, так, чтобы x_k \\in S. Запишем для начала соотношение для итераций:\n\n\\begin{align*}\n\\|x_{k+1} - x^*\\|^2 &= \\|(x_{k+1} - x_k) + (x_k - x^*)\\|^2 = \\\\\n&= \\|x_k - x_{k+1}\\|^2 + \\|x_k - x^*\\|^2 - 2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle \\\\\n2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle &=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2\n\\end{align*}\n\nЗаметим, что при работе на ограниченном множестве у нас появилась небольшая проблема: x_{k+1} может не лежать в бюджетном множестве. Сейчас мы увидим, почему это является проблемой для выписывания оценок на число итераций: если мы имеем неравенство, записанное ниже, то процесс получения оценок будет абсолютно совпадать с описанными выше процедурами (потому что в случае субградиентного метода x_k - x_{k+1} = \\alpha_k g_k).\n\n\\tag{Target}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle\n\nОднако, в нашем случае мы можем лишь получить (будет показано ниже) оценки следующего вида:\n\n\\tag{Forward Target}\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nЭто связано с тем, что x_{k+1} нам легче контролировать при построении условного метода, а значит, легче записать на него оценку. К сожалению, привычной телескопической (сворачивающейся) суммы при таком неравенстве не получится. Однако, если неравенство (Forward Target) выполняется, то из него следует следующее неравенство:\n\n\\begin{align*}\n\\tag{Forward Target Fix}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle &\\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\\\\n& - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nДля того, чтобы доказать его, запишем (Forward Target Fix):\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle + \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\n\\leq  \\\\ \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle + \\langle x_k - x_{k+1}, x_{k+1} - x_k \\rangle\n\\end{align*}\n\nПереписывая его еще раз, получаем:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\|x_{k} - x_{k+1}\\|^2 - \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle = \\\\\n&= \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2}\\left(\\|x_{k} - x_{k+1}\\|^2 + 2\\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\\right) \\leq \\\\\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2} \\left( - \\alpha_k^2 g_k^2\\right) = \\\\\n&= \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nИтак, пускай мы имеем неравенство (Forward Target) - напомню, что мы его пока не доказали. Теперь покажем, как с его помощью получить оценки на сходимость метода. Для этого запишем неравенство (Forward Target Fix):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k, x_k &- x^* \\rangle + \\|x_k - x_{k+1}\\|^2 - \\alpha_k^2 g_k^2 \\leq \\\\\n&\\leq 2\\langle x_k - x_{k+1}, x_k - x^* \\rangle \\\\\n&= \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2 \\\\\n&\\quad \\\\\n2 \\langle \\alpha_k g_k, x_k - x^* \\rangle\n&\\leq \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k^2\n\\end{align*}\n\nЕсли внимательно посмотреть на полученный результат, то это в точности совпадает с исходной точкой доказательства для субградиентного метода в безусловном сеттинге.\nМожем сразу получить оценки:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T} \\\\\nf(\\overline{x}) - f^* &\\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nТаким образом, мы показали, что для метода проекции субградиента справедлива точно такая же оценка на число итераций, если выполняется неравенство (Forward Target) :) Давайте разбираться с ним\nНам следует доказать, что:\n\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nВ более общем случае \\forall y \\in S:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle & \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nВспомним из неравенства для проекции (равно как и условия оптимальности первого порядка), что \\forall y \\in S для некоторой гладкой выпуклой минимизируемой функции g(x) в точке оптимума x \\in S:\n\n\\langle \\nabla g(x), x - y \\rangle \\leq 0\n\nВ противном бы случае, можно было бы сделать градиентный шаг в направлении y -x и уменьшить значение функции.\nРассмотрим теперь следующую функцию g(x):\n\ng(x) = \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2, \\quad \\nabla g(x) = \\alpha_k g_k + x - x_k\n\nИ давайте теперь строить условный алгоритм как минимизацию этой функции:\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nТогда из условия оптимальности:\n\n\\begin{align*}\n\\langle \\nabla g(x_{k+1}), x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k + x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k , x_{k+1} - y \\rangle  + \\langle x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nПолученное неравенство в точности совпадает с неравенством (Forward Target), которое нам как раз таки и следовало доказать. Таким образом, мы получаем",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#algorithm",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#algorithm",
    "title": "1 Intuition",
    "section": "2.2 Algorithm",
    "text": "2.2 Algorithm\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nИнтересные фишки:\n\nТакая же скорость сходимости, как и для безусловного алгоритма. (Однако, стоимость каждой итерации может быть существенно больше из за необходимости решать задачу оптимизации на каждом шаге)\nВ частном случае S = \\mathbb{R}^n в точности совпадает с безусловным алгоритмом (убедитесь)\n\n\n2.2.1 Adaptive stepsize (without T)\nРазберем теперь одну из стратегий того, как избежать знания количества шагов T заранее для подбора длины шага \\alpha_k. Для этого зададим “диаметр” нашего множества D:\n\nD : \\{ \\max\\limits_{x,y \\in S} \\|x - y\\| \\leq D \\}\n\nТеперь зададим длину шага на k- ой итерации, как: \\alpha_k = \\tau \\sqrt{\\dfrac{1}{k+1}}. Константу \\tau \\geq 0 подберем чуть позже.\nДля начала легко заметить, что:\n\n\\begin{align*}\n\\sum\\limits_{k=0}^{T-1} \\alpha_k &= \\tau \\sum\\limits_{k=0}^{T-1} \\dfrac{1}{\\sqrt{k+1}} = \\tau \\left( 1 + \\sum\\limits_{k=1}^{T-1} \\dfrac{1}{\\sqrt{k+1}}\\right) \\leq \\\\\n&\\leq \\tau \\left(1 + \\int\\limits_{0}^{T-1} \\dfrac{1}{\\sqrt{x+1}} dx \\right) = \\tau (2\\sqrt{T}-1)\n\\end{align*}\n\nсм. геометрический смысл неравенства ниже:\n\n\n\nIllustration\n\n\nВозьмем теперь равенство для классического субградиентного метода (БМ) (или неравенство в случае метода проекции субгадиента (УМ)):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k ,x_k - x^*\\rangle\n&=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k ^2 \\\\\n\\sum\\limits_{k=0}^{T-1} \\langle g_k ,x_k - x^*\\rangle\n&= \\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{\\|x_k - x^*\\|^2}{2 \\alpha_k} - \\dfrac{\\|x_{k+1} - x^*\\|^2}{2 \\alpha_k} + \\dfrac{\\alpha_k}{2}g_k^2 \\right) \\\\\n&\\leq \\dfrac{\\|x_0 - x^*\\|^2}{2 \\alpha_0} - \\dfrac{\\|x_T - x^*\\|^2}{2 \\alpha_{T-1}} + \\\\\n&+ \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\|x_k - x^*\\|^2 + \\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2}g_k^2 \\leq \\\\\n& \\leq D^2 \\left( \\dfrac{1}{2 \\alpha_0} + \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\right) + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n& \\leq \\dfrac{D^2}{2 \\alpha_{T-1}} + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n&\\leq \\dfrac{1}{2} \\left( \\dfrac{D^2}{\\tau}\\sqrt{T} + \\tau G^2 \\left(2\\sqrt{T} - 1\\right)\\right) \\leq \\\\\n& \\leq DG \\sqrt{2T}\n\\end{align*}\n\nГде \\tau = \\dfrac{D}{G\\sqrt{2}} - выбран путем минимизации данной оценки по \\tau.\nТаким образом, мы получили, что в случае, когда количество шагов T неизвестно заранее (весьма важное свойство), оценка ухудшается в \\sqrt{2} раз. Такие оценки называют anytime bounds.\n\n\n2.2.2 Online learning:\nPSD - Projected Subgradient Descent\n\n\\begin{align*}\n\\tag{anytime PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{2T} \\\\\n\\tag{PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{T}\n\\end{align*}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#least-squares-with-l_1-regularization",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#least-squares-with-l_1-regularization",
    "title": "1 Intuition",
    "section": "3.1 Least squares with l_1 regularization",
    "text": "3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R^n}} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\n\n3.1.1 Nonnegativity\n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\geq 0 \\}\n\n\n\n3.1.2 l_2 - ball\n\nS = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}\n\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\n\n\n3.1.3 Linear equality constraints\n\nS = \\{x \\in \\mathbb{R}^n \\mid Ax = b \\}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html",
    "href": "docs/methods/fom/SGD.html",
    "title": "1 Summary",
    "section": "",
    "text": "Suppose, our target function is the sum of functions.\n\n\\min\\limits_{\\theta \\in \\mathbb{R}^{p}} g(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\theta)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost (calculation of gradient is linear in n).\nThus, we can switch from the full gradient calculation to its unbiased estimator:\n\n\\theta_{k+1} = \\theta_k - \\alpha_k\\nabla f_{i_k} (\\theta),\n\nwhere we randomly choose i_k index of point at each iteration uniformly:\n\n\\mathbb{E}[\\nabla f_{i_k} (\\theta)] = \\sum_{i=1}^n p(i_k=i) \\nabla f_i(\\theta) = \\dfrac{1}{n}\\sum_{i=1}^n \\nabla f_i(\\theta) = \\nabla g(\\theta)\n\nIterations could be n times cheaper! But convergence requires \\alpha_k \\to 0.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#general-setup",
    "href": "docs/methods/fom/SGD.html#general-setup",
    "title": "1 Summary",
    "section": "2.1 General setup",
    "text": "2.1 General setup\nWe consider classic finite-sample average minimization:\n\n\\min_{x \\in \\mathbb{R}^p} f(x) = \\min_{x \\in \\mathbb{R}^p}\\frac{1}{n} \\sum_{i=1}^n f_i(x)\n\nLet us consider stochastic gradient descent assuming \\nabla f is Lipschitz:\n\n\\tag{SGD}\nx_{k+1} = x_k - \\alpha_k \\nabla f_{i_k}(x_k)\n\nLipschitz continiity implies:\n\nf(x_{k+1}) \\leq f(x_k) + \\langle \\nabla f(x_k), x_{k+1} - x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|^2\n\nusing (\\text{SGD}):\n\nf(x_{k+1}) \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2\n\nNow let’s take expectation with respect to i_k:\n\n\\mathbb{E}[f(x_{k+1})] \\leq \\mathbb{E}[f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2]\n\nUsing linearity of expectation:\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\mathbb{E}[\\nabla f_{i_k}(x_k)]\\rangle + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nSince uniform sampling implies unbiased estimate of gradient: \\mathbb{E}[\\nabla f_{i_k}(x_k)] = \\nabla f(x_k):\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\|\\nabla f(x_k)\\|^2 + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#polyak-lojasiewicz-conditions",
    "href": "docs/methods/fom/SGD.html#polyak-lojasiewicz-conditions",
    "title": "1 Summary",
    "section": "2.2 Polyak-Lojasiewicz conditions",
    "text": "2.2 Polyak-Lojasiewicz conditions\n\n\\tag{PL}\n\\frac{1}{2}\\|\\nabla f(x)\\|_2^2 \\geq \\mu(f(x) - f^*), \\forall x \\in \\mathbb{R}^p\n\nThis inequality simply requires that the gradient grows faster than a quadratic function as we move away from the optimal function value. Note, that strong convexity implies \\text{PL}, but not vice versa. Using \\text{PL} we can write:\n\n\\mathbb{E}[f(x_{k+1})] - f^* \\leq (1 - 2\\alpha_k \\mu) [f(x_k) - f^*] + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nThis bound already indicates, that we have something like linear convergence if far from solution and gradients are similar, but no progress if close to solution or have high variance in gradients at the same time.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#stochastic-subgradient-descent",
    "href": "docs/methods/fom/SGD.html#stochastic-subgradient-descent",
    "title": "1 Summary",
    "section": "2.3 Stochastic subgradient descent",
    "text": "2.3 Stochastic subgradient descent\n\n\\tag{SSD}\nx_{k+1} = x_k - \\alpha_k g_{i_k}\n\nfor some g_{i_k} \\in \\partial f_{i_k}(x_k).\nFor convex f we have\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 \\mathbb{E}[\\|g_{i_k}\\|^2]\n\nHere we can see, that step-size \\alpha_k controls how fast we move towards solution. And squared step-size \\alpha_k^2 controls how much variance moves us away. Usually, we bound \\mathbb{E}[\\|g_{i_k}\\|^2] by some constant B^2.\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 B^2\n\nIf we also have strong convexity:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu) \\|x_{k-1} - x^*\\|^2 + \\alpha_k^2 B^2\n\nAnd finally, with \\alpha_k = \\alpha &lt; \\frac{2}{\\mu}:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu)^{k} R^2 + \\frac{\\alpha B^2}{2\\mu},\n\nwhere R = \\|x_0- x^*\\|",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/index.html",
    "href": "docs/methods/fom/index.html",
    "title": "1 Materials",
    "section": "",
    "text": "Now we have only first order information from the oracle.\n\n\n\n\n\n\n\n\nGradient descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgradient descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjected subgradient descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMirror descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic gradient descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic average gradient\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADAM: A Method for Stochastic Optimization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLookahead Optimizer: k steps forward, 1 step back\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n1 Materials\n\nVisualization of optimization algorithms.",
    "crumbs": [
      "Methods",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html",
    "href": "docs/methods/line_search/binary_search.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nWe divide a segment into two equal parts and choose the one that contains the solution of the problem using the values of functions.\n\n\n2 Algorithm\ndef binary_search(f, a, b, epsilon):\n    c = (a + b) / 2\n    while abs(b - a) &gt; epsilon:\n        y = (a + c) / 2.0\n        if f(y) &lt;= f(c):\n            b = c\n            c = y\n        else:\n            z = (b + c) / 2.0\n            if f(c) &lt;= f(z):\n                a = y\n                b = z\n            else:\n                a = c\n                c = z\n    return c\n\n\n\nIllustration\n\n\n\n\n3 Bounds\nThe length of the line segment on k+1-th iteration:\n\n\\Delta_{k+1} = b_{k+1} - a_{k+1} = \\dfrac{1}{2^k}(b-a)\n\nFor unimodal functions, this holds if we select the middle of a segment as an output of the iteration x_{k+1}:\n\n|x_{k+1} - x_*| \\leq \\dfrac{\\Delta_{k+1}}{2} \\leq \\dfrac{1}{2^{k+1}}(b-a) \\leq (0.5)^{k+1} \\cdot (b-a)\n\nNote, that at each iteration we ask oracle no more, than 2 times, so the number of function evaluations is N = 2 \\cdot k, which implies:\n\n|x_{k+1} - x_*| \\leq (0.5)^{\\frac{N}{2}+1} \\cdot (b-a) \\leq  (0.707)^{N}  \\frac{b-a}{2}\n\nBy marking the right side of the last inequality for \\varepsilon, we get the number of method iterations needed to achieve \\varepsilon accuracy:\n\nK = \\left\\lceil \\log_2 \\dfrac{b-a}{\\varepsilon} - 1 \\right\\rceil",
    "crumbs": [
      "Methods",
      "Line search",
      "Binary search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html",
    "href": "docs/methods/line_search/index.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\nSuppose, we have a problem of minimization of a function f(x): \\mathbb{R} \\to \\mathbb{R} of scalar variable:\n\nf(x) \\to \\min_{x \\in \\mathbb{R}}\n\nSometimes, we refer to the similar problem of finding minimum on the line segment [a,b]:\n\nf(x) \\to \\min_{x \\in [a,b]}\n\nLine search is one of the simplest formal optimization problems, however, it is an important link in solving more complex tasks, so it is very important to solve it effectively. Let’s restrict the class of problems under consideration where f(x) is a unimodal function.\nFunction f(x) is called unimodal on [a, b], if there is x_* \\in [a, b], that f(x_1) &gt; f(x_2) \\;\\;\\; \\forall a \\le x_1 &lt; x_2 &lt; x_* and f(x_1) &lt; f(x_2) \\;\\;\\; \\forall x_* &lt; x_1 &lt; x_2 \\leq b\n\n\n\nIllustration\n\n\n\n\n2 Key property of unimodal functions\nLet f(x) be unimodal function on [a, b]. Than if x_1 &lt; x_2 \\in [a, b], then:\n\nif f(x_1) \\leq f(x_2) \\to x_* \\in [a, x_2]\nif f(x_1) \\geq f(x_2) \\to x_* \\in [x_1, b] \n\n\n\n3 Code\nOpen In Colab\n\n\n\n\n\n\n\n\nBinary search\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGolden search\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInexact line search\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuccessive parabolic interpolation\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n4 References\n\nCMC seminars (ru)",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/parabola.html",
    "href": "docs/methods/line_search/parabola.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nSampling 3 points of a function determines unique parabola. Using this information we will go directly to its minimum. Suppose, we have 3 points x_1 &lt; x_2 &lt; x_3 such that line segment [x_1, x_3] contains minimum of a function f(x). Then, we need to solve the following system of equations:\n\nax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3\n\nNote, that this system is linear, since we need to solve it on a,b,c. Minimum of this parabola will be calculated as:\n\nu = -\\dfrac{b}{2a} = x_2 - \\dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\\right]}\n\nNote, that if f_2 &lt; f_1, f_2 &lt; f_3, than u will lie in [x_1, x_3]\n\n\n2 Algorithm\ndef parabola_search(f, x1, x2, x3, epsilon):\n    f1, f2, f3 = f(x1), f(x2), f(x3)\n    while x3 - x1 &gt; epsilon:\n        u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))\n        fu = f(u)\n\n        if x2 &lt;= u:\n            if f2 &lt;= fu:\n                x1, x2, x3 = x1, x2, u\n                f1, f2, f3 = f1, f2, fu\n            else:\n                x1, x2, x3 = x2, u, x3\n                f1, f2, f3 = f2, fu, f3\n        else:\n            if fu &lt;= f2:\n                x1, x2, x3 = x1, u, x2\n                f1, f2, f3 = f1, fu, f2\n            else:\n                x1, x2, x3 = u, x2, x3\n                f1, f2, f3 = fu, f2, f3\n    return (x1 + x3) / 2\n\n\n3 Bounds\nThe convergence of this method is superlinear, but local, which means, that you can take profit from using this method only near some neighbour of optimum.",
    "crumbs": [
      "Methods",
      "Line search",
      "Successive parabolic interpolation"
    ]
  },
  {
    "objectID": "docs/methods/zom/index.html",
    "href": "docs/methods/zom/index.html",
    "title": "1 Code",
    "section": "",
    "text": "Illustration\n\n\nNow we have only zero order information from the oracle. Typical speed of convergence of these methods is sublinear. A lot of methods are referred both to zero order methods and global optimization.\n\n\n\n\n\n\n\n\nBee algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNelder–Mead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated annealing\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n1 Code\n\nGlobal optimization illustration - Open In Colab{: .btn }\nNevergrad library - Open In Colab{: .btn }\nOptuna quickstart Open In Colab",
    "crumbs": [
      "Methods",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html",
    "href": "docs/methods/zom/simulated-annealing.html",
    "title": "1 Problem",
    "section": "",
    "text": "We need to optimize the global optimum of a given function on some space using only the values of the function in some points on the space.\n\n\\min_{x \\in X} F(x) = F(x^*)\n\nSimulated Annealing is a probabilistic technique for approximating the global optimum of a given function.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#steps-of-the-algorithm",
    "href": "docs/methods/zom/simulated-annealing.html#steps-of-the-algorithm",
    "title": "1 Problem",
    "section": "2.1 Steps of the Algorithm",
    "text": "2.1 Steps of the Algorithm\nStep 1 Let k = 0 - current iteration, T = T_k - initial temperature.\nStep 2 Let x_k \\in X - some random point from our space\nStep 3 Let decrease the temperature by following rule T_{k+1} = \\alpha T_k where 0 &lt; \\alpha &lt; 1 - some constant that often is closer to 1\nStep 4 Let x_{k+1} = g(x_k) - the next point which was obtained from previous one by some random rule. It is usually assumed that this rule works so that each subsequent approximation should not differ very much.\nStep 5 Calculate \\Delta E = E(x_{k+1}) - E(x_{k}), where E(x) - the function that determines the energy of the system at this point. It is supposed that energy has the minimum in desired value x^*.\nStep 6 If \\Delta E &lt; 0 then the approximation found is better than it was. So accept x_{k+1} as new started point at the next step and go to the step Step 3\nStep 7 If \\Delta E &gt;= 0, then we accept x_{k+1} with the probability of P(\\Delta E) = \\exp^{-\\Delta E / T_k}. If we don’t accept x_{k+1}, then we let k = k+ 1. Go to the step Step 3\nThe algorithm can stop working according to various criteria, for example, achieving an optimal state or lowering the temperature below a predetermined level T_{min}.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#convergence",
    "href": "docs/methods/zom/simulated-annealing.html#convergence",
    "title": "1 Problem",
    "section": "2.2 Convergence",
    "text": "2.2 Convergence\nAs it mentioned in Simulated annealing: a proof of convergence the algorithm converges almost surely to a global maximum.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#illustration",
    "href": "docs/methods/zom/simulated-annealing.html#illustration",
    "title": "1 Problem",
    "section": "2.3 Illustration",
    "text": "2.3 Illustration\nA gif from Wikipedia:\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#the-problem",
    "href": "docs/methods/zom/simulated-annealing.html#the-problem",
    "title": "1 Problem",
    "section": "3.1 The Problem",
    "text": "3.1 The Problem\nLet E(x) - the number of intersections, where x - the array of placement queens at the field (the number in array means the column, the index of the number means the row).\nThe problem is to find x^* where E(x^*) = \\min_{x \\in X} E(x) - the global minimum, that is predefined and equals to 0 (no two queens threaten each other).\nIn this code x_0 = [0,1,2,...,N] that means all queens are placed at the board’s diagonal. So at the beginning E = N(N-1), because every queen intersects others.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#results",
    "href": "docs/methods/zom/simulated-annealing.html#results",
    "title": "1 Problem",
    "section": "3.2 Results",
    "text": "3.2 Results\nResults of applying this algorithm with \\alpha = 0.95 to the N queens puzzle for N = 10 averaged by 100 runs are below:\n\n\n\nIllustration\n\n\nResults of running the code for N from 4 to 40 and measuring the time it takes to find the solution averaged by 100 runs are below:\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html",
    "href": "docs/theory/Conjugate_set.html",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "Let S \\subseteq \\mathbb{R}^n be an arbitrary non-empty set. Then its conjugate set is defined as:\n\nS^* = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\}\n\n\n\n\n\n\n\nFigure 1: Convex sets may be described in a dual way - through the elements of the set and through the set of hyperplanes supporting it\n\n\n\nA set S^{**} is called double conjugate to a set S if:\n\nS^{**} = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}\n\n\nThe sets S_1 and S_2 are called inter-conjugate if S_1^* = S_2, S_2^* = S_1.\nA set S is called self-conjugate if S^{*} = S.\n\n\n\n\n\nA conjugate set is always closed, convex, and contains zero.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup \\{0\\}) }\n  \nIf S_1 \\subseteq S_2, then S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*.\nIf S is closed, convex, and includes 0, then S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that S^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\overline{S}\\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*.\nLet p \\in S^* and x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Then by virtue of the continuity of the function f(x) = p^Tx, we have: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. So p \\in \\left(\\overline{S}\\right)^*, hence S^* \\subset \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*.\nLet p \\in S^*, x_0 \\in \\mathbf{conv}(S), i.e., x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nSo p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. So p \\in \\left( \\mathbf{conv}(S) \\right)^*, hence S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that if B(0,r) is a ball of radius r by some norm centered at zero, then \\left( B(0,r) \\right)^* = B(0,1/r).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet B(0,r) = X, B(0,1/r) = Y. Take the normal vector p \\in X^*, then for any x \\in X: p^Tx \\ge -1.\nFrom all points of the ball X, take such a point x \\in X that its scalar product of p: p^Tx is minimal, then this is the point x = -\\frac{p}{\\|p\\|}r.\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\||}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nSo X^* \\subset Y.\nNow let p \\in Y. We need to show that p \\in X^*, i.e., \\langle p, x\\rangle \\geq -1. It’s enough to apply the Cauchy-Bunyakovsky inequality:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nThe latter comes from the fact that p \\in B(0,1/r) and x \\in B(0,r).\nSo Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA conjugate cone to a cone K is a set K^* such that:\n\nK^* = \\left\\{ y \\mid \\langle x, y\\rangle \\ge 0 \\quad \\forall x \\in K\\right\\}\n\nTo show that this definition follows directly from the theorem above, recall what a conjugate set is and what a cone \\forall \\lambda &gt; 0 is.\n\n\\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\} \\to \\to \\{\\lambda y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -\\dfrac{1}{\\lambda} \\;\\; \\forall x\\in S\\}\n\n\n\n\n\n\n\nFigure 2: Illustration of dual cone\n\n\n\n\n\n\n\nLet K be a closed convex cone. Then K^{**} = K.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n and a cone K \\subseteq \\mathbb{R}^n:\n\n  \\left( S + K \\right)^* = S^* \\cap K^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n, then:\n\n  \\left( \\sum\\limits_{i=1}^m K_i \\right)^* = \\bigcap\\limits_{i=1}^m K_i^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n. Let also their intersection have an interior point, then:\n\n  \\left( \\bigcap\\limits_{i=1}^m K_i \\right)^* = \\sum\\limits_{i=1}^m K_i^*\n  \n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the conjugate cone for a monotone nonnegative cone:\n\nK = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nNote that:\n\n\\sum\\limits_{i=1}^nx_iy_i = y_1 (x_1-x_2) + (y_1 + y_2)(x_2 - x_3) + \\ldots + (y_1 + y_2 + \\ldots + y_{n-1})(x_{n-1} - x_n) + (y_1 + \\ldots + y_n)x_n\n\nSince in the presented sum in each summand, the second multiplier in each summand is non-negative, then:\n\ny_1 \\ge 0, \\;\\; y_1 + y_2 \\ge 0, \\;\\;\\ldots, \\;\\;\\;y_1 + \\ldots + y_n \\ge 0\n\nSo K^* = \\left\\{ y \\mid \\sum\\limits_{i=1}^k y_i \\ge 0, k = \\overline{1,n}\\right\\}.\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of solutions to a system of linear inequalities and equalities is a polyhedron:\n\nAx \\preceq b, \\;\\;\\; Cx = d\n\nHere A \\in \\mathbb{R}^{m\\times n}, C \\in \\mathbb{R}^{p \\times n}, and the inequality is a piecewise inequality.\n\n\n\n\n\n\nFigure 3: Polyhedra\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x_1, \\ldots, x_m \\in \\mathbb{R}^n. Conjugate to a polyhedral set:\n\nS = \\mathbf{conv}(x_1, \\ldots, x_k) + \\mathbf{cone}(x_{k+1}, \\ldots, x_m)\n\nis a polyhedron (polyhedron):\n\nS^* = \\left\\{ p \\in \\mathbb{R}^n \\mid \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k} ; \\langle p, x_i\\rangle \\ge 0, i = \\overline{k+1,m} \\right\\}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nLet S = X, S^* = Y. Take some p \\in X^*, then \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k}. At the same time, for any \\theta &gt; 0, i = \\overline{k+1,m}:\n\n  \\langle p, x_i\\rangle \\ge -1 \\to \\langle p, \\theta x_i\\rangle \\ge -1\n  \n\n  \\langle p, x_i\\rangle \\ge -\\frac{1}{\\theta} \\to \\langle p, x_i\\rangle 0.\n  \nSo p \\in Y \\to X^* \\subset Y.\nSuppose, on the other hand, that p \\in Y. For any point x \\in X:\n\n   x = \\sum\\limits_{i=1}^m\\theta_i x_i \\;\\;\\;\\;\\;\\;\\; \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0\n  \nSo:\n\n  \\langle p, x\\rangle = \\sum\\limits_{i=1}^m\\theta_i \\langle p, x_i\\rangle = \\sum\\limits_{i=1}^k\\theta_i \\langle p, x_i\\rangle + \\sum\\limits_{i=k+1}^m\\theta_i \\langle p, x_i\\rangle \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) + \\sum\\limits_{i=1}^k\\theta_i \\cdot 0 = -1.\n  \nSo p \\in X^* \\to Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind and represent the set conjugate to a polyhedral cone in the plane:\n\nS = \\mathbf{cone} \\left\\{ (-3,1), (2,3), (4,5)\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nUsing the theorem above:\n\nS^* = \\left\\{ -3p_1 + p_2 \\ge 0, 2p_1 + 3p_2 \\ge 0, 4p_1+5p_2 \\ge 0 \\right\\}\n\n\n\n\n\n\n\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) \\; Ax = b, x \\ge 0\n\n\n2) \\; p^\\top A \\ge 0, \\langle p,b\\rangle &lt; 0.\n\nAx = b when x \\geq 0 means that b lies in a cone stretched over the columns of the matrix A.\npA \\geq 0, \\; \\langle p, b \\rangle &lt; 0 means that there exists a separating hyperplane between the vector b and the cone of columns of the matrix A.\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) Ax \\leq b\n\n\n2) p^\\top A = 0, \\langle p,b\\rangle &lt; 0, p \\ge 0.\n\nIf in the minimization linear programming problem, the budget set is non-empty and the target function is bounded on it from below, then the problem has a solution.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#definitions",
    "href": "docs/theory/Conjugate_set.html#definitions",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "Let S \\subseteq \\mathbb{R}^n be an arbitrary non-empty set. Then its conjugate set is defined as:\n\nS^* = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\}\n\n\n\n\n\n\n\nFigure 1: Convex sets may be described in a dual way - through the elements of the set and through the set of hyperplanes supporting it\n\n\n\nA set S^{**} is called double conjugate to a set S if:\n\nS^{**} = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}\n\n\nThe sets S_1 and S_2 are called inter-conjugate if S_1^* = S_2, S_2^* = S_1.\nA set S is called self-conjugate if S^{*} = S.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#properties",
    "href": "docs/theory/Conjugate_set.html#properties",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "A conjugate set is always closed, convex, and contains zero.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup \\{0\\}) }\n  \nIf S_1 \\subseteq S_2, then S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*.\nIf S is closed, convex, and includes 0, then S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#examples",
    "href": "docs/theory/Conjugate_set.html#examples",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "Example\n\n\n\n\n\nProve that S^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\overline{S}\\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*.\nLet p \\in S^* and x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Then by virtue of the continuity of the function f(x) = p^Tx, we have: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. So p \\in \\left(\\overline{S}\\right)^*, hence S^* \\subset \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*.\nLet p \\in S^*, x_0 \\in \\mathbf{conv}(S), i.e., x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nSo p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. So p \\in \\left( \\mathbf{conv}(S) \\right)^*, hence S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that if B(0,r) is a ball of radius r by some norm centered at zero, then \\left( B(0,r) \\right)^* = B(0,1/r).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet B(0,r) = X, B(0,1/r) = Y. Take the normal vector p \\in X^*, then for any x \\in X: p^Tx \\ge -1.\nFrom all points of the ball X, take such a point x \\in X that its scalar product of p: p^Tx is minimal, then this is the point x = -\\frac{p}{\\|p\\|}r.\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\||}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nSo X^* \\subset Y.\nNow let p \\in Y. We need to show that p \\in X^*, i.e., \\langle p, x\\rangle \\geq -1. It’s enough to apply the Cauchy-Bunyakovsky inequality:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nThe latter comes from the fact that p \\in B(0,1/r) and x \\in B(0,r).\nSo Y \\subset X^*.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#dual-cones",
    "href": "docs/theory/Conjugate_set.html#dual-cones",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "A conjugate cone to a cone K is a set K^* such that:\n\nK^* = \\left\\{ y \\mid \\langle x, y\\rangle \\ge 0 \\quad \\forall x \\in K\\right\\}\n\nTo show that this definition follows directly from the theorem above, recall what a conjugate set is and what a cone \\forall \\lambda &gt; 0 is.\n\n\\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\} \\to \\to \\{\\lambda y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -\\dfrac{1}{\\lambda} \\;\\; \\forall x\\in S\\}\n\n\n\n\n\n\n\nFigure 2: Illustration of dual cone",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#dual-cones-properties",
    "href": "docs/theory/Conjugate_set.html#dual-cones-properties",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "Let K be a closed convex cone. Then K^{**} = K.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n and a cone K \\subseteq \\mathbb{R}^n:\n\n  \\left( S + K \\right)^* = S^* \\cap K^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n, then:\n\n  \\left( \\sum\\limits_{i=1}^m K_i \\right)^* = \\bigcap\\limits_{i=1}^m K_i^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n. Let also their intersection have an interior point, then:\n\n  \\left( \\bigcap\\limits_{i=1}^m K_i \\right)^* = \\sum\\limits_{i=1}^m K_i^*",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#examples-1",
    "href": "docs/theory/Conjugate_set.html#examples-1",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "Example\n\n\n\n\n\nFind the conjugate cone for a monotone nonnegative cone:\n\nK = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nNote that:\n\n\\sum\\limits_{i=1}^nx_iy_i = y_1 (x_1-x_2) + (y_1 + y_2)(x_2 - x_3) + \\ldots + (y_1 + y_2 + \\ldots + y_{n-1})(x_{n-1} - x_n) + (y_1 + \\ldots + y_n)x_n\n\nSince in the presented sum in each summand, the second multiplier in each summand is non-negative, then:\n\ny_1 \\ge 0, \\;\\; y_1 + y_2 \\ge 0, \\;\\;\\ldots, \\;\\;\\;y_1 + \\ldots + y_n \\ge 0\n\nSo K^* = \\left\\{ y \\mid \\sum\\limits_{i=1}^k y_i \\ge 0, k = \\overline{1,n}\\right\\}.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#polyhedra",
    "href": "docs/theory/Conjugate_set.html#polyhedra",
    "title": "1 Conjugate (Fenchel conjugate, dual, Fenchel dual) set",
    "section": "",
    "text": "The set of solutions to a system of linear inequalities and equalities is a polyhedron:\n\nAx \\preceq b, \\;\\;\\; Cx = d\n\nHere A \\in \\mathbb{R}^{m\\times n}, C \\in \\mathbb{R}^{p \\times n}, and the inequality is a piecewise inequality.\n\n\n\n\n\n\nFigure 3: Polyhedra\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x_1, \\ldots, x_m \\in \\mathbb{R}^n. Conjugate to a polyhedral set:\n\nS = \\mathbf{conv}(x_1, \\ldots, x_k) + \\mathbf{cone}(x_{k+1}, \\ldots, x_m)\n\nis a polyhedron (polyhedron):\n\nS^* = \\left\\{ p \\in \\mathbb{R}^n \\mid \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k} ; \\langle p, x_i\\rangle \\ge 0, i = \\overline{k+1,m} \\right\\}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nLet S = X, S^* = Y. Take some p \\in X^*, then \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k}. At the same time, for any \\theta &gt; 0, i = \\overline{k+1,m}:\n\n  \\langle p, x_i\\rangle \\ge -1 \\to \\langle p, \\theta x_i\\rangle \\ge -1\n  \n\n  \\langle p, x_i\\rangle \\ge -\\frac{1}{\\theta} \\to \\langle p, x_i\\rangle 0.\n  \nSo p \\in Y \\to X^* \\subset Y.\nSuppose, on the other hand, that p \\in Y. For any point x \\in X:\n\n   x = \\sum\\limits_{i=1}^m\\theta_i x_i \\;\\;\\;\\;\\;\\;\\; \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0\n  \nSo:\n\n  \\langle p, x\\rangle = \\sum\\limits_{i=1}^m\\theta_i \\langle p, x_i\\rangle = \\sum\\limits_{i=1}^k\\theta_i \\langle p, x_i\\rangle + \\sum\\limits_{i=k+1}^m\\theta_i \\langle p, x_i\\rangle \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) + \\sum\\limits_{i=1}^k\\theta_i \\cdot 0 = -1.\n  \nSo p \\in X^* \\to Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind and represent the set conjugate to a polyhedral cone in the plane:\n\nS = \\mathbf{cone} \\left\\{ (-3,1), (2,3), (4,5)\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nUsing the theorem above:\n\nS^* = \\left\\{ -3p_1 + p_2 \\ge 0, 2p_1 + 3p_2 \\ge 0, 4p_1+5p_2 \\ge 0 \\right\\}\n\n\n\n\n\n\n\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) \\; Ax = b, x \\ge 0\n\n\n2) \\; p^\\top A \\ge 0, \\langle p,b\\rangle &lt; 0.\n\nAx = b when x \\geq 0 means that b lies in a cone stretched over the columns of the matrix A.\npA \\geq 0, \\; \\langle p, b \\rangle &lt; 0 means that there exists a separating hyperplane between the vector b and the cone of columns of the matrix A.\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) Ax \\leq b\n\n\n2) p^\\top A = 0, \\langle p,b\\rangle &lt; 0, p \\ge 0.\n\nIf in the minimization linear programming problem, the budget set is non-empty and the target function is bounded on it from below, then the problem has a solution.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Convex_optimization_problem.html",
    "href": "docs/theory/Convex_optimization_problem.html",
    "title": "1 Convex optimization problem",
    "section": "",
    "text": "1 Convex optimization problem\n\n\n\nThe idea behind the definition of a convex optimization problem\n\n\nNote, that there is an agreement in notation of mathematical programming. The problems of the following type are called Convex optimization problem:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{COP}\n\nwhere all the functions f_0(x), f_1(x), \\ldots, f_m(x) are convex and all the equality constraints are affine. It sounds a bit strange, but not all convex problems are convex optimization problems.\n\n\\tag{CP}\nf_0(x) \\to \\min\\limits_{x \\in S},\n\nwhere f_0(x) is a convex function, defined on the convex set S. The necessity of affine equality constraint is essential.\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis problem is not a convex optimization problem (but implies minimizing the convex function over the convex set):\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& (x_1 + x_2)^2 = 0,\n\\end{split}\n\\tag{CP}\n\nwhile the following equivalent problem is a convex optimization problem\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& x_1 + x_2 = 0,\n\\end{split}\n\\tag{COP}\n\n\n\n\n\nSuch confusion in notation is sometimes being avoided by naming problems of type \\text{(CP)} as abstract form convex optimization problem.\n\n\n2 Materials\n\nConvex Optimization — Boyd & Vandenberghe @ Stanford",
    "crumbs": [
      "Theory",
      "Convex optimization problem"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html",
    "href": "docs/theory/Duality.html",
    "title": "1 Motivation",
    "section": "",
    "text": "Duality lets us associate to any constrained optimization problem a concave maximization problem, whose solutions lower bound the optimal value of the original problem. What is interesting is that there are cases, when one can solve the primal problem by first solving the dual one. Now, consider a general constrained optimization problem:\n\n\\text{ Primal: }f(x) \\to \\min\\limits_{x \\in S}  \\qquad \\text{ Dual: } g(y) \\to \\max\\limits_{y \\in \\Omega}\n\nWe’ll build g(y), that preserves the uniform bound:\n\ng(y) \\leq f(x) \\qquad \\forall x \\in S, \\forall y \\in \\Omega\n\nAs a consequence:\n\n\\max\\limits_{y \\in \\Omega} g(y) \\leq \\min\\limits_{x \\in S} f(x)  \n\nWe’ll consider one of many possible ways to construct g(y) in case, when we have a general mathematical programming problem with functional constraints:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nAnd the Lagrangian, associated with this problem:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f_0(x) + \\lambda^\\top f(x) + \\nu^\\top h(x)\n\nWe assume \\mathcal{D} = \\bigcap\\limits_{i=0}^m\\textbf{dom } f_i \\cap \\bigcap\\limits_{i=1}^p\\textbf{dom } h_i is nonempty. We define the Lagrange dual function (or just dual function) g: \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} as the minimum value of the Lagrangian over x: for \\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} \\left( f_0(x) +\\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) \\right)\n\nWhen the Lagrangian is unbounded below in x, the dual function takes on the value −\\infty. Since the dual function is the pointwise infimum of a family of affine functions of (\\lambda, \\nu), it is concave, even when the original problem is not convex.\nLet us show, that the dual function yields lower bounds on the optimal value p^* of the original problem for any \\lambda \\succeq 0, \\nu. Suppose some \\hat{x} is a feasible point for the original problem, i.e., f_i(\\hat{x}) \\leq 0 and h_i(\\hat{x}) = 0, \\; λ \\succeq 0. Then we have:\n\nL(\\hat{x}, \\lambda, \\nu) = f_0(\\hat{x}) + \\underbrace{\\lambda^\\top f(\\hat{x})}_{\\leq 0} + \\underbrace{\\nu^\\top h(\\hat{x})}_{= 0} \\leq f_0(\\hat{x})\n\nHence\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) \\leq L(\\hat{x}, \\lambda, \\nu)  \\leq f_0(\\hat{x})\n\n\ng(\\lambda, \\nu) \\leq p^*\n\nA natural question is: what is the best lower bound that can be obtained from the Lagrange dual function? This leads to the following optimization problem:\n\n\\begin{split}\n& g(\\lambda, \\nu) \\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\; \\nu \\in \\mathbb{R}^p }\\\\\n\\text{s.t. } & \\lambda \\succeq 0\n\\end{split}\n\nThe term “dual feasible”, to describe a pair (\\lambda, \\nu) with \\lambda \\succeq 0 and g(\\lambda, \\nu) &gt; −\\infty, now makes sense. It means, as the name implies, that (\\lambda, \\nu) is feasible for the dual problem. We refer to (\\lambda^*, \\nu^*) as dual optimal or optimal Lagrange multipliers if they are optimal for the above problem.\n\n\n\n\n\n\n\n\n\n\n\nPrimal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in S \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\n\\begin{matrix} & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\nProblem\n\\begin{matrix}& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\ \\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\begin{matrix} g(\\lambda, \\nu) &\\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p }\\\\ \\text{s.t. } & \\lambda \\succeq 0 \\end{matrix}\n\n\nOptimal\n\\begin{matrix} &x^* \\text{ if feasible}, \\\\ &p^* = f_0(x^*)\\end{matrix}\n\\begin{matrix} &\\lambda^*, \\nu^* \\text{ if } \\max \\text{ is achieved}, \\\\ &d^* = g(\\lambda^*, \\nu^*)\\end{matrix}\n\n\n\n\n\n\n\n\n\nLeast-squares solution of linear equations\n\n\n\n\n\nWe are addressing a problem within a non-empty budget set, defined as follows: \n\\begin{aligned}\n    & \\text{min} \\quad x^T x \\\\\n    & \\text{s.t.} \\quad Ax = b,\n\\end{aligned}\n with the matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThis problem is devoid of inequality constraints, presenting m linear equality constraints instead. The Lagrangian is expressed as L(x, \\nu) = x^T x + \\nu^T (Ax - b), spanning the domain \\mathbb{R}^n \\times \\mathbb{R}^m. The dual function is denoted by g(\\nu) = \\inf_x L(x, \\nu). Given that L(x, \\nu) manifests as a convex quadratic function in terms of x, the minimizing x can be derived from the optimality condition \n\\nabla_x L(x, \\nu) = 2x + A^T \\nu = 0,\n leading to x = -(1/2)A^T \\nu. As a result, the dual function is articulated as \n    g(\\nu) = L(-(1/2)A^T \\nu, \\nu) = -(1/4)\\nu^T A A^T \\nu - b^T \\nu,\n emerging as a concave quadratic function within the domain \\mathbb{R}^p. According to the lower bound property (5.2), for any \\nu \\in \\mathbb{R}^p, the following holds true: \n    -(1/4)\\nu^T A A^T \\nu - b^T \\nu \\leq \\inf\\{x^T x \\,|\\, Ax = b\\}.\n Which is a simple non-trivial lower bound without any problem solving.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-way partitioning problem\n\n\n\n\n\n\nWe are examining a (nonconvex) problem: \n\\begin{aligned}\n    & \\text{minimize} \\quad x^T W x \\\\\n    & \\text{subject to} \\quad x_i^2 =1, \\quad i=1,\\ldots,n,\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe matrix W belongs to S_n. The constraints stipulate that the values of x_i can only be 1 or -1, rendering this problem analogous to finding a vector, with components \\pm1, that minimizes x^T W x. The set of feasible solutions is finite, encompassing 2^n points, thereby allowing, in theory, for the resolution of this problem by evaluating the objective value at each feasible point. However, as the count of feasible points escalates exponentially, this approach is viable only for modest-sized problems (for instance, when n \\leq 30). Generally, and especially when n exceeds 50, the problem poses a formidable challenge to solve.\nThis problem can be construed as a two-way partitioning problem over a set of n elements, denoted as \\{1, \\ldots , n\\}: A viable x corresponds to the partition \n\\{1,\\ldots,n\\} = \\{i|x_i =-1\\} \\cup \\{i|x_i =1\\}.\n The coefficient W_{ij} in the matrix represents the expense associated with placing elements i and j in the same partition, while -W_{ij} signifies the cost of segregating them. The objective encapsulates the aggregate cost across all pairs of elements, and the challenge posed by problem is to find the partition that minimizes the total cost.\nWe now derive the dual function for this problem. The Lagrangian is expressed as \nL(x,\\nu) = x^T W x + \\sum_{i=1}^n \\nu_i (x_i^2 -1) = x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu.\n By minimizing over x, we procure the Lagrange dual function: \ng(\\nu) = \\inf_x x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu\n= \\begin{cases}\\begin{array}{ll}\n    -\\mathbf{1}^T\\nu & \\text{if } W+\\text{diag}(\\nu) \\succeq 0 \\\\\n    -\\infty & \\text{otherwise},\n\\end{array} \\end{cases}\n\nexploiting the realization that the infimum of a quadratic form is either zero (when the form is positive semidefinite) or -\\infty (when it’s not).\nThis dual function furnishes lower bounds on the optimal value of the problem. For instance, we can adopt the particular value of the dual variable\n\n\\nu = -\\lambda_{\\text{min}}(W) \\mathbf{1}\n\nwhich is dual feasible, since\n\nW +\\text{diag}(\\nu)=W -\\lambda_{\\text{min}}(W) I \\succeq 0.\n\nThis renders a simple bound on the optimal value p^*\n\np^* \\geq -\\mathbf{1}^T\\nu = n \\lambda_{\\text{min}}(W).\n\nThe code for the problem is available here 🧑‍💻",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#summary",
    "href": "docs/theory/Duality.html#summary",
    "title": "1 Motivation",
    "section": "",
    "text": "Primal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in S \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\n\\begin{matrix} & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\nProblem\n\\begin{matrix}& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\ \\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\begin{matrix} g(\\lambda, \\nu) &\\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p }\\\\ \\text{s.t. } & \\lambda \\succeq 0 \\end{matrix}\n\n\nOptimal\n\\begin{matrix} &x^* \\text{ if feasible}, \\\\ &p^* = f_0(x^*)\\end{matrix}\n\\begin{matrix} &\\lambda^*, \\nu^* \\text{ if } \\max \\text{ is achieved}, \\\\ &d^* = g(\\lambda^*, \\nu^*)\\end{matrix}\n\n\n\n\n\n\n\n\n\nLeast-squares solution of linear equations\n\n\n\n\n\nWe are addressing a problem within a non-empty budget set, defined as follows: \n\\begin{aligned}\n    & \\text{min} \\quad x^T x \\\\\n    & \\text{s.t.} \\quad Ax = b,\n\\end{aligned}\n with the matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThis problem is devoid of inequality constraints, presenting m linear equality constraints instead. The Lagrangian is expressed as L(x, \\nu) = x^T x + \\nu^T (Ax - b), spanning the domain \\mathbb{R}^n \\times \\mathbb{R}^m. The dual function is denoted by g(\\nu) = \\inf_x L(x, \\nu). Given that L(x, \\nu) manifests as a convex quadratic function in terms of x, the minimizing x can be derived from the optimality condition \n\\nabla_x L(x, \\nu) = 2x + A^T \\nu = 0,\n leading to x = -(1/2)A^T \\nu. As a result, the dual function is articulated as \n    g(\\nu) = L(-(1/2)A^T \\nu, \\nu) = -(1/4)\\nu^T A A^T \\nu - b^T \\nu,\n emerging as a concave quadratic function within the domain \\mathbb{R}^p. According to the lower bound property (5.2), for any \\nu \\in \\mathbb{R}^p, the following holds true: \n    -(1/4)\\nu^T A A^T \\nu - b^T \\nu \\leq \\inf\\{x^T x \\,|\\, Ax = b\\}.\n Which is a simple non-trivial lower bound without any problem solving.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-way partitioning problem\n\n\n\n\n\n\nWe are examining a (nonconvex) problem: \n\\begin{aligned}\n    & \\text{minimize} \\quad x^T W x \\\\\n    & \\text{subject to} \\quad x_i^2 =1, \\quad i=1,\\ldots,n,\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe matrix W belongs to S_n. The constraints stipulate that the values of x_i can only be 1 or -1, rendering this problem analogous to finding a vector, with components \\pm1, that minimizes x^T W x. The set of feasible solutions is finite, encompassing 2^n points, thereby allowing, in theory, for the resolution of this problem by evaluating the objective value at each feasible point. However, as the count of feasible points escalates exponentially, this approach is viable only for modest-sized problems (for instance, when n \\leq 30). Generally, and especially when n exceeds 50, the problem poses a formidable challenge to solve.\nThis problem can be construed as a two-way partitioning problem over a set of n elements, denoted as \\{1, \\ldots , n\\}: A viable x corresponds to the partition \n\\{1,\\ldots,n\\} = \\{i|x_i =-1\\} \\cup \\{i|x_i =1\\}.\n The coefficient W_{ij} in the matrix represents the expense associated with placing elements i and j in the same partition, while -W_{ij} signifies the cost of segregating them. The objective encapsulates the aggregate cost across all pairs of elements, and the challenge posed by problem is to find the partition that minimizes the total cost.\nWe now derive the dual function for this problem. The Lagrangian is expressed as \nL(x,\\nu) = x^T W x + \\sum_{i=1}^n \\nu_i (x_i^2 -1) = x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu.\n By minimizing over x, we procure the Lagrange dual function: \ng(\\nu) = \\inf_x x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu\n= \\begin{cases}\\begin{array}{ll}\n    -\\mathbf{1}^T\\nu & \\text{if } W+\\text{diag}(\\nu) \\succeq 0 \\\\\n    -\\infty & \\text{otherwise},\n\\end{array} \\end{cases}\n\nexploiting the realization that the infimum of a quadratic form is either zero (when the form is positive semidefinite) or -\\infty (when it’s not).\nThis dual function furnishes lower bounds on the optimal value of the problem. For instance, we can adopt the particular value of the dual variable\n\n\\nu = -\\lambda_{\\text{min}}(W) \\mathbf{1}\n\nwhich is dual feasible, since\n\nW +\\text{diag}(\\nu)=W -\\lambda_{\\text{min}}(W) I \\succeq 0.\n\nThis renders a simple bound on the optimal value p^*\n\np^* \\geq -\\mathbf{1}^T\\nu = n \\lambda_{\\text{min}}(W).\n\nThe code for the problem is available here 🧑‍💻",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#reminder-of-kkt-statements",
    "href": "docs/theory/Duality.html#reminder-of-kkt-statements",
    "title": "1 Motivation",
    "section": "4.1 Reminder of KKT statements:",
    "text": "4.1 Reminder of KKT statements:\nSuppose we have a general optimization problem (from the chapter)\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{1}\nand convex optimization problem (see corresponding chapter), where all equality constraints are affine: h_i(x) = a_i^Tx - b_i, i \\in 1, \\ldots p\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{2}\nThe Lagrangian is\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)\n\nThe KKT system is:\n\n\\begin{split}\n& \\nabla_x L(x^*, \\lambda^*, \\nu^*) = 0 \\\\\n& \\nabla_\\nu L(x^*, \\lambda^*, \\nu^*) = 0 \\\\\n& \\lambda^*_i \\geq 0, i = 1,\\ldots,m \\\\\n& \\lambda^*_i f_i(x^*) = 0, i = 1,\\ldots,m \\\\\n& f_i(x^*) \\leq 0, i = 1,\\ldots,m \\\\\n\\end{split}\n\\tag{3}\n\n\n\n\n\n\nKKT becomes necessary\n\n\n\n\n\nIf x^* is a solution of the original problem Equation 1, then if any of the following regularity conditions is satisfied:\n\nStrong duality If f_1, \\ldots f_m, h_1, \\ldots h_p are differentiable functions and we have a problem Equation 1 with zero duality gap, then Equation 3 are necessary (i.e. any optimal set x^*, \\lambda^*, \\nu^* should satisfy Equation 3)\nLCQ (Linearity constraint qualification). If f_1, \\ldots f_m, h_1, \\ldots h_p are affine functions, then no other condition is needed.\nLICQ (Linear independence constraint qualification). The gradients of the active inequality constraints and the gradients of the equality constraints are linearly independent at x^*\nSC (Slater’s condition) For a convex optimization problem Equation 2 (i.e., assuming minimization, f_i are convex and h_j is affine), there exists a point x such that h_j(x)=0 and g_i(x) &lt; 0.\n\nThan it should satisfy Equation 3\n\n\n\n\n\n\n\n\n\n\nKKT in convex case\n\n\n\n\n\nIf a convex optimization problem Equation 2 with differentiable objective and constraint functions satisfies Slater’s condition, then the KKT conditions provide necessary and sufficient conditions for optimality: Slater’s condition implies that the optimal duality gap is zero and the dual optimum is attained, so x^* is optimal if and only if there are (\\lambda^*,\\nu^*) that, together with x^*, satisfy the KKT conditions.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#connection-between-fenchel-duality-and-lagrange-duality",
    "href": "docs/theory/Duality.html#connection-between-fenchel-duality-and-lagrange-duality",
    "title": "1 Motivation",
    "section": "5.1 Connection between Fenchel duality and Lagrange duality",
    "text": "5.1 Connection between Fenchel duality and Lagrange duality\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\begin{split}\n& f_0(x) = \\sum_{i=1}^n f_i(x_i)\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b\n\\end{split}\n\nThe dual problem is thus\n\n\\begin{split}\n& -b \\nu - \\sum_{i=1}^n f_i^*(-\\nu a_i)  \\to \\max\\limits_{\\nu \\in \\mathbb{R}}\\\\\n\\text{s.t. } & \\nu \\geq - \\lambda_{\\text{min}}(A)\n\\end{split}\n\nwith (scalar) variable \\nu \\in \\mathbb{R}. Now suppose we have found an optimal dual variable \\nu^* (There are several simple methods for solving a convex problem with one scalar variable, such as the bisection method.). It is very easy to recover the optimal value for the primal problem.\n\n\n\n\nLet f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. We call Fenchel - Rockafellar problem the following minimization task:\n\nf(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\nwhere A^{-1}(G) := \\{x \\in V : Ax \\in G\\} — preimage of G. We’ll build the dual problem using variable separation. Let’s introduce new variable y = Ax. The problem could be rewritten:\n\n\\begin{split}\n& f(x) + g(y) \\to \\min\\limits_{x \\in E, \\; y \\in G }\\\\\n\\text{s.t. } & Ax = y\n\\end{split}\n\nLagrangian\n\nL(x,y, \\lambda) =  f(x) + g(y) + \\lambda^\\top (Ax - y)\n\nDual function\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, \\; y \\in G} L(x,y, \\lambda) \\\\\n&= \\min\\limits_{x \\in E}\\left[ f(x) + (A^*\\lambda)^\\top x \\right] + \\min\\limits_{y \\in G} \\left[g(y) - \\lambda^\\top y\\right] = \\\\\n&= -\\max\\limits_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] - \\max\\limits_{y \\in G} \\left[\\lambda^\\top y - g(y)\\right]\n\\end{split}\n\nNow, we need to remember the definition of the conjugate function:\n\n\\sup_{y \\in G}\\left[\\lambda^\\top y - g(y)\\right] = \\begin{cases} g^*(\\lambda), &\\text{ if } \\lambda \\in G_*\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\n\n\\sup_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] = \\begin{cases} f^*(-A^*\\lambda), &\\text{ if } \\lambda \\in (-A^*)^{-1}(E_*)\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\nSo, we have:\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, y \\in G} L(x,y, \\lambda) = \\\\\n&= \\begin{cases} -g^*(\\lambda) - f^*(-A^*\\lambda) &\\text{ if } \\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)\\\\ -\\infty, &\\text{ otherwise}  \\end{cases}\n\\end{split}\n\nwhich allows us to formulate one of the most important theorems, that connects dual problems and conjugate functions:\n\n\n\n\n\n\nFenchel - Rockafellar theorem\n\n\n\n\n\nLet f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. Let p^*, d^* \\in [- \\infty, + \\infty] - optimal values of primal and dual problems:\n\np^* = f(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\n\nd^* = f^*(-A^*\\lambda) + g^*(\\lambda) \\to \\min\\limits_{\\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)},\n\nThen we have weak duality: p^* \\geq d^*. Furthermore, if the functions f and g are convex and A(\\mathbf{relint}(E)) \\cap \\mathbf{relint}(G) \\neq \\varnothing, then we have strong duality: p^* = d^*. While points x^* \\in E \\cap A^{-1}(G) and \\lambda^* \\in G_* \\cap (-A^*)^{-1}(E_*) are optimal values for primal and dual problem if and only if:\n\n\\begin{split}\n-A^*\\lambda^* &\\in \\partial f(x^*) \\\\\n\\lambda^* &\\in \\partial g(Ax^*)\n\\end{split}\n\n\n\n\n\nConvex case is especially important since if we have Fenchel - Rockafellar problem with parameters (f, g, A), than the dual problem has the form (f^*, g^*, -A^*).",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#sensitivity-analysis",
    "href": "docs/theory/Duality.html#sensitivity-analysis",
    "title": "1 Motivation",
    "section": "5.2 Sensitivity analysis",
    "text": "5.2 Sensitivity analysis\nLet us switch from the original optimization problem\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{P}\n\nTo the perturbed version of it:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq u_i, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = v_i, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{Per}\n\nNote, that we still have the only variable x \\in \\mathbb{R}^n, while treating u \\in \\mathbb{R}^m, v \\in \\mathbb{R}^p as parameters. It is obvious, that \\text{Per}(u,v) \\to \\text{P} if u = 0, v = 0. We will denote the optimal value of \\text{Per} as p^*(u, v), while the optimal value of the original problem \\text{P} is just p^*. One can immediately say, that p^*(u, v) = p^*.\nSpeaking of the value of some i-th constraint we can say, that\n\nu_i = 0 leaves the original problem\nu_i &gt; 0 means that we have relaxed the inequality\nu_i &lt; 0 means that we have tightened the constraint\n\nOne can even show, that when \\text{P} is convex optimization problem, p^*(u,v) is a convex function.\nSuppose, that strong duality holds for the orriginal problem and suppose, that x is any feasible point for the perturbed problem:\n\n\\begin{split}\np^*(0,0) &= p^* = d^* = g(\\lambda^*, \\nu^*) \\leq \\\\\n& \\leq L(x, \\lambda^*, \\nu^*) = \\\\\n& = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i^* f_i(x) + \\sum\\limits_{i=1}^p\\nu_i^* h_i(x) \\leq \\\\\n& \\leq f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i^* u_i + \\sum\\limits_{i=1}^p\\nu_i^* v_i\n\\end{split}\n\nWhich means\n\n\\begin{split}\nf_0(x) \\geq p^*(0,0) - {\\lambda^*}^T u - {\\nu^*}^T v\n\\end{split}\n\nAnd taking the optimal x for the perturbed problem, we have:\n\np^*(u,v) \\geq p^*(0,0) - {\\lambda^*}^T u - {\\nu^*}^T v\n\\tag{4}\nIn scenarios where strong duality holds, we can draw several insights about the sensitivity of optimal solutions in relation to the Lagrange multipliers. These insights are derived from the inequality expressed in equation above:\n\nImpact of Tightening a Constraint (Large \\lambda_i^\\star):\nWhen the ith constraint’s Lagrange multiplier, \\lambda_i^\\star, holds a substantial value, and if this constraint is tightened (choosing u_i &lt; 0), there is a guarantee that the optimal value, denoted by p^\\star(u, v), will significantly increase.\nEffect of Adjusting Constraints with Large Positive or Negative \\nu_i^\\star:\n\nIf \\nu_i^\\star is large and positive and v_i &lt; 0 is chosen, or\n\nIf \\nu_i^\\star is large and negative and v_i &gt; 0 is selected,\nthen in either scenario, the optimal value p^\\star(u, v) is expected to increase greatly.\n\nConsequences of Loosening a Constraint (Small \\lambda_i^\\star):\nIf the Lagrange multiplier \\lambda_i^\\star for the ith constraint is relatively small, and the constraint is loosened (choosing u_i &gt; 0), it is anticipated that the optimal value p^\\star(u, v) will not significantly decrease.\nOutcomes of Tiny Adjustments in Constraints with Small \\nu_i^\\star:\n\nWhen \\nu_i^\\star is small and positive, and v_i &gt; 0 is chosen, or\n\nWhen \\nu_i^\\star is small and negative, and v_i &lt; 0 is opted for,\nin both cases, the optimal value p^\\star(u, v) will not significantly decrease.\n\n\nThese interpretations provide a framework for understanding how changes in constraints, reflected through their corresponding Lagrange multipliers, impact the optimal solution in problems where strong duality holds.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#local-sensitivity",
    "href": "docs/theory/Duality.html#local-sensitivity",
    "title": "1 Motivation",
    "section": "5.3 Local sensitivity",
    "text": "5.3 Local sensitivity\nSuppose now that p^*(u, v) is differentiable at u = 0, v = 0.\n\n\\lambda_i^* = -\\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\quad \\nu_i^* = -\\dfrac{\\partial p^*(0,0)}{\\partial v_i}\n\\tag{5}\nTo show this result we consider the directional derivative of p^*(u,v) along the direction of some i-th basis vector e_i:\n\n\\lim_{t \\to 0} \\dfrac{p^*(t e_i,0) - p^*(0,0)}{t} = \\dfrac{\\partial p^*(0,0)}{\\partial u_i}\n\nFrom the inequality Equation 4 and taking the limit t \\to0 with t&gt;0 we have\n\n\\dfrac{p^*(t e_i,0) - p^*}{t} \\geq -\\lambda_i^* \\to  \\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\geq -\\lambda_i^*\n\nFor the negative t &lt; 0 we have:\n\n\\dfrac{p^*(t e_i,0) - p^*}{t} \\leq -\\lambda_i^* \\to  \\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\leq -\\lambda_i^*\n\nThe same idea can be used to establish the fact about v_i.\nThe local sensitivity result Equation 5 provides a way to understand the impact of constraints on the optimal solution x^* of an optimization problem. If a constraint f_i(x^*) is negative at x^*, it’s not affecting the optimal solution, meaning small changes to this constraint won’t alter the optimal value. In this case, the corresponding optimal Lagrange multiplier will be zero, as per the principle of complementary slackness.\nHowever, if f_i(x^*) = 0, meaning the constraint is precisely met at the optimum, then the situation is different. The value of the i-th optimal Lagrange multiplier, \\lambda^*_i, gives us insight into how ‘sensitive’ or ‘active’ this constraint is. A small \\lambda^*_i indicates that slight adjustments to the constraint won’t significantly affect the optimal value. Conversely, a large \\lambda^*_i implies that even minor changes to the constraint can have a significant impact on the optimal solution.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#shadow-prices-or-tax-interpretation",
    "href": "docs/theory/Duality.html#shadow-prices-or-tax-interpretation",
    "title": "1 Motivation",
    "section": "5.4 Shadow prices or tax interpretation",
    "text": "5.4 Shadow prices or tax interpretation\nConsider an enterprise where x represents its operational strategy and f_0(x) is the operating cost. Therefore, -f_0(x) denotes the profit in dollars. Each constraint f_i(x) \\leq 0 signifies a resource or regulatory limit. The goal is to maximize profit while adhering to these limits, which is equivalent to solving:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\n\\end{split}\n\nThe optimal profit here is -p^*.\nNow, imagine a scenario where exceeding limits is allowed, but at a cost. This cost is linear to the extent of violation, quantified by f_i. The charge for breaching the i^{th} constraint is \\lambda_i f_i(x). If f_i(x) &lt; 0, meaning the constraint is not fully utilized, \\lambda_i f_i(x) represents income for the firm. Here, \\lambda_i is the cost (in dollars) per unit of violation for f_i(x).\nFor instance, if f_1(x) \\leq 0 limits warehouse space, the firm can rent out extra space at \\lambda_1 dollars per square meter or rent out unused space for the same rate.\nThe firm’s total cost, considering operational and constraint costs, is L(x, \\lambda) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x). The firm aims to minimize L(x, \\lambda), resulting in an optimal cost g(\\lambda). The dual function g(\\lambda) represents the best possible cost for the firm based on the prices of constraints \\lambda, and the optimal dual value d^* is this cost under the most unfavorable price conditions.\nWeak duality implies that the cost in this flexible scenario (where the firm can trade constraint violations) is always less than or equal to the cost in the strict original scenario. This is because any optimal operation x^* from the original scenario will cost less in the flexible scenario, as the firm can earn from underused constraints.\nIf strong duality holds and the dual optimum is reached, the optimal \\lambda^* represents prices where the firm gains no extra advantage from trading constraint violations. These optimal \\lambda^* values are often termed ‘shadow prices’ for the original problem, indicating the hypothetical cost of constraint flexibility.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#mixed-strategies-for-matrix-games",
    "href": "docs/theory/Duality.html#mixed-strategies-for-matrix-games",
    "title": "1 Motivation",
    "section": "5.5 Mixed strategies for matrix games",
    "text": "5.5 Mixed strategies for matrix games\nIn zero-sum matrix games, players 1 and 2 choose actions from sets \\{1,...,n\\} and \\{1,...,m\\}, respectively. The outcome is a payment from player 1 to player 2, determined by a payoff matrix P \\in \\mathbb{R}^{n \\times m}. Each player aims to use mixed strategies, choosing actions according to a probability distribution: player 1 uses probabilities u_i for each action i, and player 2 uses v_i.\nThe expected payoff from player 1 to player 2 is given by $ {k=1}^{n} {l=1}^{m} u_k v_l P_{kl} = u^T P v $. Player 1 seeks to minimize this expected payoff, while player 2 aims to maximize it.\n\n5.5.1 Player 1’s Perspective\nAssuming player 2 knows player 1’s strategy u, player 2 will choose v to maximize u^T P v. The worst-case expected payoff is thus:\n\n\\max_{v \\geq 0, 1^T v = 1} u^T P v = \\max_{i=1,...,m} (P^T u)_i\n\nPlayer 1’s optimal strategy minimizes this worst-case payoff, leading to the optimization problem:\n\n\\minimize \\max_{i=1,...,m} (P^T u)_i \\quad \\subjectto \\quad u \\geq 0, 1^T u = 1\n\nThis forms a convex optimization problem with the optimal value denoted as p^*_1.\n\n\n5.5.2 Player 2’s Perspective\nConversely, if player 1 knows player 2’s strategy v, the goal is to minimize u^T P v. This leads to:\n\n\\min_{u \\geq 0, 1^T u = 1} u^T P v = \\min_{i=1,...,n} (P v)_i\n\nPlayer 2 then maximizes this to get the largest guaranteed payoff, solving the optimization problem:\n\n\\maximize \\min_{i=1,...,n} (P v)_i \\quad \\subjectto \\quad v \\geq 0, 1^T v = 1\n\nThe optimal value here is p^*_2.\n\n\n5.5.3 Duality and Equivalence\nIt’s generally advantageous to know the opponent’s strategy, but surprisingly, in mixed strategy matrix games, this advantage disappears. The key lies in duality: the problems above are Lagrange duals. By formulating player 1’s problem as a linear program and introducing Lagrange multipliers, we find that the dual problem matches player 2’s problem. Due to strong duality in feasible linear programs, p^*_1 = p^*_2, showing no advantage in knowing the opponent’s strategy.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html",
    "href": "docs/theory/Optimality.html",
    "title": "1 Background",
    "section": "",
    "text": "f(x) \\to \\min\\limits_{x \\in S}\n\n\nA set S is usually called a budget set.\n\nWe say that the problem has a solution if the budget set is not empty: x^* \\in S, in which the minimum or the infimum of the given function is achieved.\n\nA point x^* is a global minimizer if f(x^*) \\leq f(x) for all x.\nA point x^* is a local minimizer if there exists a neighborhood N of x^* such that f(x^*) \\leq f(x) for all x \\in N.\nA point x^* is a strict local minimizer (also called a strong local minimizer) if there exists a neighborhood N of x^* such that f(x^*) &lt; f(x) for all x \\in N with x \\neq x^*.\nWe call x^* a stationary point (or critical) if \\nabla f(x^*) = 0. Any local minimizer must be a stationary point.\n\n\n\n\n\n\n\nFigure 1: Illustration of different stationary (critical) points\n\n\n\n\n\n\n\n\n\nExtreme value (Weierstrass) theorem\n\n\n\n\n\nLet S \\subset \\mathbb{R}^n be a compact set and f(x) a continuous function on S. So that, the point of the global minimum of the function f (x) on S exists.\n\n\n\n\n\n\n\n\n\n\nFigure 2: A lot of practical problems are theoretically solvable\n\n\n\n\n\n\n\n\n\nTaylor’s Theorem\n\n\n\n\n\nSuppose that f : \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and that p \\in \\mathbb{R}^n. Then we have: \nf(x + p) = f(x) + \\nabla f(x + tp)^T p \\quad \\text{ for some } t \\in (0, 1)\n\nMoreover, if f is twice continuously differentiable, we have:\n\n\\nabla f(x + p) = \\nabla f(x) + \\int_0^1 \\nabla^2 f(x + tp)p \\, dt\n\n\nf(x + p) = f(x) + \\nabla f(x)^T p + \\frac{1}{2} p^T \\nabla^2 f(x + tp) p  \\quad \\text{ for some } t \\in (0, 1)\n\n\n\n\n\n\n\nConsider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#lagrange-multipliers",
    "href": "docs/theory/Optimality.html#lagrange-multipliers",
    "title": "1 Background",
    "section": "",
    "text": "Consider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#convex-case",
    "href": "docs/theory/Optimality.html#convex-case",
    "title": "1 Background",
    "section": "2.1 Convex case",
    "text": "2.1 Convex case\nIt should be mentioned, that in the convex case (i.e., f(x) is convex) necessary condition becomes sufficient.\nOne more important result for convex unconstrained case sounds as follows. If f(x): S \\to \\mathbb{R} - convex function defined on the convex set S, then:\n\nAny local minima is the global one.\nThe set of the local minimizers S^* is convex.\nIf f(x) - strictly or strongly (different cases 😀) convex function, then S^* contains only one single point S^* = \\{x^*\\}.",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#optimization-with-equality-conditions",
    "href": "docs/theory/Optimality.html#optimization-with-equality-conditions",
    "title": "1 Background",
    "section": "3.1 Optimization with equality conditions",
    "text": "3.1 Optimization with equality conditions\n\n3.1.1 Intuition\nThings are pretty simple and intuitive in unconstrained problem. In this section we will add one equality constraint, i.e.\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h(x) = 0\n\\end{split}\n\nWe will try to illustrate approach to solve this problem through the simple example with f(x) = x_1 + x_2 and h(x) = x_1^2 + x_2^2 - 2.\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\nGenerally: in order to move from x_F along the budget set towards decreasing the function, we need to guarantee two conditions:\n\n\\langle \\delta x, \\nabla h(x_F) \\rangle = 0\n\n\n\\langle \\delta x, - \\nabla f(x_F) \\rangle &gt; 0\n\nLet’s assume, that in the process of such a movement we have come to the point where \n-\\nabla f(x) = \\nu \\nabla h(x)\n\n\n\\langle  \\delta x, - \\nabla f(x)\\rangle = \\langle  \\delta x, \\nu\\nabla h(x)\\rangle = 0  \n\nThen we came to the point of the budget set, moving from which it will not be possible to reduce our function. This is the local minimum in the constrained problem :)\n\n\n\nIllustration of KKT\n\n\nSo let’s define a Lagrange function (just for our convenience):\n\nL(x, \\nu) = f(x) + \\nu h(x)\n\nThen if the problem is regular (we will define it later) and the point x^* is the local minimum of the problem described above, then there exist \\nu^*:\n\n\\begin{split}\n& \\text{Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\text{ that's written above}\\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\text{ budget constraint}\\\\\n% & \\text{Sufficient conditions} \\\\\n% & \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle &gt; 0,\\\\\n% & \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h(x^*)^\\top y = 0\n\\end{split}\n\nWe should notice that L(x^*, \\nu^*) = f(x^*).\n\n\n3.1.2 General formulation\n\n\\tag{ECP}\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nSolution\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f(x) + \\nu^\\top h(x)\n\nLet f(x) and h_i(x) be twice differentiable at the point x^* and continuously differentiable in some neighborhood x^*. The local minimum conditions for x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p are written as\n\n\\begin{split}\n& \\text{ECP: Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\\\\n% & \\text{ECP: Sufficient conditions} \\\\\n% & \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle \\ge 0,\\\\\n% & \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0\n\\end{split}\n\nDepending on the behavior of the Hessian, the critical points can have a different character.\n\n\n\n\n    \n    \n    How eigenvalues of the hessian affects the critical point\n    \n\n\n\n\n    \n        \n        l1: 0\n        l2: 0\n        \n    \n\n    \n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nPose the optimization problem and solve them for linear system Ax = b, A \\in \\mathbb{m \\times n} for three cases (assuming the matrix is full rank):\n\nm &lt; n\nm = n\nm &gt; n",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#optimization-with-inequality-conditions",
    "href": "docs/theory/Optimality.html#optimization-with-inequality-conditions",
    "title": "1 Background",
    "section": "3.2 Optimization with inequality conditions",
    "text": "3.2 Optimization with inequality conditions\n\n3.2.1 Example\n\nf(x) = x_1^2 + x_2^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\nThus, if the constraints of the type of inequalities are inactive in the constrained problem, then don’t worry and write out the solution to the unconstrained problem. However, this is not the whole story 🤔. Consider the second childish example\n\nf(x) = (x_1 - 1)^2 + (x_2 + 1)^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\nSo, we have a problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\nTwo possible cases:\n\n\n\n\n\n\n\ng(x) \\leq 0 is inactive. g(x^*) &lt; 0\ng(x) \\leq 0 is active. g(x^*) = 0\n\n\n\n\ng(x^*) &lt; 0  \\nabla f(x^*) = 0 \\nabla^2 f(x^*) &gt; 0\nNecessary conditions  g(x^*) = 0  - \\nabla f(x^*) = \\lambda \\nabla g(x^*), \\lambda &gt; 0  Sufficient conditions  \\langle y, \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle &gt; 0,  \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla g(x^*)^\\top y = 0\n\n\n\nCombining two possible cases, we can write down the general conditions for the problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0 \\\\\n&\n\\end{split}\n\nLet’s define the Lagrange function:\n\nL (x, \\lambda) = f(x) + \\lambda g(x)\n\nThe classical Karush-Kuhn-Tucker first and second order optimality conditions for a local minimizer x^*, stated under the linear independence constraint qualification (LICQ) (or other regularity conditions), can be written as follows:\nIf x^* is a local minimum of the problem described above, then there exists a unique Lagrange multiplier \\lambda^* such that:\n\n\\begin{split}\n    & (1) \\; \\nabla_x L (x^*, \\lambda^*) = 0 \\\\\n    & (2) \\; \\lambda^* \\geq 0 \\\\\n    & (3) \\; \\lambda^* g(x^*) = 0 \\\\\n    & (4) \\; g(x^*) \\leq 0\\\\\n    & (5) \\; \\forall y \\in C(x^*):  \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle &gt; 0 \\\\\n    &  \\text{where } C(x^*) = \\{y \\ \\in \\mathbb{R}^n |  \\nabla f(x^*) ^\\top y \\leq 0 \\text{ and } \\forall i \\in I(x^*):  \\nabla g_i(x^*)^⊤ y \\leq 0\n    \\} \\text{ is the critical cone.} \\\\\n    & I(x^*) = \\{i| g_i(x^*) = 0\\} \\\\\n\\end{split}\n\nIt’s noticeable, that L(x^*, \\lambda^*) = f(x^*). Conditions \\lambda^* = 0 , (1), (4) are the first scenario realization, and conditions \\lambda^* &gt; 0 , (1), (3) - the second one.\n\n\n3.2.2 General formulation\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nThis formulation is a general problem of mathematical programming.\nThe solution involves constructing a Lagrange function:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#necessary-conditions",
    "href": "docs/theory/Optimality.html#necessary-conditions",
    "title": "1 Background",
    "section": "4.1 Necessary conditions",
    "text": "4.1 Necessary conditions\nLet x^*, (\\lambda^*, \\nu^*) be a solution to a mathematical programming problem with zero duality gap (the optimal value for the primal problem p^* is equal to the optimal value for the dual problem d^*). Let also the functions f_0, f_i, h_i be differentiable.\n\n\\nabla_x L(x^*, \\lambda^*, \\nu^*) = 0\n\\nabla_\\nu L(x^*, \\lambda^*, \\nu^*) = 0\n\\lambda^*_i \\geq 0, i = 1,\\ldots,m\n\\lambda^*_i f_i(x^*) = 0, i = 1,\\ldots,m\nf_i(x^*) \\leq 0, i = 1,\\ldots,m",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#some-regularity-conditions",
    "href": "docs/theory/Optimality.html#some-regularity-conditions",
    "title": "1 Background",
    "section": "4.2 Some regularity conditions",
    "text": "4.2 Some regularity conditions\nThese conditions are needed in order to make KKT solutions the necessary conditions. Some of them even turn necessary conditions into sufficient (for example, Slater’s). Moreover, if you have regularity, you can write down necessary second order conditions \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle \\geq 0 with semi-definite hessian of Lagrangian.\n\nSlater’s condition. If for a convex problem (i.e., assuming minimization, f_0,f_{i} are convex and h_{i} are affine), there exists a point x such that h(x)=0 and f_{i}(x)&lt;0 (existance of a strictly feasible point), then we have a zero duality gap and KKT conditions become necessary and sufficient.\nLinearity constraint qualification If f_{i} and h_{i} are affine functions, then no other condition is needed.\nFor other examples, see wiki.",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#sufficient-conditions",
    "href": "docs/theory/Optimality.html#sufficient-conditions",
    "title": "1 Background",
    "section": "4.3 Sufficient conditions",
    "text": "4.3 Sufficient conditions\nFor smooth, non-linear optimization problems, a second order sufficient condition is given as follows. The solution x^{*},\\lambda ^{*},\\nu ^{*}, which satisfies the KKT conditions (above) is a constrained local minimum if for the Lagrangian,\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)\n\nthe following conditions hold:\n\n\\begin{split}\n& \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle &gt; 0 \\\\\n& \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0, \\nabla f_0(x^*) ^\\top y \\leq 0,\\nabla f_j(x^*)^\\top y \\leq 0 \\\\\n& i = 1,\\ldots, p \\quad \\forall j: f_j(x^*) = 0\n\\end{split}",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html",
    "href": "docs/theory/Subgradient.html",
    "title": "1 Definition",
    "section": "",
    "text": "An important property of a continuous convex function f(x) is that at any chosen point x_0 for all x \\in \\text{dom } f the inequality holds:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\nfor some vector g, i.e., the tangent to the graph of the function is the global estimate from below for the function.\n\n\n\n\n\n\nFigure 1: Taylor linear approximation serves as a global lower bound for a convex function\n\n\n\n\nIf f(x) is differentiable, then g = \\nabla f(x_0)\nNot all continuous convex functions are differentiable 🐱\n\nWe wouldn’t want to lose such a nice property.\n\n\nA vector g is called the subgradient of a function f(x): S \\to \\mathbb{R} at a point x_0 if \\forall x \\in S:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\partial f(x), if f(x) = |x|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe problem can be solved either geometrically (at each point of the numerical line indicate the angular coefficients of the lines globally supporting the function from the bottom), or by the Moreau-Rockafellar theorem, considering f(x) as a point-wise maximum of convex functions:\n\nf(x) = \\max\\{-x, x\\}\n\n\n\n\n\n\n\nFigure 2: Subgradient of \\vert x \\vert\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of all subgradients of a function f(x) at a point x_0 is called the subdifferential of f at x_0 and is denoted by \\partial f(x_0).\n\n\n\n\n\n\nFigure 3: Subgradient calculus\n\n\n\n\nIf x_0 \\in \\mathbf{ri } S, then \\partial f(x_0) is a convex compact set.\nThe convex function f(x) is differentiable at the point x_0\\Rightarrow \\partial f(x_0) = \\{\\nabla f(x_0)\\}$\nIf \\partial f(x_0) \\neq \\emptyset \\quad \\forall x_0 \\in S, then f(x) is convex on S.",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#subgradient",
    "href": "docs/theory/Subgradient.html#subgradient",
    "title": "1 Definition",
    "section": "",
    "text": "A vector g is called the subgradient of a function f(x): S \\to \\mathbb{R} at a point x_0 if \\forall x \\in S:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\partial f(x), if f(x) = |x|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe problem can be solved either geometrically (at each point of the numerical line indicate the angular coefficients of the lines globally supporting the function from the bottom), or by the Moreau-Rockafellar theorem, considering f(x) as a point-wise maximum of convex functions:\n\nf(x) = \\max\\{-x, x\\}\n\n\n\n\n\n\n\nFigure 2: Subgradient of \\vert x \\vert",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#subdifferential",
    "href": "docs/theory/Subgradient.html#subdifferential",
    "title": "1 Definition",
    "section": "",
    "text": "The set of all subgradients of a function f(x) at a point x_0 is called the subdifferential of f at x_0 and is denoted by \\partial f(x_0).\n\n\n\n\n\n\nFigure 3: Subgradient calculus\n\n\n\n\nIf x_0 \\in \\mathbf{ri } S, then \\partial f(x_0) is a convex compact set.\nThe convex function f(x) is differentiable at the point x_0\\Rightarrow \\partial f(x_0) = \\{\\nabla f(x_0)\\}$\nIf \\partial f(x_0) \\neq \\emptyset \\quad \\forall x_0 \\in S, then f(x) is convex on S.",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html",
    "href": "docs/theory/convex sets/Conic_sets.html",
    "title": "1 Cone",
    "section": "",
    "text": "A non-empty set S is called a cone, if:\n\n\\forall x \\in S, \\; \\theta \\ge 0 \\;\\; \\rightarrow \\;\\; \\theta x \\in S\n\n\n\n\n\n\n\nFigure 1: Illustration of a cone",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#conic-combination",
    "href": "docs/theory/convex sets/Conic_sets.html#conic-combination",
    "title": "1 Cone",
    "section": "3.1 Conic combination",
    "text": "3.1 Conic combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called conic combination of x_1, x_2, \\ldots, x_k if \\theta_i \\ge 0.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#conic-hull",
    "href": "docs/theory/convex sets/Conic_sets.html#conic-hull",
    "title": "1 Cone",
    "section": "3.2 Conic hull",
    "text": "3.2 Conic hull\nThe set of all conic combinations of points in set S is called the conic hull of S:\n\n\\mathbf{cone}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\; \\theta_i \\ge 0\\right\\}\n\n\n\n\n\n\n\nFigure 3: Illustration of a convex hull",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html",
    "href": "docs/theory/convex sets/Projection.html",
    "title": "1 Definitions",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}\n\n\n\n\nProjection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Projection"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html#distance-between-point-and-set",
    "href": "docs/theory/convex sets/Projection.html#distance-between-point-and-set",
    "title": "1 Definitions",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Projection"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html#projection-of-a-point-on-set",
    "href": "docs/theory/convex sets/Projection.html#projection-of-a-point-on-set",
    "title": "1 Definitions",
    "section": "",
    "text": "Projection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Projection"
    ]
  },
  {
    "objectID": "docs/theory/index.html",
    "href": "docs/theory/index.html",
    "title": "",
    "section": "",
    "text": "This chapter provides the information about foundation terms and notations for optimization.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\n\nConvex sets\n\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\n\nConvex function\n\n\n\n\n\n\n\n\n\n\n\nConjugate set\n\n\n\n\n\n\n\n\n\n\n\nConjugate function\n\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\n\nDual norm\n\n\n\n\n\n\n\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\n\n\n\n\n\n\n\nOptimality conditions. KKT\n\n\n\n\n\n\n\n\n\n\n\nConvex optimization problem\n\n\n\n\n\n\n\n\n\n\n\nDuality\n\n\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Theory"
    ]
  }
]