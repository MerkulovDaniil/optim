[
  {
    "objectID": "docs/visualizations/sgd_scalar.html",
    "href": "docs/visualizations/sgd_scalar.html",
    "title": "SGD intuition in a scalar case",
    "section": "",
    "text": "Gradient descent with an appropriate constant learning rate converges for a minimum for a convex function:\nYour browser does not support the video tag.\nBut what if the minimized function is not convex?\nYour browser does not support the video tag.\nIn contrast, Stochastic Gradient Descent (SGD) could escape local minimuma:\nYour browser does not support the video tag.\nRecent studies suggest, that we should care not only about the depth of the local minimum, but of its width as well:\n  \nOne more interesting case, where the classical convergence of Gradient Descent may not be useful:\nYour browser does not support the video tag.\nWhile what initially looks like a clear divergence leads to a better minimum from the generalization perspective:\nYour browser does not support the video tag.\nCode",
    "crumbs": [
      "Visualizations",
      "SGD intuition in a scalar case"
    ]
  },
  {
    "objectID": "docs/visualizations/sg_algorithms.html",
    "href": "docs/visualizations/sg_algorithms.html",
    "title": "Adaptive gradient algorithms visualization",
    "section": "",
    "text": "Your browser does not support the video tag.\nYour browser does not support the video tag.\nWatching points rolling off the surface of the minimized function is my guilty pleasure. Put Lipschitz if you like such animations too.\nCode",
    "crumbs": [
      "Visualizations",
      "Adaptive gradient algorithms visualization"
    ]
  },
  {
    "objectID": "docs/visualizations/ls_gd.html",
    "href": "docs/visualizations/ls_gd.html",
    "title": "Step Size Selection in Gradient Descent via Line Search",
    "section": "",
    "text": "If a function is strongly convex with known parameters, the optimal fixed step size is \\frac{2}{\\mu + L}, where \\mu and L are the smallest and largest Hessian eigenvalues.\nIn practice, these are rarely known, and functions may be non-convex. Instead, we tune the learning rate manually, use line search, or adaptive methods like AdamW & NAG-GS.\n\nGolden-section search works well for convex functions but lacks guarantees for non-unimodal ones.\nWolfe conditions ensure sufficient function decrease while avoiding too tiny steps.\n\nLine search often reduces function values faster with fewer iterations, despite higher per-step costs.\n\n\n\nLine search\n\n\nYour browser does not support the video tag.\nCode",
    "crumbs": [
      "Visualizations",
      "Step Size Selection in Gradient Descent via Line Search"
    ]
  },
  {
    "objectID": "docs/visualizations/double_descent.html",
    "href": "docs/visualizations/double_descent.html",
    "title": "We have Double Descent at home!",
    "section": "",
    "text": "Your browser does not support the video tag.\nMom, I want double descent!\n“We have double descent at home.”\nThe phenomenon of double descent is a curious effect observed in machine learning models. As model complexity increases, prediction quality on test data first improves (the first descent), then, as expected, error rises due to overfitting. But unexpectedly, beyond a certain point, the model’s generalization ability improves again (the second descent). Interestingly, a similar pattern can be observed in a relatively simple example of polynomial regression. In the animation above, the left side shows 50 sinusoidal points used as a training set for polynomial regression.\nA typical outcome when increasing the polynomial degree is overfitting — almost zero error on the training data (the blue points on the left) but high error on the test set (100 evenly spaced points along the black line). However, as the model’s complexity continues to increase, we observe that, out of the infinite number of possible solutions, smoother ones are somehow selected. In this way, we clearly witness double descent.\nOf course, this doesn’t happen by accident. In this particular animation, I used Chebyshev polynomials, and from the infinite set of suitable polynomial coefficients, I chose those with the smallest norm. In machine learning, it’s often not possible to explicitly enforce this, so techniques like weight decay are used to favor solutions with smaller norms.\nCode",
    "crumbs": [
      "Visualizations",
      "We have Double Descent at home!"
    ]
  },
  {
    "objectID": "docs/theory/index.html",
    "href": "docs/theory/index.html",
    "title": "Theory",
    "section": "",
    "text": "This chapter provides the information about foundation terms and notations for optimization.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\nConvex sets\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\nConvex function\n\n\n\n\n\n\n\n\n\n\nConjugate set\n\n\n\n\n\n\n\n\n\n\nConjugate function\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\nDual norm\n\n\n\n\n\n\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\n\n\n\n\n\n\nOptimality conditions. KKT\n\n\n\n\n\n\n\n\n\n\nConvex optimization problem\n\n\n\n\n\n\n\n\n\n\nDuality\n\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Theory"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html",
    "href": "docs/theory/convex sets/Projection.html",
    "title": "Projection",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}\n\n\n\n\nProjection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the halfspace\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n},  b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Projection"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html#definitions",
    "href": "docs/theory/convex sets/Projection.html#definitions",
    "title": "Projection",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}\n\n\n\n\nProjection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the halfspace\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n},  b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Projection"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html",
    "href": "docs/theory/convex sets/Conic_sets.html",
    "title": "Conic set",
    "section": "",
    "text": "A non-empty set S is called a cone, if:\n\n\\forall x \\in S, \\; \\theta \\ge 0 \\;\\; \\rightarrow \\;\\; \\theta x \\in S\n\n\n\n\n\n\n\nFigure 1: Illustration of a cone",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#cone",
    "href": "docs/theory/convex sets/Conic_sets.html#cone",
    "title": "Conic set",
    "section": "",
    "text": "A non-empty set S is called a cone, if:\n\n\\forall x \\in S, \\; \\theta \\ge 0 \\;\\; \\rightarrow \\;\\; \\theta x \\in S\n\n\n\n\n\n\n\nFigure 1: Illustration of a cone",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#convex-cone",
    "href": "docs/theory/convex sets/Conic_sets.html#convex-cone",
    "title": "Conic set",
    "section": "2 Convex cone",
    "text": "2 Convex cone\nThe set S is called a convex cone, if:\n\n\\forall x_1, x_2 \\in S, \\; \\theta_1, \\theta_2 \\ge 0 \\;\\; \\rightarrow \\;\\; \\theta_1 x_1 + \\theta_2 x_2 \\in S\n\n\n\n\n\n\n\nFigure 2: Illustration of a convex cone\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\mathbb{R}^n\nAffine sets, containing 0\nRay\n\\mathbf{S}^n_+ - the set of symmetric positive semi-definite matrices",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#related-definitions",
    "href": "docs/theory/convex sets/Conic_sets.html#related-definitions",
    "title": "Conic set",
    "section": "3 Related definitions",
    "text": "3 Related definitions\n\n3.1 Conic combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called conic combination of x_1, x_2, \\ldots, x_k if \\theta_i \\ge 0.\n\n\n3.2 Conic hull\nThe set of all conic combinations of points in set S is called the conic hull of S:\n\n\\mathbf{cone}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\; \\theta_i \\ge 0\\right\\}\n\n\n\n\n\n\n\nFigure 3: Illustration of a convex hull",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Conic set"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html",
    "href": "docs/theory/Subgradient.html",
    "title": "Subgradient and subdifferential",
    "section": "",
    "text": "An important property of a continuous convex function f(x) is that at any chosen point x_0 for all x \\in \\text{dom } f the inequality holds:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\nfor some vector g, i.e., the tangent to the function’s graph is the global estimate from below for the function.\n\n\n\n\n\n\nFigure 1: Taylor linear approximation serves as a global lower bound for a convex function\n\n\n\n\nIf f(x) is differentiable, then g = \\nabla f(x_0)\nNot all continuous convex functions are differentiable 🐱\n\nWe wouldn’t want to lose such a nice property.\n\n\nA vector g is called the subgradient of a function f(x): S \\to \\mathbb{R} at a point x_0 if \\forall x \\in S:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\partial f(x), if f(x) = |x|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe problem can be solved either geometrically (at each point of the numerical line indicate the angular coefficients of the lines globally supporting the function from the bottom), or by the Moreau-Rockafellar theorem, considering f(x) as a point-wise maximum of convex functions:\n\nf(x) = \\max\\{-x, x\\}\n\n\n\n\n\n\n\nFigure 2: Subgradient of \\vert x \\vert\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of all subgradients of a function f(x) at a point x_0 is called the subdifferential of f at x_0 and is denoted by \\partial f(x_0).\n\n\n\n\n\n\nFigure 3: Subgradient calculus\n\n\n\n\nIf x_0 \\in \\mathbf{ri } S, then \\partial f(x_0) is a convex compact set.\nThe convex function f(x) is differentiable at the point x_0\\Rightarrow \\partial f(x_0) = \\{\\nabla f(x_0)\\}.\nIf \\partial f(x_0) \\neq \\emptyset \\quad \\forall x_0 \\in S, then f(x) is convex on S.",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#definition",
    "href": "docs/theory/Subgradient.html#definition",
    "title": "Subgradient and subdifferential",
    "section": "",
    "text": "An important property of a continuous convex function f(x) is that at any chosen point x_0 for all x \\in \\text{dom } f the inequality holds:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\nfor some vector g, i.e., the tangent to the function’s graph is the global estimate from below for the function.\n\n\n\n\n\n\nFigure 1: Taylor linear approximation serves as a global lower bound for a convex function\n\n\n\n\nIf f(x) is differentiable, then g = \\nabla f(x_0)\nNot all continuous convex functions are differentiable 🐱\n\nWe wouldn’t want to lose such a nice property.\n\n\nA vector g is called the subgradient of a function f(x): S \\to \\mathbb{R} at a point x_0 if \\forall x \\in S:\n\nf(x) \\geq f(x_0) + \\langle g, x - x_0 \\rangle\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\partial f(x), if f(x) = |x|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe problem can be solved either geometrically (at each point of the numerical line indicate the angular coefficients of the lines globally supporting the function from the bottom), or by the Moreau-Rockafellar theorem, considering f(x) as a point-wise maximum of convex functions:\n\nf(x) = \\max\\{-x, x\\}\n\n\n\n\n\n\n\nFigure 2: Subgradient of \\vert x \\vert\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of all subgradients of a function f(x) at a point x_0 is called the subdifferential of f at x_0 and is denoted by \\partial f(x_0).\n\n\n\n\n\n\nFigure 3: Subgradient calculus\n\n\n\n\nIf x_0 \\in \\mathbf{ri } S, then \\partial f(x_0) is a convex compact set.\nThe convex function f(x) is differentiable at the point x_0\\Rightarrow \\partial f(x_0) = \\{\\nabla f(x_0)\\}.\nIf \\partial f(x_0) \\neq \\emptyset \\quad \\forall x_0 \\in S, then f(x) is convex on S.",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#subdifferentiability-and-convexity",
    "href": "docs/theory/Subgradient.html#subdifferentiability-and-convexity",
    "title": "Subgradient and subdifferential",
    "section": "2 Subdifferentiability and convexity",
    "text": "2 Subdifferentiability and convexity\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if the function has a subdifferential at some point, the function is convex?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\partial f(x), if f(x) = \\sin x, x \\in [\\pi/2; 2\\pi] \n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\partial_S f(x) =\n\\begin{cases}\n(-\\infty ; \\cos x_0], &x = \\frac\\pi2 \\\\\n\\emptyset, &x \\in \\left(\\frac\\pi2; x_0\\right) \\\\\n\\cos x, &x \\in [x_0; 2\\pi) \\\\\n[1; \\infty), &x = 2\\pi\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nSubdifferential of a differentiable function Let f : S \\to \\mathbb{R} be a function defined on the set S in a Euclidean space \\mathbb{R}^n. If x_0 \\in \\mathbf{ri }(S) and f is differentiable at x_0, then either \\partial f(x_0) = \\emptyset or \\partial f(x_0) = \\{\\nabla f(x_0)\\}. Moreover, if the function f is convex, the first scenario is impossible.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nAssume, that s \\in \\partial f(x_0) for some s \\in \\mathbb{R}^n distinct from \\nabla f(x_0). Let v \\in  \\mathbb{R}^n be a unit vector. Because x_0 is an interior point of S, there exists \\delta &gt; 0 such that x_0 + tv \\in S for all 0 &lt; t &lt; \\delta. By the definition of the subgradient, we have \nf(x_0 + tv) \\geq f(x_0) + t \\langle s, v \\rangle\n which implies: \n\\frac{f(x_0 + tv) - f(x_0)}{t} \\geq \\langle s, v \\rangle\n for all 0 &lt; t &lt; \\delta. Taking the limit as t approaches 0 and using the definition of the gradient, we get: \n\\langle \\nabla f(x_0), v \\rangle = \\lim_{{t \\to 0; 0 &lt; t &lt; \\delta}} \\frac{f(x_0 + tv) - f(x_0)}{t} \\geq \\langle s, v \\rangle\n\nFrom this, \\langle s - \\nabla f(x_0), v \\rangle \\geq 0. Due to the arbitrariness of v, one can set \nv = -\\frac{s - \\nabla f(x_0)}{\\| s - \\nabla f(x_0) \\|},\n leading to s = \\nabla f(x_0).\nFurthermore, if the function f is convex, then according to the differential condition of convexity f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle for all x \\in S. But by definition, this means \\nabla f(x_0) \\in \\partial f(x_0).\n\n\n\n\n\n\n\n\n\n\nIt is interesting to mention, that the statement for the convex function could be strengthened. Let f : S \\to \\mathbb{R} be a convex function defined on the set S in a finite-dimensional Euclidean space \\mathbb{R}^n, and let x_0 \\in \\mathbf{ri }(S). Then, f is differentiable at x_0 if and only if the subdifferential \\partial f(x_0) contains exactly one element. In this case, \\partial f(x_0) = \\{\\nabla f(x_0)\\}.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nLet f : S \\to \\mathbb{R} be a function defined on the set S in a Euclidean space, and let x_0 \\in S. Show that the point x_0 is a minimum of the function f if and only if 0 \\in \\partial f(x_0).\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if the function is convex, it has a subgradient at any point?\n\n\n\n\nConvexity follows from subdifferentiability at any point. A natural question to ask is whether the converse is true: is every convex function subdifferentiable? It turns out that, generally speaking, the answer to this question is negative.\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet f : [0,\\infty) \\to \\mathbb{R} be the function defined by f(x) := -\\sqrt{x}. Then, \\partial f(0) = \\emptyset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nAssume, that s \\in \\partial f(0) for some s \\in \\mathbb{R}. Then, by definition, we must have sx \\leq -\\sqrt{x} for all x \\geq 0. From this, we can deduce s \\leq -\\sqrt{1} for all x &gt; 0. Taking the limit as x approaches 0 from the right, we get s \\leq -\\infty, which is impossible.",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#subdifferential-calculus",
    "href": "docs/theory/Subgradient.html#subdifferential-calculus",
    "title": "Subgradient and subdifferential",
    "section": "3 Subdifferential calculus",
    "text": "3 Subdifferential calculus\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nMoreau - Rockafellar theorem (subdifferential of a linear combination). Пусть f_i(x) - выпуклые функции на выпуклых множествах S_i, \\; i = \\overline{1,n}.\nТогда, если \\bigcap\\limits_{i=1}^n \\mathbf{ri } S_i \\neq \\emptyset то функция f(x) = \\sum\\limits_{i=1}^n a_i f_i(x), \\; a_i &gt; 0 имеет субдифференциал \\partial_S f(x) на множестве S = \\bigcap\\limits_{i=1}^n S_i и\n\n\\partial_S f(x) = \\sum\\limits_{i=1}^n a_i \\partial_{S_i} f_i(x)\n\n\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nDubovitsky - Milutin theorem (subdifferential of a point-wise maximum). Пусть f_i(x) - выпуклые функции на открытом выпуклом множестве S  \\subseteq \\mathbb{R}^n, \\; x_0 \\in S, а поточечный максимум определяется как f(x)  = \\underset{i}{\\operatorname{max}} f_i(x). Тогда:\n\n\\partial_S f(x_0) = \\mathbf{conv}\\left\\{  \\bigcup\\limits_{i \\in I(x_0)} \\partial_S f_i(x_0) \\right\\},\n\nгде I(x) = \\{ i \\in [1:m]: f_i(x) = f(x)\\}\n\n\n\n\nChain rule for subdifferentials Пусть g_1, \\ldots, g_m - выпуклые функции на открытом выпуклом множестве S \\subseteq \\mathbb{R}^n, g = (g_1, \\ldots, g_m) - образованная из них вектор - функция, \\varphi - монотонно неубывающая выпуклая функция на открытом выпуклом множестве U \\subseteq \\mathbb{R}^m, причем g(S) \\subseteq U. Тогда субдифференциал функции f(x) = \\varphi \\left( g(x)\\right) имеет вид:\n\n\\partial f(x) = \\bigcup\\limits_{p \\in \\partial \\varphi(u)} \\left( \\sum\\limits_{i=1}^{m}p_i \\partial g_i(x) \\right),\n\nгде u = g(x)\nВ частности, если функция \\varphi дифференцируема в точке u = g(x), то формула запишется так:\n\n\\partial f(x) = \\sum\\limits_{i=1}^{m}\\dfrac{\\partial \\varphi}{\\partial u_i}(u) \\partial g_i(x)\n\n\n\\partial (\\alpha f)(x) = \\alpha \\partial f(x), for \\alpha \\geq 0\n\\partial (\\sum f_i)(x) = \\sum \\partial f_i (x), f_i - выпуклые функции\n\\partial (f(Ax + b))(x) = A^T\\partial f(Ax + b), f - выпуклая функция\nz \\in \\partial f(x) if and only if x \\in \\partial f^*(z).",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#examples",
    "href": "docs/theory/Subgradient.html#examples",
    "title": "Subgradient and subdifferential",
    "section": "4 Examples",
    "text": "4 Examples\nКонцептуально, различают три способа решения задач на поиск субградиента:\n\nТеоремы Моро - Рокафеллара, композиции, максимума\nГеометрически\nПо определению\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nНайти \\partial f(x), если f(x) = |x - 1| + |x + 1|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nСовершенно аналогично применяем теорему Моро - Рокафеллара, учитывая следующее:\n\n\\partial f_1(x) = \\begin{cases} -1,  &x &lt; 1\\\\ [-1;1], \\quad &x = 1 \\\\ 1,  &x &gt; 1 \\end{cases} \\qquad \\partial f_2(x) = \\begin{cases} -1,  &x &lt; -1\\\\ [-1;1], &x = -1 \\\\ 1,  &x &gt; -1  \\end{cases}\n\nТаким образом:\n\n\\partial f(x) = \\begin{cases} -2, &x &lt; -1\\\\ [-2;0], &x = -1 \\\\ 0,  &-1 &lt; x &lt; 1 \\\\ [0;2], &x = 1 \\\\ 2, &x &gt; 1 \\\\ \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nНайти \\partial f(x), если f(x) = |c_1^\\top x| + |c_2^\\top x|\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nПусть f_1(x) = |c_1^\\top x|, а f_2(x) = |c_2^\\top x|. Так как эти функции выпуклы, субдифференциал их суммы равен сумме субдифференциалов. Найдем каждый из них:\n\\partial f_1(x) = \\partial \\left( \\max \\{c_1^\\top x, -c_1^\\top x\\} \\right) = \\begin{cases} -c_1,  &c_1^\\top x &lt; 0\\\\ \\mathbf{conv}(-c_1;c_1), &c_1^\\top x = 0 \\\\ c_1,  &c_1^\\top x &gt; 0 \\end{cases} \\partial f_2(x) = \\partial \\left( \\max \\{c_2^\\top x, -c_2^\\top x\\} \\right) = \\begin{cases} -c_2,  &c_2^\\top x &lt; 0\\\\ \\mathbf{conv}(-c_2;c_2), &c_2^\\top x = 0 \\\\ c_2,  &c_2^\\top x &gt; 0 \\end{cases}\nДалее интересными представляются лишь различные взаимные расположения векторов c_1 и c_2, рассмотрение которых предлагается читателю.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nНайти \\partial f(x), если f(x) = \\left[ \\max(0, f_0(x))\\right]^q. Здесь f_0(x) - выпуклая функция на открытом выпуклом множестве S, q \\geq 1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nСогласно теореме о композиции (функция \\varphi (x) = x^q - дифференцируема), а g(x) = \\max(0, f_0(x)) имеем: \\partial f(x) = q(g(x))^{q-1} \\partial g(x)\nПо теореме о поточечном максимуме:\n\n\\partial g(x) = \\begin{cases} \\partial f_0(x), \\quad f_0(x) &gt; 0,\\\\ \\{0\\}, \\quad f_0(x) &lt; 0 \\\\ \\{a \\mid a = \\lambda a', \\; 0 \\le \\lambda \\le 1, \\; a' \\in \\partial f_0(x)\\}, \\;\\; f_0(x) = 0 \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nНайти \\partial f(x), если f(x) = \\| x\\|_1\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nПо определению\n\n\\|x\\|_1 = |x_1| + |x_2| + \\ldots + |x_n| = s_1 x_1 + s_2 x_2 + \\ldots + s_n x_n\n\nРассмотрим эту сумму как поточечный максимум линейных функций по x: g(x) = s^\\top x, где s_i = \\{ -1, 1\\}. Каждая такая функция однозначно определяется набором коэффициентов \\{s_i\\}_{i=1}^n.\nТогда по теореме Дубовицкого - Милютина, в каждой точке \\partial f = \\mathbf{conv}\\left(\\bigcup\\limits_{i \\in I(x)} \\partial g_i(x)\\right)\nЗаметим, что \\partial g(x) = \\partial \\left( \\max \\{s^\\top x, -s^\\top x\\} \\right) = \\begin{cases} -s,  &s^\\top x &lt; 0\\\\ \\mathbf{conv}(-s;s), &s^\\top x = 0 \\\\ s,  &s^\\top x &gt; 0 \\end{cases}.\nПричем, правило выбора “активной” функции поточечного максимума в каждой точке следующее: * Если j-ая координата точки отрицательна, s_i^j = -1 * Если j-ая координата точки положительна, s_i^j = 1 * Если j-ая координата точки равна нулю, то подходят оба варианта коэффициентов и соответствующих им функций, а значит, необходимо включать субградиенты этих функций в объединение в теореме Дубовицкого - Милютина.\nВ итоге получаем ответ:\n\n\\partial f(x) = \\left\\{ g \\; : \\; \\|g\\|_\\infty \\leq 1, \\quad g^\\top x = \\|x\\|_1 \\right\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSubdifferential of the Norm. Let V be a finite-dimensional Euclidean space, and x_0 \\in V. Let \\lVert \\cdot \\rVert be an arbitrary norm in V (not necessarily induced by the scalar product), and let \\lVert \\cdot \\rVert_* be the corresponding conjugate norm. Then,\n\n\\partial \\lVert \\cdot \\rVert (x_0) =\n\\begin{cases}\nB_{\\lVert \\cdot \\rVert_*}(0, 1), & \\text{if } x_0 = 0, \\\\\n\\{s \\in V : \\lVert s \\rVert_* \\leq 1; \\langle s, x_0 \\rangle = \\lVert x_0 \\rVert \\} = \\{s \\in V : \\lVert s \\rVert_* = 1; \\langle s, x_0 \\rangle = \\lVert x_0 \\rVert \\}, & \\text{otherwise.}\n\\end{cases}\n\nWhere B_{\\lVert \\cdot \\rVert_*}(0,1) is the closed unit ball centered at zero with respect to the conjugate norm. In other words, a vector s \\in V with \\lVert s \\rVert_* = 1 is a subgradient of the norm \\lVert \\cdot \\rVert at point x_0 \\neq 0 if and only if the inequality from the definition of the dual norm \\langle s, x_0 \\rangle \\leq \\lVert x_0 \\rVert becomes equality.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nLet s \\in V. By definition, s \\in \\partial \\lVert \\cdot \\rVert (x_0) if and only if\n\n\\langle s, x \\rangle - \\lVert x \\rVert \\leq \\langle s, x_0 \\rangle - \\lVert x_0 \\rVert, \\text{ for all } x \\in V,\n\nor equivalently,\n\n\\sup_{x \\in V} \\{\\langle s, x \\rangle - \\lVert x \\rVert\\} \\leq \\langle s, x_0 \\rangle - \\lVert x_0 \\rVert.\n\nBy the definition of the supremum, the latter is equivalent to\n\n\\sup_{x \\in V} \\{\\langle s, x \\rangle - \\lVert x \\rVert\\} = \\langle s, x_0 \\rangle - \\lVert x_0 \\rVert.\n\nIt is important to note that the expression on the left side is the supremum from the definition of the Fenchel conjugate function for the norm, which is known to be\n\n\\sup_{x \\in V} \\{\\langle s, x \\rangle - \\lVert x \\rVert\\} =\n\\begin{cases}\n0, & \\text{if } \\lVert s \\rVert_* \\leq 1, \\\\\n+\\infty, & \\text{otherwise.}\n\\end{cases}\n\nThus, equation is equivalent to \\lVert s \\rVert_* \\leq 1 and \\langle s, x_0 \\rangle = \\lVert x_0 \\rVert.\nConsequently, it remains to note that for x_0 \\neq 0, the inequality \\lVert s \\rVert_* \\leq 1 must become an equality since, when \\lVert s \\rVert_* &lt; 1, definition of the dual norm implies \\langle s, x_0 \\rangle \\leq \\lVert s \\rVert_* \\lVert x_0 \\rVert &lt; \\lVert x_0 \\rVert.\n\n\n\n\n\n\n\n\n\nThe conjugate norm in Example above does not appear by chance. It turns out that, in a completely similar manner for an arbitrary function f (not just for the norm), its subdifferential can be described in terms of the dual object — the Fenchel conjugate function.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCharacterization of the subdifferential through the conjugate function. Let f: E \\to \\mathbb{R} be a function defined on the set E in a Euclidean space. Let x_0 \\in E and let f^*: E^* \\to \\mathbb{R} be the conjugate function. Show that\n\n\\partial f(x_0) = \\{s \\in E^* : \\langle s, x_0 \\rangle = f^*(s) + f(x_0)\\},\n\n\n\n\n\nIn other words, a vector s \\in E^* is a subgradient of the function f at point x_0 if and only if the Fenchel-Young inequality \\langle s, x_0 \\rangle \\leq f^*(s) + f(x_0) becomes an equality.\nIn the case f = \\lVert \\cdot \\rVert, we have f^* = \\delta_{B_{\\lVert \\cdot \\rVert_*}(0,1)}, i.e., the conjugate function is equal to the indicator function of the ball B_{\\lVert \\cdot \\rVert_*}(0,1), and equation becomes.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nCriteria for equality in the Fenchel-Young inequality. Let f: E \\to \\mathbb{R} be a convex closed function, f^*: E^* \\to \\mathbb{R} the conjugate function, and let x \\in E, s \\in E^*. The following statements are equivalent:\n\n\\langle s, x \\rangle = f^*(s) + f(x).\ns \\in \\partial f(x).\nx \\in \\partial f^*(s).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nAccording to Exercise above, the condition \\langle s, x \\rangle = f^*(s) + f(x) is equivalent to s \\in \\partial f(x). On the other hand, since f is convex and closed, by the Fenchel-Moreau theorem, we have f^{**} = f. Applying previous results to the function f^*, it follows that the equality \\langle s, x \\rangle = f^*(s) + f(x) is equivalent to x \\in \\partial f^*(s).",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Subgradient.html#references",
    "href": "docs/theory/Subgradient.html#references",
    "title": "Subgradient and subdifferential",
    "section": "5 References",
    "text": "5 References\n\nLecture Notes for ORIE 6300: Mathematical Programming I by Damek Davis",
    "crumbs": [
      "Theory",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html",
    "href": "docs/theory/Optimality.html",
    "title": "Optimality conditions. KKT",
    "section": "",
    "text": "f(x) \\to \\min\\limits_{x \\in S}\n\n\nA set S is usually called a budget set.\n\nWe say that the problem has a solution if the budget set is not empty: x^* \\in S, in which the minimum or the infimum of the given function is achieved.\n\nA point x^* is a global minimizer if f(x^*) \\leq f(x) for all x.\nA point x^* is a local minimizer if there exists a neighborhood N of x^* such that f(x^*) \\leq f(x) for all x \\in N.\nA point x^* is a strict local minimizer (also called a strong local minimizer) if there exists a neighborhood N of x^* such that f(x^*) &lt; f(x) for all x \\in N with x \\neq x^*.\nWe call x^* a stationary point (or critical) if \\nabla f(x^*) = 0. Any local minimizer must be a stationary point.\n\n\n\n\n\n\n\nFigure 1: Illustration of different stationary (critical) points\n\n\n\n\n\n\n\n\n\nExtreme value (Weierstrass) theorem\n\n\n\n\n\nLet S \\subset \\mathbb{R}^n be a compact set, and let f(x) be a continuous function on S. Then, the point of the global minimum of the function f (x) on S exists.\n\n\n\n\n\n\n\n\n\n\nFigure 2: A lot of practical problems are theoretically solvable\n\n\n\n\n\n\n\n\n\nTaylor’s Theorem\n\n\n\n\n\nSuppose that f : \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and that p \\in \\mathbb{R}^n. Then we have: \nf(x + p) = f(x) + \\nabla f(x + tp)^T p \\quad \\text{ for some } t \\in (0, 1)\n\nMoreover, if f is twice continuously differentiable, we have:\n\n\\nabla f(x + p) = \\nabla f(x) + \\int_0^1 \\nabla^2 f(x + tp)p \\, dt\n\n\nf(x + p) = f(x) + \\nabla f(x)^T p + \\frac{1}{2} p^T \\nabla^2 f(x + tp) p  \\quad \\text{ for some } t \\in (0, 1)\n\n\n\n\n\n\n\nConsider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#background",
    "href": "docs/theory/Optimality.html#background",
    "title": "Optimality conditions. KKT",
    "section": "",
    "text": "f(x) \\to \\min\\limits_{x \\in S}\n\n\nA set S is usually called a budget set.\n\nWe say that the problem has a solution if the budget set is not empty: x^* \\in S, in which the minimum or the infimum of the given function is achieved.\n\nA point x^* is a global minimizer if f(x^*) \\leq f(x) for all x.\nA point x^* is a local minimizer if there exists a neighborhood N of x^* such that f(x^*) \\leq f(x) for all x \\in N.\nA point x^* is a strict local minimizer (also called a strong local minimizer) if there exists a neighborhood N of x^* such that f(x^*) &lt; f(x) for all x \\in N with x \\neq x^*.\nWe call x^* a stationary point (or critical) if \\nabla f(x^*) = 0. Any local minimizer must be a stationary point.\n\n\n\n\n\n\n\nFigure 1: Illustration of different stationary (critical) points\n\n\n\n\n\n\n\n\n\nExtreme value (Weierstrass) theorem\n\n\n\n\n\nLet S \\subset \\mathbb{R}^n be a compact set, and let f(x) be a continuous function on S. Then, the point of the global minimum of the function f (x) on S exists.\n\n\n\n\n\n\n\n\n\n\nFigure 2: A lot of practical problems are theoretically solvable\n\n\n\n\n\n\n\n\n\nTaylor’s Theorem\n\n\n\n\n\nSuppose that f : \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and that p \\in \\mathbb{R}^n. Then we have: \nf(x + p) = f(x) + \\nabla f(x + tp)^T p \\quad \\text{ for some } t \\in (0, 1)\n\nMoreover, if f is twice continuously differentiable, we have:\n\n\\nabla f(x + p) = \\nabla f(x) + \\int_0^1 \\nabla^2 f(x + tp)p \\, dt\n\n\nf(x + p) = f(x) + \\nabla f(x)^T p + \\frac{1}{2} p^T \\nabla^2 f(x + tp) p  \\quad \\text{ for some } t \\in (0, 1)\n\n\n\n\n\n\n\nConsider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#unconstrained-optimization",
    "href": "docs/theory/Optimality.html#unconstrained-optimization",
    "title": "Optimality conditions. KKT",
    "section": "2 Unconstrained optimization",
    "text": "2 Unconstrained optimization\n\n\n\n\n\n\nFirst-Order Necessary Conditions\n\n\n\n\n\nIf x^* is a local minimizer and f is continuously differentiable in an open neighborhood, then\n\n\\nabla f(x^*) = 0\n\\tag{1}\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nSuppose for contradiction that \\nabla f(x^*) \\neq 0. Define the vector p = -\\nabla f(x^*) and note that\n\np^T \\nabla f(x^*) = -\\| \\nabla f(x^*) \\|^2 &lt; 0\n\nBecause \\nabla f is continuous near x^*, there is a scalar T &gt; 0 such that\n\np^T \\nabla f(x^* + tp) &lt; 0, \\text{ for all } t \\in [0,T]\n\nFor any \\bar{t} \\in (0, T], we have by Taylor’s theorem that\n\nf(x^* + \\bar{t}p) = f(x^*) + \\bar{t} p^T \\nabla f(x^* + tp), \\text{ for some } t \\in (0,\\bar{t})\n\nTherefore, f(x^* + \\bar{t}p) &lt; f(x^*) for all \\bar{t} \\in (0, T]. We have found a direction from x^* along which f decreases, so x^* is not a local minimizer, leading to a contradiction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond-Order Sufficient Conditions\n\n\n\n\n\nSuppose that \\nabla^2 f is continuous in an open neighborhood of x^* and that\n\n\\nabla f(x^*) = 0 \\quad \\nabla^2 f(x^*) \\succ 0.\n\nThen x^* is a strict local minimizer of f.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nBecause the Hessian is continuous and positive definite at x^*, we can choose a radius r &gt; 0 such that \\nabla^2 f(x) remains positive definite for all x in the open ball B = \\{ z \\mid \\|z - x^*\\| &lt; r \\}. Taking any nonzero vector p with \\|p\\| &lt; r, we have x^* + p \\in B and so\n\nf(x^* + p) = f(x^*) + p^T \\nabla f(x^*) + \\frac{1}{2} p^T \\nabla^2 f(z) p\n\n\n= f(x^*) + \\frac{1}{2} p^T \\nabla^2 f(z) p\n\nwhere z = x^* + tp for some t \\in (0,1). Since z \\in B, we have p^T \\nabla^2 f(z) p &gt; 0, and therefore f(x^* + p) &gt; f(x^*), giving the result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeano surface\n\n\n\n\n\nNote, that if \\nabla f(x^*) = 0, \\nabla^2 f(x^*) \\succeq 0, i.e. the hessian is positive semidefinite, we cannot be sure if x^* is a local minimum.\n\nf(x,y) = (2x^2 - y)(x^2 - y)\n\n                        \n                                            \nOne can verify, that \\nabla^2 f(0, 0) = \\mathbb{0} and \\nabla f(0, 0) = (0, 0), but (0, 0) is not a local minimizer. Although the surface does not have a local minimizer at the origin, its intersection with any vertical plane through the origin (a plane with equation y=mx or x=0) is a curve that has a local minimum at the origin. In other words, if a point starts at the origin (0,0) of the plane, and moves away from the origin along any straight line, the value of (2x^2-y)(x^2 - y) will increase at the start of the motion. Nevertheless, (0,0) is not a local minimizer of the function, because moving along a parabola such as y=\\sqrt{2}x^2 will cause the function value to decrease.\n\n\n\n\nDirection d \\in \\mathbb{R}^n is a feasible direction at x^* \\in S \\subseteq \\mathbb{R}^n if small steps along d do not take us outside of S.\nConsider a set S \\subseteq \\mathbb{R}^n and a function f : \\mathbb{R}^n \\to \\mathbb{R}. Suppose that x^* \\in S is a point of local minimum for f over S, and further assume that f is continuously differentiable around x^*.\n\nThen for every feasible direction d \\in \\mathbb{R}^n at x^* it holds that \\nabla f(x^*)^\\top d \\geq 0.\nIf, additionally, S is convex then\n\n\\nabla f(x^*)^\\top(x − x^*) \\geq 0, \\forall x \\in S.\n\n\n\n\n\n\n\n\nFigure 3: General first order local optimality condition\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nBut how to determine optimality if the function is non-smooth?\n\n\n\n\n\n\n\n\n\n\nFermat’s rule\n\n\n\n\n\nSuppose f : \\mathbb{R}^n \\to\\mathbb{R} \\cup \\{\\infty\\}, then x^* is a global minimizer of f if and only if \n0 \\in \\partial f(x^*)\n\n\n\n\n\n\n2.1 Convex case\nIt should be mentioned, that in the convex case (i.e., f(x) is convex) necessary condition becomes sufficient.\nOne more important result for convex unconstrained case sounds as follows. If f(x): S \\to \\mathbb{R} - convex function defined on the convex set S, then:\n\nAny local minima is the global one.\nThe set of the local minimizers S^* is convex.\nIf f(x) - strictly or strongly (different cases 😀) convex function, then S^* contains only one single point S^* = \\{x^*\\}.",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#constrained-problem",
    "href": "docs/theory/Optimality.html#constrained-problem",
    "title": "Optimality conditions. KKT",
    "section": "3 Constrained problem",
    "text": "3 Constrained problem\n\n3.1 Optimization with equality conditions\n\n3.1.1 Intuition\nThings are pretty simple and intuitive in unconstrained problem. In this section we will add one equality constraint, i.e.\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h(x) = 0\n\\end{split}\n\nWe will try to illustrate approach to solve this problem through the simple example with f(x) = x_1 + x_2 and h(x) = x_1^2 + x_2^2 - 2.\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\n\n\n\nIllustration of KKT\n\n\nGenerally: in order to move from x_F along the budget set towards decreasing the function, we need to guarantee two conditions:\n\n\\langle \\delta x, \\nabla h(x_F) \\rangle = 0\n\n\n\\langle \\delta x, - \\nabla f(x_F) \\rangle &gt; 0\n\nLet’s assume, that in the process of such a movement we have come to the point where \n-\\nabla f(x) = \\nu \\nabla h(x)\n\n\n\\langle  \\delta x, - \\nabla f(x)\\rangle = \\langle  \\delta x, \\nu\\nabla h(x)\\rangle = 0  \n\nThen we came to the point of the budget set, moving from which it will not be possible to reduce our function. This is the local minimum in the constrained problem :)\n\n\n\nIllustration of KKT\n\n\nSo let’s define a Lagrange function (just for our convenience):\n\nL(x, \\nu) = f(x) + \\nu h(x)\n\nThen if the problem is regular (we will define it later) and the point x^* is the local minimum of the problem described above, then there exist \\nu^*:\n\n\\begin{split}\n& \\text{Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\text{ that's written above}\\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\text{ budget constraint}\\\\\n% & \\text{Sufficient conditions} \\\\\n% & \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle &gt; 0,\\\\\n% & \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h(x^*)^\\top y = 0\n\\end{split}\n\nWe should notice that L(x^*, \\nu^*) = f(x^*).\n\n\n3.1.2 General formulation\n\n\\tag{ECP}\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nSolution\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f(x) + \\nu^\\top h(x)\n\nLet f(x) and h_i(x) be twice differentiable at the point x^* and continuously differentiable in some neighborhood x^*. The local minimum conditions for x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p are written as\n\n\\begin{split}\n& \\text{ECP: Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\\\\n% & \\text{ECP: Sufficient conditions} \\\\\n% & \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle \\ge 0,\\\\\n% & \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0\n\\end{split}\n\nDepending on the behavior of the Hessian, the critical points can have a different character.\n\n\n\n\n    \n    \n    How eigenvalues of the hessian affects the critical point\n    \n\n\n\n\n    \n        \n        l1: 0\n        l2: 0\n        \n    \n\n    \n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nPose the optimization problem and solve them for linear system Ax = b, A \\in \\mathbb{m \\times n} for three cases (assuming the matrix is full rank):\n\nm &lt; n\nm = n\nm &gt; n\n\n\n\n\n\n\n\n\n3.2 Optimization with inequality conditions\n\n3.2.1 Example\n\nf(x) = x_1^2 + x_2^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\nThus, if the constraints of the type of inequalities are inactive in the constrained problem, then don’t worry and write out the solution to the unconstrained problem. However, this is not the whole story 🤔. Consider the second childish example\n\nf(x) = (x_1 - 1)^2 + (x_2 + 1)^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\n\n\n\nIllustration of KKT (inequality case)\n\n\nSo, we have a problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\nTwo possible cases:\n\n\n\n\n\n\n\ng(x) \\leq 0 is inactive. g(x^*) &lt; 0\ng(x) \\leq 0 is active. g(x^*) = 0\n\n\n\n\ng(x^*) &lt; 0  \\nabla f(x^*) = 0 \\nabla^2 f(x^*) &gt; 0\nNecessary conditions  g(x^*) = 0  - \\nabla f(x^*) = \\lambda \\nabla g(x^*), \\lambda &gt; 0  Sufficient conditions  \\langle y, \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle &gt; 0,  \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla g(x^*)^\\top y = 0\n\n\n\nCombining two possible cases, we can write down the general conditions for the problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0 \\\\\n&\n\\end{split}\n\nLet’s define the Lagrange function:\n\nL (x, \\lambda) = f(x) + \\lambda g(x)\n\nThe classical Karush-Kuhn-Tucker first and second order optimality conditions for a local minimizer x^*, stated under the linear independence constraint qualification (LICQ) (or other regularity conditions), can be written as follows:\nIf x^* is a local minimum of the problem described above, then there exists a unique Lagrange multiplier \\lambda^* such that:\n\n\\begin{split}\n    & (1) \\; \\nabla_x L (x^*, \\lambda^*) = 0 \\\\\n    & (2) \\; \\lambda^* \\geq 0 \\\\\n    & (3) \\; \\lambda^* g(x^*) = 0 \\\\\n    & (4) \\; g(x^*) \\leq 0\\\\\n    & (5) \\; \\forall y \\in C(x^*):  \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle &gt; 0 \\\\\n    &  \\text{where } C(x^*) = \\{y \\ \\in \\mathbb{R}^n |  \\nabla f(x^*) ^\\top y \\leq 0 \\text{ and } \\forall i \\in I(x^*):  \\nabla g_i(x^*)^⊤ y \\leq 0\n    \\} \\text{ is the critical cone.} \\\\\n    & I(x^*) = \\{i| g_i(x^*) = 0\\} \\\\\n\\end{split}\n\nIt’s noticeable, that L(x^*, \\lambda^*) = f(x^*). Conditions \\lambda^* = 0 , (1), (4) are the first scenario realization, and conditions \\lambda^* &gt; 0 , (1), (3) - the second one.\n\n\n3.2.2 General formulation\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nThis formulation is a general problem of mathematical programming.\nThe solution involves constructing a Lagrange function:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#sec-KKT",
    "href": "docs/theory/Optimality.html#sec-KKT",
    "title": "Optimality conditions. KKT",
    "section": "4 Karush-Kuhn-Tucker conditions",
    "text": "4 Karush-Kuhn-Tucker conditions\n\n@bibtexfile\n\n\n@misc{kuhn1951nonlinear,\n  title={Nonlinear programming, in (J. Neyman, ed.) Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability},\n  author={Kuhn, Harold W and Tucker, Albert W},\n  year={1951},\n  publisher={University of California Press, Berkeley}\n}\n\n\n📜 File\n\n\n\n\n@bibtexfile\n\n\n@article{karush1939minima,\n  title={Minima of functions of several variables with inequalities as side constraints},\n  author={Karush, William},\n  journal={M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago},\n  year={1939}\n}\n\n\n📜 File\n\n\n\n\n\n\n\n\n\nSubdifferential form of KKT\n\n\n\n\n\nLet X be a linear normed space, and let f_j: X \\to \\mathbb{R}, j = 0, 1, \\ldots, m, be convex proper (it never takes on the value -\\infty and also is not identically equal to \\infty) functions. Consider the problem\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in X}\\\\\n\\text{s.t. } & f_j(x) \\leq 0, \\; j = 1,\\ldots,m\\\\\n\\end{split}\n\nLet x^* \\in X be a minimum in problem above and the functions f_j, j = 0, 1, \\ldots, m, be continuous at the point x^*. Then there exist numbers \\lambda_j \\geq 0, j = 0, 1, \\ldots, m, such that\n\n\\sum_{j=0}^{m} \\lambda_j = 1,\n\n\n\\lambda_j f_j(x^*) = 0, \\quad j = 1, \\ldots, m,\n\n\n0 \\in \\sum_{j=0}^{m} \\lambda_j \\partial f_j(x^*).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nConsider the function\n\nf(x) = \\max\\{f_0(x) - f_0(x^*), f_1(x), \\ldots, f_m(x)\\}.\n\nThe point x^* is a global minimum of this function. Indeed, if at some point x_e \\in X the inequality f(x_e) &lt; 0 were satisfied, it would imply that f_0(x_e) &lt; f_0(x^*) and f_j(x_e) &lt; 0, j = 1, \\ldots, m, contradicting the minimality of x^* in problem above.\nThen, from Fermat’s theorem in subdifferential form, it follows that\n\n0 \\in \\partial f(x^*).\n\nBy the Dubovitskii-Milyutin theorem, we have\n\n\\partial f(x^*) = \\text{conv } \\left( \\bigcup\\limits_{j \\in I}\\partial f_j(x^*)\\right),\n\nwhere I = \\{0\\} \\cup \\{j : f_j(x^*) = 0, 1 \\leq j \\leq m\\}.\nTherefore, there exist g_j \\in \\partial f_j(x^*), j \\in I, such that\n\n\\sum_{j \\in I} \\lambda_j g_j = 0, \\quad \\sum\\limits_{j \\in I}\\lambda_j = 1, \\quad \\lambda_j \\geq 0, \\quad j \\in I.\n\nIt remains to set \\lambda_j = 0 for j \\notin I.\n\n\n\n\n\n\n\n\n\n\n4.1 Necessary conditions\nLet x^*, (\\lambda^*, \\nu^*) be a solution to a mathematical programming problem with zero duality gap (the optimal value for the primal problem p^* is equal to the optimal value for the dual problem d^*). Let also the functions f_0, f_i, h_i be differentiable.\n\n\\nabla_x L(x^*, \\lambda^*, \\nu^*) = 0\n\\nabla_\\nu L(x^*, \\lambda^*, \\nu^*) = 0\n\\lambda^*_i \\geq 0, i = 1,\\ldots,m\n\\lambda^*_i f_i(x^*) = 0, i = 1,\\ldots,m\nf_i(x^*) \\leq 0, i = 1,\\ldots,m\n\n\n\n4.2 Some regularity conditions\nThese conditions are required in order to make KKT solutions the necessary conditions. Some of them even turn necessary conditions into sufficient (for example, Slater’s). Moreover, if you have regularity, you can write down necessary second order conditions \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle \\geq 0 with semi-definite hessian of Lagrangian.\n\nSlater’s condition. If for a convex problem (i.e., assuming minimization, f_0,f_{i} are convex and h_{i} are affine), there exists a point x such that h(x)=0 and f_{i}(x)&lt;0 (existance of a strictly feasible point), then we have a zero duality gap and KKT conditions become necessary and sufficient.\nLinearity constraint qualification If f_{i} and h_{i} are affine functions, then no other condition is needed.\nFor other examples, see wiki.\n\n\n\n4.3 Sufficient conditions\nFor smooth, non-linear optimization problems, a second order sufficient condition is given as follows. The solution x^{*},\\lambda ^{*},\\nu ^{*}, which satisfies the KKT conditions (above) is a constrained local minimum if for the Lagrangian,\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)\n\nthe following conditions hold:\n\n\\begin{split}\n& \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle &gt; 0 \\\\\n& \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0, \\nabla f_0(x^*) ^\\top y \\leq 0,\\nabla f_j(x^*)^\\top y \\leq 0 \\\\\n& i = 1,\\ldots, p \\quad \\forall j: f_j(x^*) = 0\n\\end{split}",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Optimality.html#references",
    "href": "docs/theory/Optimality.html#references",
    "title": "Optimality conditions. KKT",
    "section": "5 References",
    "text": "5 References\n\nLecture on KKT conditions (very intuitive explanation) in course “Elements of Statistical Learning” @ KTH.\nOne-line proof of KKT\nOn the Second Order Optimality Conditions for Optimization Problems with Inequality Constraints\nOn Second Order Optimality Conditions in Nonlinear Optimization\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright.",
    "crumbs": [
      "Theory",
      "Optimality conditions. KKT"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html",
    "href": "docs/theory/Duality.html",
    "title": "Duality",
    "section": "",
    "text": "Duality lets us associate to any constrained optimization problem a concave maximization problem, whose solutions lower bound the optimal value of the original problem. What is interesting is that there are cases, when one can solve the primal problem by first solving the dual one. Now, consider a general constrained optimization problem:\n\n\\text{ Primal: }f(x) \\to \\min\\limits_{x \\in S}  \\qquad \\text{ Dual: } g(y) \\to \\max\\limits_{y \\in \\Omega}\n\nWe’ll build g(y), that preserves the uniform bound:\n\ng(y) \\leq f(x) \\qquad \\forall x \\in S, \\forall y \\in \\Omega\n\nAs a consequence:\n\n\\max\\limits_{y \\in \\Omega} g(y) \\leq \\min\\limits_{x \\in S} f(x)  \n\nWe’ll consider one of many possible ways to construct g(y) in case, when we have a general mathematical programming problem with functional constraints:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nAnd the Lagrangian, associated with this problem:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f_0(x) + \\lambda^\\top f(x) + \\nu^\\top h(x)\n\nWe assume \\mathcal{D} = \\bigcap\\limits_{i=0}^m\\textbf{dom } f_i \\cap \\bigcap\\limits_{i=1}^p\\textbf{dom } h_i is nonempty. We define the Lagrange dual function (or just dual function) g: \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} as the minimum value of the Lagrangian over x: for \\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} \\left( f_0(x) +\\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) \\right)\n\nWhen the Lagrangian is unbounded below in x, the dual function takes on the value −\\infty. Since the dual function is the pointwise infimum of a family of affine functions of (\\lambda, \\nu), it is concave, even when the original problem is not convex.\nLet us show, that the dual function yields lower bounds on the optimal value p^* of the original problem for any \\lambda \\succeq 0, \\nu. Suppose some \\hat{x} is a feasible point for the original problem, i.e., f_i(\\hat{x}) \\leq 0 and h_i(\\hat{x}) = 0, \\; λ \\succeq 0. Then we have:\n\nL(\\hat{x}, \\lambda, \\nu) = f_0(\\hat{x}) + \\underbrace{\\lambda^\\top f(\\hat{x})}_{\\leq 0} + \\underbrace{\\nu^\\top h(\\hat{x})}_{= 0} \\leq f_0(\\hat{x})\n\nHence\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) \\leq L(\\hat{x}, \\lambda, \\nu)  \\leq f_0(\\hat{x})\n\n\ng(\\lambda, \\nu) \\leq p^*\n\nA natural question is: what is the best lower bound that can be obtained from the Lagrange dual function? This leads to the following optimization problem:\n\n\\begin{split}\n& g(\\lambda, \\nu) \\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\; \\nu \\in \\mathbb{R}^p }\\\\\n\\text{s.t. } & \\lambda \\succeq 0\n\\end{split}\n\nThe term “dual feasible”, to describe a pair (\\lambda, \\nu) with \\lambda \\succeq 0 and g(\\lambda, \\nu) &gt; −\\infty, now makes sense. It means, as the name implies, that (\\lambda, \\nu) is feasible for the dual problem. We refer to (\\lambda^*, \\nu^*) as dual optimal or optimal Lagrange multipliers if they are optimal for the above problem.\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in S \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\nf_i(x) \\leq 0, i = 1,\\ldots,m h_i(x) = 0, \\; i = 1,\\ldots, p\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast-squares solution of linear equations\n\n\n\n\n\nWe are addressing a problem within a non-empty budget set, defined as follows: \n\\begin{aligned}\n    & \\text{min} \\quad x^T x \\\\\n    & \\text{s.t.} \\quad Ax = b,\n\\end{aligned}\n with the matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThis problem is devoid of inequality constraints, presenting m linear equality constraints instead. The Lagrangian is expressed as L(x, \\nu) = x^T x + \\nu^T (Ax - b), spanning the domain \\mathbb{R}^n \\times \\mathbb{R}^m. The dual function is denoted by g(\\nu) = \\inf_x L(x, \\nu). Given that L(x, \\nu) manifests as a convex quadratic function in terms of x, the minimizing x can be derived from the optimality condition \n\\nabla_x L(x, \\nu) = 2x + A^T \\nu = 0,\n leading to x = -(1/2)A^T \\nu. As a result, the dual function is articulated as \n    g(\\nu) = L(-(1/2)A^T \\nu, \\nu) = -(1/4)\\nu^T A A^T \\nu - b^T \\nu,\n emerging as a concave quadratic function within the domain \\mathbb{R}^p. According to the lower bound property (5.2), for any \\nu \\in \\mathbb{R}^p, the following holds true: \n    -(1/4)\\nu^T A A^T \\nu - b^T \\nu \\leq \\inf\\{x^T x \\,|\\, Ax = b\\}.\n Which is a simple non-trivial lower bound without any problem solving.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-way partitioning problem\n\n\n\n\n\n\nWe are examining a (nonconvex) problem: \n\\begin{aligned}\n    & \\text{minimize} \\quad x^T W x \\\\\n    & \\text{subject to} \\quad x_i^2 =1, \\quad i=1,\\ldots,n,\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe matrix W belongs to S_n. The constraints stipulate that the values of x_i can only be 1 or -1, rendering this problem analogous to finding a vector, with components \\pm1, that minimizes x^T W x. The set of feasible solutions is finite, encompassing 2^n points, thereby allowing, in theory, for the resolution of this problem by evaluating the objective value at each feasible point. However, as the count of feasible points escalates exponentially, this approach is viable only for modest-sized problems (for instance, when n \\leq 30). Generally, and especially when n exceeds 50, the problem poses a formidable challenge to solve.\nThis problem can be construed as a two-way partitioning problem over a set of n elements, denoted as \\{1, \\ldots , n\\}: A viable x corresponds to the partition \n\\{1,\\ldots,n\\} = \\{i|x_i =-1\\} \\cup \\{i|x_i =1\\}.\n The coefficient W_{ij} in the matrix represents the expense associated with placing elements i and j in the same partition, while -W_{ij} signifies the cost of segregating them. The objective encapsulates the aggregate cost across all pairs of elements, and the challenge posed by problem is to find the partition that minimizes the total cost.\nWe now derive the dual function for this problem. The Lagrangian is expressed as \nL(x,\\nu) = x^T W x + \\sum_{i=1}^n \\nu_i (x_i^2 -1) = x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu.\n By minimizing over x, we procure the Lagrange dual function: \ng(\\nu) = \\inf_x x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu\n= \\begin{cases}\\begin{array}{ll}\n    -\\mathbf{1}^T\\nu & \\text{if } W+\\text{diag}(\\nu) \\succeq 0 \\\\\n    -\\infty & \\text{otherwise},\n\\end{array} \\end{cases}\n\nexploiting the realization that the infimum of a quadratic form is either zero (when the form is positive semidefinite) or -\\infty (when it’s not).\nThis dual function furnishes lower bounds on the optimal value of the problem. For instance, we can adopt the particular value of the dual variable\n\n\\nu = -\\lambda_{\\text{min}}(W) \\mathbf{1}\n\nwhich is dual feasible, since\n\nW +\\text{diag}(\\nu)=W -\\lambda_{\\text{min}}(W) I \\succeq 0.\n\nThis renders a simple bound on the optimal value p^*\n\np^* \\geq -\\mathbf{1}^T\\nu = n \\lambda_{\\text{min}}(W).\n\nThe code for the problem is available here 🧑‍💻",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#motivation",
    "href": "docs/theory/Duality.html#motivation",
    "title": "Duality",
    "section": "",
    "text": "Duality lets us associate to any constrained optimization problem a concave maximization problem, whose solutions lower bound the optimal value of the original problem. What is interesting is that there are cases, when one can solve the primal problem by first solving the dual one. Now, consider a general constrained optimization problem:\n\n\\text{ Primal: }f(x) \\to \\min\\limits_{x \\in S}  \\qquad \\text{ Dual: } g(y) \\to \\max\\limits_{y \\in \\Omega}\n\nWe’ll build g(y), that preserves the uniform bound:\n\ng(y) \\leq f(x) \\qquad \\forall x \\in S, \\forall y \\in \\Omega\n\nAs a consequence:\n\n\\max\\limits_{y \\in \\Omega} g(y) \\leq \\min\\limits_{x \\in S} f(x)  \n\nWe’ll consider one of many possible ways to construct g(y) in case, when we have a general mathematical programming problem with functional constraints:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nAnd the Lagrangian, associated with this problem:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f_0(x) + \\lambda^\\top f(x) + \\nu^\\top h(x)\n\nWe assume \\mathcal{D} = \\bigcap\\limits_{i=0}^m\\textbf{dom } f_i \\cap \\bigcap\\limits_{i=1}^p\\textbf{dom } h_i is nonempty. We define the Lagrange dual function (or just dual function) g: \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} as the minimum value of the Lagrangian over x: for \\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} \\left( f_0(x) +\\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) \\right)\n\nWhen the Lagrangian is unbounded below in x, the dual function takes on the value −\\infty. Since the dual function is the pointwise infimum of a family of affine functions of (\\lambda, \\nu), it is concave, even when the original problem is not convex.\nLet us show, that the dual function yields lower bounds on the optimal value p^* of the original problem for any \\lambda \\succeq 0, \\nu. Suppose some \\hat{x} is a feasible point for the original problem, i.e., f_i(\\hat{x}) \\leq 0 and h_i(\\hat{x}) = 0, \\; λ \\succeq 0. Then we have:\n\nL(\\hat{x}, \\lambda, \\nu) = f_0(\\hat{x}) + \\underbrace{\\lambda^\\top f(\\hat{x})}_{\\leq 0} + \\underbrace{\\nu^\\top h(\\hat{x})}_{= 0} \\leq f_0(\\hat{x})\n\nHence\n\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu) \\leq L(\\hat{x}, \\lambda, \\nu)  \\leq f_0(\\hat{x})\n\n\ng(\\lambda, \\nu) \\leq p^*\n\nA natural question is: what is the best lower bound that can be obtained from the Lagrange dual function? This leads to the following optimization problem:\n\n\\begin{split}\n& g(\\lambda, \\nu) \\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\; \\nu \\in \\mathbb{R}^p }\\\\\n\\text{s.t. } & \\lambda \\succeq 0\n\\end{split}\n\nThe term “dual feasible”, to describe a pair (\\lambda, \\nu) with \\lambda \\succeq 0 and g(\\lambda, \\nu) &gt; −\\infty, now makes sense. It means, as the name implies, that (\\lambda, \\nu) is feasible for the dual problem. We refer to (\\lambda^*, \\nu^*) as dual optimal or optimal Lagrange multipliers if they are optimal for the above problem.\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\mathcal{D}} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in S \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\nf_i(x) \\leq 0, i = 1,\\ldots,m h_i(x) = 0, \\; i = 1,\\ldots, p\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast-squares solution of linear equations\n\n\n\n\n\nWe are addressing a problem within a non-empty budget set, defined as follows: \n\\begin{aligned}\n    & \\text{min} \\quad x^T x \\\\\n    & \\text{s.t.} \\quad Ax = b,\n\\end{aligned}\n with the matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThis problem is devoid of inequality constraints, presenting m linear equality constraints instead. The Lagrangian is expressed as L(x, \\nu) = x^T x + \\nu^T (Ax - b), spanning the domain \\mathbb{R}^n \\times \\mathbb{R}^m. The dual function is denoted by g(\\nu) = \\inf_x L(x, \\nu). Given that L(x, \\nu) manifests as a convex quadratic function in terms of x, the minimizing x can be derived from the optimality condition \n\\nabla_x L(x, \\nu) = 2x + A^T \\nu = 0,\n leading to x = -(1/2)A^T \\nu. As a result, the dual function is articulated as \n    g(\\nu) = L(-(1/2)A^T \\nu, \\nu) = -(1/4)\\nu^T A A^T \\nu - b^T \\nu,\n emerging as a concave quadratic function within the domain \\mathbb{R}^p. According to the lower bound property (5.2), for any \\nu \\in \\mathbb{R}^p, the following holds true: \n    -(1/4)\\nu^T A A^T \\nu - b^T \\nu \\leq \\inf\\{x^T x \\,|\\, Ax = b\\}.\n Which is a simple non-trivial lower bound without any problem solving.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-way partitioning problem\n\n\n\n\n\n\nWe are examining a (nonconvex) problem: \n\\begin{aligned}\n    & \\text{minimize} \\quad x^T W x \\\\\n    & \\text{subject to} \\quad x_i^2 =1, \\quad i=1,\\ldots,n,\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe matrix W belongs to S_n. The constraints stipulate that the values of x_i can only be 1 or -1, rendering this problem analogous to finding a vector, with components \\pm1, that minimizes x^T W x. The set of feasible solutions is finite, encompassing 2^n points, thereby allowing, in theory, for the resolution of this problem by evaluating the objective value at each feasible point. However, as the count of feasible points escalates exponentially, this approach is viable only for modest-sized problems (for instance, when n \\leq 30). Generally, and especially when n exceeds 50, the problem poses a formidable challenge to solve.\nThis problem can be construed as a two-way partitioning problem over a set of n elements, denoted as \\{1, \\ldots , n\\}: A viable x corresponds to the partition \n\\{1,\\ldots,n\\} = \\{i|x_i =-1\\} \\cup \\{i|x_i =1\\}.\n The coefficient W_{ij} in the matrix represents the expense associated with placing elements i and j in the same partition, while -W_{ij} signifies the cost of segregating them. The objective encapsulates the aggregate cost across all pairs of elements, and the challenge posed by problem is to find the partition that minimizes the total cost.\nWe now derive the dual function for this problem. The Lagrangian is expressed as \nL(x,\\nu) = x^T W x + \\sum_{i=1}^n \\nu_i (x_i^2 -1) = x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu.\n By minimizing over x, we procure the Lagrange dual function: \ng(\\nu) = \\inf_x x^T (W + \\text{diag}(\\nu)) x - \\mathbf{1}^T \\nu\n= \\begin{cases}\\begin{array}{ll}\n    -\\mathbf{1}^T\\nu & \\text{if } W+\\text{diag}(\\nu) \\succeq 0 \\\\\n    -\\infty & \\text{otherwise},\n\\end{array} \\end{cases}\n\nexploiting the realization that the infimum of a quadratic form is either zero (when the form is positive semidefinite) or -\\infty (when it’s not).\nThis dual function furnishes lower bounds on the optimal value of the problem. For instance, we can adopt the particular value of the dual variable\n\n\\nu = -\\lambda_{\\text{min}}(W) \\mathbf{1}\n\nwhich is dual feasible, since\n\nW +\\text{diag}(\\nu)=W -\\lambda_{\\text{min}}(W) I \\succeq 0.\n\nThis renders a simple bound on the optimal value p^*\n\np^* \\geq -\\mathbf{1}^T\\nu = n \\lambda_{\\text{min}}(W).\n\nThe code for the problem is available here 🧑‍💻",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#strong-duality",
    "href": "docs/theory/Duality.html#strong-duality",
    "title": "Duality",
    "section": "2 Strong duality",
    "text": "2 Strong duality\nIt is common to name this relation between optimals of primal and dual problems as weak duality. For problem, we have:\n\np^* \\geq d^*\n\nWhile the difference between them is often called duality gap:\n\np^* - d^* \\geq 0\n\nNote, that we always have weak duality, if we’ve formulated primal and dual problem. It means, that if we have managed to solve the dual problem (which is always concave, no matter whether the initial problem was or not), then we have some lower bound. Surprisingly, there are some notable cases, when these solutions are equal.\nStrong duality happens if duality gap is zero:\n\np^∗ = d^*\n\nNotice: both p^* and d^* may be + \\infty.\n\nSeveral sufficient conditions known!\n“Easy” necessary and sufficient conditions: unknown.\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIn the Least-squares solution of linear equations example above calculate the primal optimum p^* and the dual optimum d^* and check whether this problem has strong duality or not.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#useful-features",
    "href": "docs/theory/Duality.html#useful-features",
    "title": "Duality",
    "section": "3 Useful features",
    "text": "3 Useful features\n\nConstruction of lower bound on solution of the direct problem.\nIt could be very complicated to solve the initial problem. But if we have the dual problem, we can take an arbitrary y \\in \\Omega and substitute it in g(y) - we’ll immediately obtain some lower bound.\nChecking for the problem’s solvability and attainability of the solution.\nFrom the inequality \\max\\limits_{y \\in \\Omega} g(y) \\leq \\min\\limits_{x \\in S} f_0(x) follows: if \\min\\limits_{x \\in S} f_0(x) = -\\infty, then \\Omega = \\varnothing and vice versa.\nSometimes it is easier to solve a dual problem than a primal one.\nIn this case, if the strong duality holds: g(y^∗) = f_0(x^∗) we lose nothing.\nObtaining a lower bound on the function’s residual.\nf_0(x) - f_0^∗ \\leq f_0(x) - g(y) for an arbitrary y \\in \\Omega (suboptimality certificate). Moreover, p^* \\in [g(y), f_0(x)], d^* \\in [g(y), f_0(x)]\nDual function is always concave\nAs a pointwise minimum of affine functions.\n\n\n\n\n\n\n\nProjection onto probability simplex\n\n\n\n\n\nTo find the Euclidean projection of x \\in \\mathbb{R}^n onto probability simplex \\mathcal{P} = \\{z \\in \\mathbb{R}^n \\mid z \\succeq 0, \\mathbf{1}^\\top z = 1\\}, we solve the following problem:\n\n\\begin{split}\n& \\dfrac{1}{2}\\|y - x\\|_2^2 \\to \\min\\limits_{y \\in \\mathbb{R}^{n} \\succeq 0}\\\\\n\\text{s.t. } & \\mathbf{1}^\\top y = 1\n\\end{split}\n\nHint: Consider the problem of minimizing \\frac{1}{2}\\|y - x\\|_2^2 subject to subject to y \\succeq 0, \\mathbf{1}^\\top y = 1. Form the partial Lagrangian\n\nL(y, \\nu) = \\dfrac{1}{2}\\|y - x\\|_2^2 +\\nu(\\mathbf{1}^\\top y - 1),\n\nleaving the constraint y \\succeq 0 implicit. Show that y = (x − \\nu \\mathbf{1})_+ minimizes L(y, \\nu) over y \\succeq 0.\n\n\n\n\n\n\n\n\n\n\nProjection on the Euclidian Ball\n\n\n\n\n\nFind the projection of a point x on the Euclidian ball \n\\begin{split}\n& \\dfrac{1}{2}\\|y - x\\|_2^2 \\to \\min\\limits_{y \\in \\mathbb{R}^{n}}\\\\\n\\text{s.t. } & \\|y\\|_2^2 \\leq 1\n\\end{split}",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#slaters-condition",
    "href": "docs/theory/Duality.html#slaters-condition",
    "title": "Duality",
    "section": "4 Slater’s condition",
    "text": "4 Slater’s condition\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nIf for a convex optimization problem (i.e., assuming minimization, f_0,f_{i} are convex and h_{i} are affine), there exists a point x such that h(x)=0 and f_{i}(x)&lt;0 (existance of a strictly feasible point), then we have a zero duality gap and KKT conditions become necessary and sufficient.\n\n\n\n\n\n\n\n\n\n\nAn example of convex problem, when Slater’s condition does not hold\n\n\n\n\n\n\n\\min \\{ f_0(x) = x \\mid f_1(x) = \\frac{x^2}{2} \\leq 0 \\},\n\nThe only point in the budget set is: x^* = 0. However, it is impossible to find a non-negative \\lambda^* \\geq 0, such that \n\\nabla f_0(0) + \\lambda^* \\nabla f_1(0) = 1 + \\lambda^* x = 0.\n\n\n\n\n\n\n\n\n\n\n\nA nonconvex quadratic problem with strong duality\n\n\n\n\n\nOn rare occasions strong duality obtains for a nonconvex problem. As an important example, we consider the problem of minimizing a nonconvex quadratic function over the unit ball\n\n\\begin{split}\n& x^\\top A x  + 2b^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n, A \\nsucceq 0 and b \\in \\mathbb{R}^n. Since A \\nsucceq 0, this is not a convex problem. This problem is sometimes called the trust region problem, and arises in minimizing a second-order approximation of a function over the unit ball, which is the region in which the approximation is assumed to be approximately valid.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLagrangian and dual function\n\nL(x, \\lambda) = x^\\top A x + 2 b^\\top x + \\lambda (x^\\top x - 1) = x^\\top( A + \\lambda I)x + 2 b^\\top x - \\lambda\n\n\ng(\\lambda) = \\begin{cases} -b^\\top(A + \\lambda I)^{\\dagger}b - \\lambda &\\text{ if } A + \\lambda I \\succeq 0 \\\\ -\\infty, &\\text{ otherwise}  \\end{cases}\n\nDual problem:\n\n\\begin{split}\n& -b^\\top(A + \\lambda I)^{\\dagger}b - \\lambda \\to \\max\\limits_{\\lambda \\in \\mathbb{R}}\\\\\n\\text{s.t. } & A + \\lambda I \\succeq 0\n\\end{split}\n\n\n\\begin{split}\n& -\\sum\\limits_{i=1}^n \\dfrac{(q_i^\\top b)^2}{\\lambda_i + \\lambda} - \\lambda  \\to \\max\\limits_{\\lambda \\in \\mathbb{R}}\\\\\n\\text{s.t. } & \\lambda \\geq - \\lambda_{min}(A)\n\\end{split}\n\n\n\n\n\n\n\n\n\n\n\n4.1 Reminder of KKT statements:\nSuppose we have a general optimization problem (from the chapter)\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{1}\nand convex optimization problem (see corresponding chapter), where all equality constraints are affine: h_i(x) = a_i^Tx - b_i, i \\in 1, \\ldots p\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{2}\nThe Lagrangian is\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)\n\nThe KKT system is:\n\n\\begin{split}\n& \\nabla_x L(x^*, \\lambda^*, \\nu^*) = 0 \\\\\n& \\nabla_\\nu L(x^*, \\lambda^*, \\nu^*) = 0 \\\\\n& \\lambda^*_i \\geq 0, i = 1,\\ldots,m \\\\\n& \\lambda^*_i f_i(x^*) = 0, i = 1,\\ldots,m \\\\\n& f_i(x^*) \\leq 0, i = 1,\\ldots,m \\\\\n\\end{split}\n\\tag{3}\n\n\n\n\n\n\nKKT becomes necessary\n\n\n\n\n\nIf x^* is a solution of the original problem Equation 1, then if any of the following regularity conditions is satisfied:\n\nStrong duality If f_1, \\ldots f_m, h_1, \\ldots h_p are differentiable functions and we have a problem Equation 1 with zero duality gap, then Equation 3 are necessary (i.e. any optimal set x^*, \\lambda^*, \\nu^* should satisfy Equation 3)\nLCQ (Linearity constraint qualification). If f_1, \\ldots f_m, h_1, \\ldots h_p are affine functions, then no other condition is needed.\nLICQ (Linear independence constraint qualification). The gradients of the active inequality constraints and the gradients of the equality constraints are linearly independent at x^*\nSC (Slater’s condition) For a convex optimization problem Equation 2 (i.e., assuming minimization, f_i are convex and h_j is affine), there exists a point x such that h_j(x)=0 and g_i(x) &lt; 0.\n\nThan it should satisfy Equation 3\n\n\n\n\n\n\n\n\n\n\nKKT in convex case\n\n\n\n\n\nIf a convex optimization problem Equation 2 with differentiable objective and constraint functions satisfies Slater’s condition, then the KKT conditions provide necessary and sufficient conditions for optimality: Slater’s condition implies that the optimal duality gap is zero and the dual optimum is attained, so x^* is optimal if and only if there are (\\lambda^*,\\nu^*) that, together with x^*, satisfy the KKT conditions.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#applications",
    "href": "docs/theory/Duality.html#applications",
    "title": "Duality",
    "section": "5 Applications",
    "text": "5 Applications\n\n5.1 Connection between Fenchel duality and Lagrange duality\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\begin{split}\n& f_0(x) = \\sum_{i=1}^n f_i(x_i)\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b\n\\end{split}\n\nThe dual problem is thus\n\n\\begin{split}\n& -b \\nu - \\sum_{i=1}^n f_i^*(-\\nu a_i)  \\to \\max\\limits_{\\nu \\in \\mathbb{R}}\\\\\n\\text{s.t. } & \\nu \\geq - \\lambda_{\\text{min}}(A)\n\\end{split}\n\nwith (scalar) variable \\nu \\in \\mathbb{R}. Now suppose we have found an optimal dual variable \\nu^* (There are several simple methods for solving a convex problem with one scalar variable, such as the bisection method.). It is very easy to recover the optimal value for the primal problem.\n\n\n\n\nLet f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. We call Fenchel - Rockafellar problem the following minimization task:\n\nf(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\nwhere A^{-1}(G) := \\{x \\in V : Ax \\in G\\} — preimage of G. We’ll build the dual problem using variable separation. Let’s introduce new variable y = Ax. The problem could be rewritten:\n\n\\begin{split}\n& f(x) + g(y) \\to \\min\\limits_{x \\in E, \\; y \\in G }\\\\\n\\text{s.t. } & Ax = y\n\\end{split}\n\nLagrangian\n\nL(x,y, \\lambda) =  f(x) + g(y) + \\lambda^\\top (Ax - y)\n\nDual function\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, \\; y \\in G} L(x,y, \\lambda) \\\\\n&= \\min\\limits_{x \\in E}\\left[ f(x) + (A^*\\lambda)^\\top x \\right] + \\min\\limits_{y \\in G} \\left[g(y) - \\lambda^\\top y\\right] = \\\\\n&= -\\max\\limits_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] - \\max\\limits_{y \\in G} \\left[\\lambda^\\top y - g(y)\\right]\n\\end{split}\n\nNow, we need to remember the definition of the conjugate function:\n\n\\sup_{y \\in G}\\left[\\lambda^\\top y - g(y)\\right] = \\begin{cases} g^*(\\lambda), &\\text{ if } \\lambda \\in G_*\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\n\n\\sup_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] = \\begin{cases} f^*(-A^*\\lambda), &\\text{ if } \\lambda \\in (-A^*)^{-1}(E_*)\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\nSo, we have:\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, y \\in G} L(x,y, \\lambda) = \\\\\n&= \\begin{cases} -g^*(\\lambda) - f^*(-A^*\\lambda) &\\text{ if } \\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)\\\\ -\\infty, &\\text{ otherwise}  \\end{cases}\n\\end{split}\n\nwhich allows us to formulate one of the most important theorems, that connects dual problems and conjugate functions:\n\n\n\n\n\n\nFenchel - Rockafellar theorem\n\n\n\n\n\nLet f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. Let p^*, d^* \\in [- \\infty, + \\infty] - optimal values of primal and dual problems:\n\np^* = f(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\n\nd^* = f^*(-A^*\\lambda) + g^*(\\lambda) \\to \\min\\limits_{\\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)},\n\nThen we have weak duality: p^* \\geq d^*. Furthermore, if the functions f and g are convex and A(\\mathbf{relint}(E)) \\cap \\mathbf{relint}(G) \\neq \\varnothing, then we have strong duality: p^* = d^*. While points x^* \\in E \\cap A^{-1}(G) and \\lambda^* \\in G_* \\cap (-A^*)^{-1}(E_*) are optimal values for primal and dual problem if and only if:\n\n\\begin{split}\n-A^*\\lambda^* &\\in \\partial f(x^*) \\\\\n\\lambda^* &\\in \\partial g(Ax^*)\n\\end{split}\n\n\n\n\n\nConvex case is especially important since if we have Fenchel - Rockafellar problem with parameters (f, g, A), than the dual problem has the form (f^*, g^*, -A^*).\n\n\n5.2 Sensitivity analysis\nLet us switch from the original optimization problem\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{P}\n\nTo the perturbed version of it:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq u_i, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = v_i, \\; i = 1,\\ldots, p\n\\end{split}\n\\tag{Per}\n\nNote, that we still have the only variable x \\in \\mathbb{R}^n, while treating u \\in \\mathbb{R}^m, v \\in \\mathbb{R}^p as parameters. It is obvious, that \\text{Per}(u,v) \\to \\text{P} if u = 0, v = 0. We will denote the optimal value of \\text{Per} as p^*(u, v), while the optimal value of the original problem \\text{P} is just p^*. One can immediately say, that p^*(u, v) = p^*.\nSpeaking of the value of some i-th constraint we can say, that\n\nu_i = 0 leaves the original problem\nu_i &gt; 0 means that we have relaxed the inequality\nu_i &lt; 0 means that we have tightened the constraint\n\nOne can even show, that when \\text{P} is convex optimization problem, p^*(u,v) is a convex function.\nSuppose, that strong duality holds for the orriginal problem and suppose, that x is any feasible point for the perturbed problem:\n\n\\begin{split}\np^*(0,0) &= p^* = d^* = g(\\lambda^*, \\nu^*) \\leq \\\\\n& \\leq L(x, \\lambda^*, \\nu^*) = \\\\\n& = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i^* f_i(x) + \\sum\\limits_{i=1}^p\\nu_i^* h_i(x) \\leq \\\\\n& \\leq f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i^* u_i + \\sum\\limits_{i=1}^p\\nu_i^* v_i\n\\end{split}\n\nWhich means\n\n\\begin{split}\nf_0(x) \\geq p^*(0,0) - {\\lambda^*}^T u - {\\nu^*}^T v\n\\end{split}\n\nAnd taking the optimal x for the perturbed problem, we have:\n\np^*(u,v) \\geq p^*(0,0) - {\\lambda^*}^T u - {\\nu^*}^T v\n\\tag{4}\nIn scenarios where strong duality holds, we can draw several insights about the sensitivity of optimal solutions in relation to the Lagrange multipliers. These insights are derived from the inequality expressed in equation above:\n\nImpact of Tightening a Constraint (Large \\lambda_i^\\star):\nWhen the ith constraint’s Lagrange multiplier, \\lambda_i^\\star, holds a substantial value, and if this constraint is tightened (choosing u_i &lt; 0), there is a guarantee that the optimal value, denoted by p^\\star(u, v), will significantly increase.\nEffect of Adjusting Constraints with Large Positive or Negative \\nu_i^\\star:\n\nIf \\nu_i^\\star is large and positive and v_i &lt; 0 is chosen, or\n\nIf \\nu_i^\\star is large and negative and v_i &gt; 0 is selected,\nthen in either scenario, the optimal value p^\\star(u, v) is expected to increase greatly.\n\nConsequences of Loosening a Constraint (Small \\lambda_i^\\star):\nIf the Lagrange multiplier \\lambda_i^\\star for the ith constraint is relatively small, and the constraint is loosened (choosing u_i &gt; 0), it is anticipated that the optimal value p^\\star(u, v) will not significantly decrease.\nOutcomes of Tiny Adjustments in Constraints with Small \\nu_i^\\star:\n\nWhen \\nu_i^\\star is small and positive, and v_i &gt; 0 is chosen, or\n\nWhen \\nu_i^\\star is small and negative, and v_i &lt; 0 is opted for,\nin both cases, the optimal value p^\\star(u, v) will not significantly decrease.\n\n\nThese interpretations provide a framework for understanding how changes in constraints, reflected through their corresponding Lagrange multipliers, impact the optimal solution in problems where strong duality holds.\n\n\n5.3 Local sensitivity\nSuppose now that p^*(u, v) is differentiable at u = 0, v = 0.\n\n\\lambda_i^* = -\\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\quad \\nu_i^* = -\\dfrac{\\partial p^*(0,0)}{\\partial v_i}\n\\tag{5}\nTo show this result we consider the directional derivative of p^*(u,v) along the direction of some i-th basis vector e_i:\n\n\\lim_{t \\to 0} \\dfrac{p^*(t e_i,0) - p^*(0,0)}{t} = \\dfrac{\\partial p^*(0,0)}{\\partial u_i}\n\nFrom the inequality Equation 4 and taking the limit t \\to0 with t&gt;0 we have\n\n\\dfrac{p^*(t e_i,0) - p^*}{t} \\geq -\\lambda_i^* \\to  \\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\geq -\\lambda_i^*\n\nFor the negative t &lt; 0 we have:\n\n\\dfrac{p^*(t e_i,0) - p^*}{t} \\leq -\\lambda_i^* \\to  \\dfrac{\\partial p^*(0,0)}{\\partial u_i} \\leq -\\lambda_i^*\n\nThe same idea can be used to establish the fact about v_i.\nThe local sensitivity result Equation 5 provides a way to understand the impact of constraints on the optimal solution x^* of an optimization problem. If a constraint f_i(x^*) is negative at x^*, it’s not affecting the optimal solution, meaning small changes to this constraint won’t alter the optimal value. In this case, the corresponding optimal Lagrange multiplier will be zero, as per the principle of complementary slackness.\nHowever, if f_i(x^*) = 0, meaning the constraint is precisely met at the optimum, then the situation is different. The value of the i-th optimal Lagrange multiplier, \\lambda^*_i, gives us insight into how ‘sensitive’ or ‘active’ this constraint is. A small \\lambda^*_i indicates that slight adjustments to the constraint won’t significantly affect the optimal value. Conversely, a large \\lambda^*_i implies that even minor changes to the constraint can have a significant impact on the optimal solution.\n\n\n5.4 Shadow prices or tax interpretation\nConsider an enterprise where x represents its operational strategy and f_0(x) is the operating cost. Therefore, -f_0(x) denotes the profit in dollars. Each constraint f_i(x) \\leq 0 signifies a resource or regulatory limit. The goal is to maximize profit while adhering to these limits, which is equivalent to solving:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\n\\end{split}\n\nThe optimal profit here is -p^*.\nNow, imagine a scenario where exceeding limits is allowed, but at a cost. This cost is linear to the extent of violation, quantified by f_i. The charge for breaching the i^{th} constraint is \\lambda_i f_i(x). If f_i(x) &lt; 0, meaning the constraint is not fully utilized, \\lambda_i f_i(x) represents income for the firm. Here, \\lambda_i is the cost (in dollars) per unit of violation for f_i(x).\nFor instance, if f_1(x) \\leq 0 limits warehouse space, the firm can rent out extra space at \\lambda_1 dollars per square meter or rent out unused space for the same rate.\nThe firm’s total cost, considering operational and constraint costs, is L(x, \\lambda) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x). The firm aims to minimize L(x, \\lambda), resulting in an optimal cost g(\\lambda). The dual function g(\\lambda) represents the best possible cost for the firm based on the prices of constraints \\lambda, and the optimal dual value d^* is this cost under the most unfavorable price conditions.\nWeak duality implies that the cost in this flexible scenario (where the firm can trade constraint violations) is always less than or equal to the cost in the strict original scenario. This is because any optimal operation x^* from the original scenario will cost less in the flexible scenario, as the firm can earn from underused constraints.\nIf strong duality holds and the dual optimum is reached, the optimal \\lambda^* represents prices where the firm gains no extra advantage from trading constraint violations. These optimal \\lambda^* values are often termed ‘shadow prices’ for the original problem, indicating the hypothetical cost of constraint flexibility.\n\n\n5.5 Mixed strategies for matrix games\n\n\n\nThe scheme of a mixed strategy matrix game\n\n\nIn zero-sum matrix games, players 1 and 2 choose actions from sets \\{1,...,n\\} and \\{1,...,m\\}, respectively. The outcome is a payment from player 1 to player 2, determined by a payoff matrix P \\in \\mathbb{R}^{n \\times m}. Each player aims to use mixed strategies, choosing actions according to a probability distribution: player 1 uses probabilities u_k for each action i, and player 2 uses v_l.\nThe expected payoff from player 1 to player 2 is given by \\sum_{k=1}^{n} \\sum_{l=1}^{m} u_k v_l P_{kl} = u^T P v. Player 1 seeks to minimize this expected payoff, while player 2 aims to maximize it.\n\n5.5.1 Player 1’s Perspective\nAssuming player 2 knows player 1’s strategy u, player 2 will choose v to maximize u^T P v. The worst-case expected payoff is thus:\n\n\\max_{v \\geq 0, 1^T v = 1} u^T P v = \\max_{i=1,...,m} (P^T u)_i\n\nPlayer 1’s optimal strategy minimizes this worst-case payoff, leading to the optimization problem:\n\n\\begin{split}\n& \\min \\max_{i=1,...,m} (P^T u)_i\\\\\n& \\text{s.t. } u \\geq 0 \\\\\n& 1^T u = 1\n\\end{split}\n\\tag{6}\nThis forms a convex optimization problem with the optimal value denoted as p^*_1.\n\n\n5.5.2 Player 2’s Perspective\nConversely, if player 1 knows player 2’s strategy v, the goal is to minimize u^T P v. This leads to:\n\n\\min_{u \\geq 0, 1^T u = 1} u^T P v = \\min_{i=1,...,n} (P v)_i\n\nPlayer 2 then maximizes this to get the largest guaranteed payoff, solving the optimization problem:\n\n\\begin{split}\n& \\max \\min_{i=1,...,n} (P v)_i \\\\\n& \\text{s.t. }  v \\geq 0 \\\\\n& 1^T v = 1\n\\end{split}\n\\tag{7}\nThe optimal value here is p^*_2.\n\n\n5.5.3 Duality and Equivalence\nIt’s generally advantageous to know the opponent’s strategy, but surprisingly, in mixed strategy matrix games, this advantage disappears. The key lies in duality: the problems above are Lagrange duals. By formulating player 1’s problem as a linear program and introducing Lagrange multipliers, we find that the dual problem matches player 2’s problem. Due to strong duality in feasible linear programs, p^*_1 = p^*_2, showing no advantage in knowing the opponent’s strategy.\n\n\n5.5.4 Formulating and Solving the Lagrange Dual\nWe approach problem Equation 6 by setting it up as a linear programming (LP) problem. The goal is to minimize a variable t, subject to certain constraints:\n\nu \\geq 0,\nThe sum of elements in u equals 1 (1^T u = 1),\nP^T u is less than or equal to t times a vector of ones (P^T u \\leq t \\mathbf{1}).\n\nHere, t is an additional variable in the real numbers (t \\in \\mathbb{R}).\n\n\n5.5.5 Constructing the Lagrangian\nWe introduce multipliers for the constraints: \\lambda for P^T u \\leq t \\mathbf{1}, \\mu for u \\geq 0, and \\nu for 1^T u = 1. The Lagrangian is then formed as:\n\nL = t + \\lambda^T (P^T u - t \\mathbf{1}) - \\mu^T u + \\nu (1 - 1^T u) = \\nu + (1 - 1^T \\lambda)t + (P\\lambda - \\nu \\mathbf{1} - \\mu)^T u\n\n\n\n5.5.6 Defining the Dual Function\nThe dual function g(\\lambda, \\mu, \\nu) is defined as:\n\ng(\\lambda, \\mu, \\nu) =\n\\begin{cases}\n\\nu & \\text{if } 1^T\\lambda=1 \\text{ and } P\\lambda - \\nu \\mathbf{1} = \\mu \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}\n\n\n\n5.5.7 Solving the Dual Problem\nThe dual problem seeks to maximize \\nu under the following conditions:\n\n\\lambda \\geq 0,\nThe sum of elements in \\lambda equals 1 (1^T \\lambda = 1),\n\\mu \\geq 0,\nP\\lambda - \\nu \\mathbf{1} = \\mu.\n\nUpon eliminating \\mu, we obtain the Lagrange dual of Equation 6:\n\n\\begin{split}\n& \\max \\nu \\\\\n& \\text{s.t. }   \\lambda \\geq 0 \\\\\n&  \\lambda \\geq 0 \\\\\n& P\\lambda \\geq \\nu \\mathbf{1}\n\\end{split}\n\n\n\n5.5.8 Conclusion\nThis formulation shows that the Lagrange dual problem is equivalent to problem Equation 7. Given the feasibility of these linear programs, strong duality holds, meaning the optimal values of Equation 6 and Equation 7 are equal.",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Duality.html#references",
    "href": "docs/theory/Duality.html#references",
    "title": "Duality",
    "section": "6 References",
    "text": "6 References\n\nConvex Optimization — Boyd & Vandenberghe @ Stanford\nCourse Notes for EE227C. Lecture 13\nCourse Notes for EE227C. Lecture 14\nOptimality conditions\nSeminar 7 @ CMC MSU\nSeminar 8 @ CMC MSU\nConvex Optimization @ Berkeley - 10th lecture",
    "crumbs": [
      "Theory",
      "Duality"
    ]
  },
  {
    "objectID": "docs/theory/Convex_optimization_problem.html",
    "href": "docs/theory/Convex_optimization_problem.html",
    "title": "Convex optimization problem",
    "section": "",
    "text": "The idea behind the definition of a convex optimization problem\n\n\nNote, that there is an agreement in notation of mathematical programming. The problems of the following type are called Convex optimization problem:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{COP}\n\nwhere all the functions f_0(x), f_1(x), \\ldots, f_m(x) are convex and all the equality constraints are affine. It sounds a bit strange, but not all convex problems are convex optimization problems.\n\n\\tag{CP}\nf_0(x) \\to \\min\\limits_{x \\in S},\n\nwhere f_0(x) is a convex function, defined on the convex set S. The necessity of affine equality constraint is essential.\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis problem is not a convex optimization problem (but implies minimizing the convex function over the convex set):\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& (x_1 + x_2)^2 = 0,\n\\end{split}\n\\tag{CP}\n\nwhile the following equivalent problem is a convex optimization problem\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& x_1 + x_2 = 0,\n\\end{split}\n\\tag{COP}\n\n\n\n\n\nSuch confusion in notation is sometimes being avoided by naming problems of type \\text{(CP)} as abstract form convex optimization problem.",
    "crumbs": [
      "Theory",
      "Convex optimization problem"
    ]
  },
  {
    "objectID": "docs/theory/Convex_optimization_problem.html#convex-optimization-problem",
    "href": "docs/theory/Convex_optimization_problem.html#convex-optimization-problem",
    "title": "Convex optimization problem",
    "section": "",
    "text": "The idea behind the definition of a convex optimization problem\n\n\nNote, that there is an agreement in notation of mathematical programming. The problems of the following type are called Convex optimization problem:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{COP}\n\nwhere all the functions f_0(x), f_1(x), \\ldots, f_m(x) are convex and all the equality constraints are affine. It sounds a bit strange, but not all convex problems are convex optimization problems.\n\n\\tag{CP}\nf_0(x) \\to \\min\\limits_{x \\in S},\n\nwhere f_0(x) is a convex function, defined on the convex set S. The necessity of affine equality constraint is essential.\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis problem is not a convex optimization problem (but implies minimizing the convex function over the convex set):\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& (x_1 + x_2)^2 = 0,\n\\end{split}\n\\tag{CP}\n\nwhile the following equivalent problem is a convex optimization problem\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& x_1 + x_2 = 0,\n\\end{split}\n\\tag{COP}\n\n\n\n\n\nSuch confusion in notation is sometimes being avoided by naming problems of type \\text{(CP)} as abstract form convex optimization problem.",
    "crumbs": [
      "Theory",
      "Convex optimization problem"
    ]
  },
  {
    "objectID": "docs/theory/Convex_optimization_problem.html#materials",
    "href": "docs/theory/Convex_optimization_problem.html#materials",
    "title": "Convex optimization problem",
    "section": "2 Materials",
    "text": "2 Materials\n\nConvex Optimization — Boyd & Vandenberghe @ Stanford",
    "crumbs": [
      "Theory",
      "Convex optimization problem"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html",
    "href": "docs/theory/Conjugate_set.html",
    "title": "Conjugate set",
    "section": "",
    "text": "Let S \\subseteq \\mathbb{R}^n be an arbitrary non-empty set. Then its conjugate set is defined as:\n\nS^* = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\}\n\n\n\n\n\n\n\nFigure 1: Convex sets may be described in a dual way - through the elements of the set and through the set of hyperplanes supporting it\n\n\n\nA set S^{**} is called double conjugate to a set S if:\n\nS^{**} = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}\n\n\nThe sets S_1 and S_2 are called inter-conjugate if S_1^* = S_2, S_2^* = S_1.\nA set S is called self-conjugate if S^{*} = S.\n\n\n\n\n\nA conjugate set is always closed, convex, and contains zero.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup \\{0\\}) }\n  \nIf S_1 \\subseteq S_2, then S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*.\nIf S is closed, convex, and includes 0, then S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that S^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\overline{S}\\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*.\nLet p \\in S^* and x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Then by virtue of the continuity of the function f(x) = p^Tx, we have: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. So p \\in \\left(\\overline{S}\\right)^*, hence S^* \\subset \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*.\nLet p \\in S^*, x_0 \\in \\mathbf{conv}(S), i.e., x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nSo p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. So p \\in \\left( \\mathbf{conv}(S) \\right)^*, hence S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that if B(0,r) is a ball of radius r by some norm centered at zero, then \\left( B(0,r) \\right)^* = B(0,1/r).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet B(0,r) = X, B(0,1/r) = Y. Take the normal vector p \\in X^*, then for any x \\in X: p^Tx \\ge -1.\nFrom all points of the ball X, take such a point x \\in X that its scalar product of p: p^Tx is minimal, then this is the point x = -\\frac{p}{\\|p\\|}r.\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\||}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nSo X^* \\subset Y.\nNow let p \\in Y. We need to show that p \\in X^*, i.e., \\langle p, x\\rangle \\geq -1. It’s enough to apply the Cauchy-Bunyakovsky inequality:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nThe latter comes from the fact that p \\in B(0,1/r) and x \\in B(0,r).\nSo Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA conjugate cone to a cone K is a set K^* such that:\n\nK^* = \\left\\{ y \\mid \\langle x, y\\rangle \\ge 0 \\quad \\forall x \\in K\\right\\}\n\nTo show that this definition follows directly from the theorem above, recall what a conjugate set is and what a cone \\forall \\lambda &gt; 0 is.\n\n\\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\} \\to \\to \\{\\lambda y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -\\dfrac{1}{\\lambda} \\;\\; \\forall x\\in S\\}\n\n\n\n\n\n\n\nFigure 2: Illustration of dual cone\n\n\n\n\n\n\n\nLet K be a closed convex cone. Then K^{**} = K.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n and a cone K \\subseteq \\mathbb{R}^n:\n\n  \\left( S + K \\right)^* = S^* \\cap K^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n, then:\n\n  \\left( \\sum\\limits_{i=1}^m K_i \\right)^* = \\bigcap\\limits_{i=1}^m K_i^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n. Let also their intersection have an interior point, then:\n\n  \\left( \\bigcap\\limits_{i=1}^m K_i \\right)^* = \\sum\\limits_{i=1}^m K_i^*\n  \n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the conjugate cone for a monotone nonnegative cone:\n\nK = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nNote that:\n\n\\sum\\limits_{i=1}^nx_iy_i = y_1 (x_1-x_2) + (y_1 + y_2)(x_2 - x_3) + \\ldots + (y_1 + y_2 + \\ldots + y_{n-1})(x_{n-1} - x_n) + (y_1 + \\ldots + y_n)x_n\n\nSince in the presented sum in each summand, the second multiplier in each summand is non-negative, then:\n\ny_1 \\ge 0, \\;\\; y_1 + y_2 \\ge 0, \\;\\;\\ldots, \\;\\;\\;y_1 + \\ldots + y_n \\ge 0\n\nSo K^* = \\left\\{ y \\mid \\sum\\limits_{i=1}^k y_i \\ge 0, k = \\overline{1,n}\\right\\}.\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of solutions to a system of linear inequalities and equalities is a polyhedron:\n\nAx \\preceq b, \\;\\;\\; Cx = d\n\nHere A \\in \\mathbb{R}^{m\\times n}, C \\in \\mathbb{R}^{p \\times n}, and the inequality is a piecewise inequality.\n\n\n\n\n\n\nFigure 3: Polyhedra\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x_1, \\ldots, x_m \\in \\mathbb{R}^n. Conjugate to a polyhedral set:\n\nS = \\mathbf{conv}(x_1, \\ldots, x_k) + \\mathbf{cone}(x_{k+1}, \\ldots, x_m)\n\nis a polyhedron (polyhedron):\n\nS^* = \\left\\{ p \\in \\mathbb{R}^n \\mid \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k} ; \\langle p, x_i\\rangle \\ge 0, i = \\overline{k+1,m} \\right\\}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nLet S = X, S^* = Y. Take some p \\in X^*, then \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k}. At the same time, for any \\theta &gt; 0, i = \\overline{k+1,m}:\n\n  \\langle p, x_i\\rangle \\ge -1 \\to \\langle p, \\theta x_i\\rangle \\ge -1\n  \n\n  \\langle p, x_i\\rangle \\ge -\\frac{1}{\\theta} \\to \\langle p, x_i\\rangle \\geq 0.\n  \nSo p \\in Y \\to X^* \\subset Y.\nSuppose, on the other hand, that p \\in Y. For any point x \\in X:\n\n   x = \\sum\\limits_{i=1}^m\\theta_i x_i \\;\\;\\;\\;\\;\\;\\; \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0\n  \nSo:\n\n  \\langle p, x\\rangle = \\sum\\limits_{i=1}^m\\theta_i \\langle p, x_i\\rangle = \\sum\\limits_{i=1}^k\\theta_i \\langle p, x_i\\rangle + \\sum\\limits_{i=k+1}^m\\theta_i \\langle p, x_i\\rangle \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) + \\sum\\limits_{i=1}^k\\theta_i \\cdot 0 = -1.\n  \nSo p \\in X^* \\to Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind and represent the set conjugate to a polyhedral cone in the plane:\n\nS = \\mathbf{cone} \\left\\{ (-3,1), (2,3), (4,5)\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nUsing the theorem above:\n\nS^* = \\left\\{ -3p_1 + p_2 \\ge 0, 2p_1 + 3p_2 \\ge 0, 4p_1+5p_2 \\ge 0 \\right\\}\n\n\n\n\n\n\n\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) \\; Ax = b, x \\ge 0\n\n\n2) \\; p^\\top A \\ge 0, \\langle p,b\\rangle &lt; 0.\n\nAx = b when x \\geq 0 means that b lies in a cone stretched over the columns of the matrix A.\npA \\geq 0, \\; \\langle p, b \\rangle &lt; 0 means that there exists a separating hyperplane between the vector b and the cone of columns of the matrix A.\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) Ax \\leq b\n\n\n2) p^\\top A = 0, \\langle p,b\\rangle &lt; 0, p \\ge 0.\n\nIf in the minimization linear programming problem, the budget set is non-empty and the target function is bounded on it from below, then the problem has a solution.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#conjugate-fenchel-conjugate-dual-fenchel-dual-set",
    "href": "docs/theory/Conjugate_set.html#conjugate-fenchel-conjugate-dual-fenchel-dual-set",
    "title": "Conjugate set",
    "section": "",
    "text": "Let S \\subseteq \\mathbb{R}^n be an arbitrary non-empty set. Then its conjugate set is defined as:\n\nS^* = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\}\n\n\n\n\n\n\n\nFigure 1: Convex sets may be described in a dual way - through the elements of the set and through the set of hyperplanes supporting it\n\n\n\nA set S^{**} is called double conjugate to a set S if:\n\nS^{**} = \\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}\n\n\nThe sets S_1 and S_2 are called inter-conjugate if S_1^* = S_2, S_2^* = S_1.\nA set S is called self-conjugate if S^{*} = S.\n\n\n\n\n\nA conjugate set is always closed, convex, and contains zero.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup \\{0\\}) }\n  \nIf S_1 \\subseteq S_2, then S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*.\nIf S is closed, convex, and includes 0, then S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that S^* = \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\overline{S}\\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*.\nLet p \\in S^* and x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Then by virtue of the continuity of the function f(x) = p^Tx, we have: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. So p \\in \\left(\\overline{S}\\right)^*, hence S^* \\subset \\left(\\overline{S}\\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*.\nLet p \\in S^*, x_0 \\in \\mathbf{conv}(S), i.e., x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nSo p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. So p \\in \\left( \\mathbf{conv}(S) \\right)^*, hence S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve that if B(0,r) is a ball of radius r by some norm centered at zero, then \\left( B(0,r) \\right)^* = B(0,1/r).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet B(0,r) = X, B(0,1/r) = Y. Take the normal vector p \\in X^*, then for any x \\in X: p^Tx \\ge -1.\nFrom all points of the ball X, take such a point x \\in X that its scalar product of p: p^Tx is minimal, then this is the point x = -\\frac{p}{\\|p\\|}r.\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\||}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nSo X^* \\subset Y.\nNow let p \\in Y. We need to show that p \\in X^*, i.e., \\langle p, x\\rangle \\geq -1. It’s enough to apply the Cauchy-Bunyakovsky inequality:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nThe latter comes from the fact that p \\in B(0,1/r) and x \\in B(0,r).\nSo Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA conjugate cone to a cone K is a set K^* such that:\n\nK^* = \\left\\{ y \\mid \\langle x, y\\rangle \\ge 0 \\quad \\forall x \\in K\\right\\}\n\nTo show that this definition follows directly from the theorem above, recall what a conjugate set is and what a cone \\forall \\lambda &gt; 0 is.\n\n\\{y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\} \\to \\to \\{\\lambda y \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -\\dfrac{1}{\\lambda} \\;\\; \\forall x\\in S\\}\n\n\n\n\n\n\n\nFigure 2: Illustration of dual cone\n\n\n\n\n\n\n\nLet K be a closed convex cone. Then K^{**} = K.\nFor an arbitrary set S \\subseteq \\mathbb{R}^n and a cone K \\subseteq \\mathbb{R}^n:\n\n  \\left( S + K \\right)^* = S^* \\cap K^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n, then:\n\n  \\left( \\sum\\limits_{i=1}^m K_i \\right)^* = \\bigcap\\limits_{i=1}^m K_i^*\n  \nLet K_1, \\ldots, K_m be cones in \\mathbb{R}^n. Let also their intersection have an interior point, then:\n\n  \\left( \\bigcap\\limits_{i=1}^m K_i \\right)^* = \\sum\\limits_{i=1}^m K_i^*\n  \n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the conjugate cone for a monotone nonnegative cone:\n\nK = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nNote that:\n\n\\sum\\limits_{i=1}^nx_iy_i = y_1 (x_1-x_2) + (y_1 + y_2)(x_2 - x_3) + \\ldots + (y_1 + y_2 + \\ldots + y_{n-1})(x_{n-1} - x_n) + (y_1 + \\ldots + y_n)x_n\n\nSince in the presented sum in each summand, the second multiplier in each summand is non-negative, then:\n\ny_1 \\ge 0, \\;\\; y_1 + y_2 \\ge 0, \\;\\;\\ldots, \\;\\;\\;y_1 + \\ldots + y_n \\ge 0\n\nSo K^* = \\left\\{ y \\mid \\sum\\limits_{i=1}^k y_i \\ge 0, k = \\overline{1,n}\\right\\}.\n\n\n\n\n\n\n\n\n\n\n\n\nThe set of solutions to a system of linear inequalities and equalities is a polyhedron:\n\nAx \\preceq b, \\;\\;\\; Cx = d\n\nHere A \\in \\mathbb{R}^{m\\times n}, C \\in \\mathbb{R}^{p \\times n}, and the inequality is a piecewise inequality.\n\n\n\n\n\n\nFigure 3: Polyhedra\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x_1, \\ldots, x_m \\in \\mathbb{R}^n. Conjugate to a polyhedral set:\n\nS = \\mathbf{conv}(x_1, \\ldots, x_k) + \\mathbf{cone}(x_{k+1}, \\ldots, x_m)\n\nis a polyhedron (polyhedron):\n\nS^* = \\left\\{ p \\in \\mathbb{R}^n \\mid \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k} ; \\langle p, x_i\\rangle \\ge 0, i = \\overline{k+1,m} \\right\\}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nLet S = X, S^* = Y. Take some p \\in X^*, then \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k}. At the same time, for any \\theta &gt; 0, i = \\overline{k+1,m}:\n\n  \\langle p, x_i\\rangle \\ge -1 \\to \\langle p, \\theta x_i\\rangle \\ge -1\n  \n\n  \\langle p, x_i\\rangle \\ge -\\frac{1}{\\theta} \\to \\langle p, x_i\\rangle \\geq 0.\n  \nSo p \\in Y \\to X^* \\subset Y.\nSuppose, on the other hand, that p \\in Y. For any point x \\in X:\n\n   x = \\sum\\limits_{i=1}^m\\theta_i x_i \\;\\;\\;\\;\\;\\;\\; \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0\n  \nSo:\n\n  \\langle p, x\\rangle = \\sum\\limits_{i=1}^m\\theta_i \\langle p, x_i\\rangle = \\sum\\limits_{i=1}^k\\theta_i \\langle p, x_i\\rangle + \\sum\\limits_{i=k+1}^m\\theta_i \\langle p, x_i\\rangle \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) + \\sum\\limits_{i=1}^k\\theta_i \\cdot 0 = -1.\n  \nSo p \\in X^* \\to Y \\subset X^*.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind and represent the set conjugate to a polyhedral cone in the plane:\n\nS = \\mathbf{cone} \\left\\{ (-3,1), (2,3), (4,5)\\right\\}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nUsing the theorem above:\n\nS^* = \\left\\{ -3p_1 + p_2 \\ge 0, 2p_1 + 3p_2 \\ge 0, 4p_1+5p_2 \\ge 0 \\right\\}\n\n\n\n\n\n\n\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) \\; Ax = b, x \\ge 0\n\n\n2) \\; p^\\top A \\ge 0, \\langle p,b\\rangle &lt; 0.\n\nAx = b when x \\geq 0 means that b lies in a cone stretched over the columns of the matrix A.\npA \\geq 0, \\; \\langle p, b \\rangle &lt; 0 means that there exists a separating hyperplane between the vector b and the cone of columns of the matrix A.\n\n\nLet A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Then one and only one of the following two systems has a solution:\n\n1) Ax \\leq b\n\n\n2) p^\\top A = 0, \\langle p,b\\rangle &lt; 0, p \\ge 0.\n\nIf in the minimization linear programming problem, the budget set is non-empty and the target function is bounded on it from below, then the problem has a solution.",
    "crumbs": [
      "Theory",
      "Conjugate set"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html",
    "href": "docs/methods/zom/simulated-annealing.html",
    "title": "Simulated annealing",
    "section": "",
    "text": "We need to optimize the global optimum of a given function on some space using only the values of the function in some points on the space.\n\n\\min_{x \\in X} F(x) = F(x^*)\n\nSimulated Annealing is a probabilistic technique for approximating the global optimum of a given function.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#problem",
    "href": "docs/methods/zom/simulated-annealing.html#problem",
    "title": "Simulated annealing",
    "section": "",
    "text": "We need to optimize the global optimum of a given function on some space using only the values of the function in some points on the space.\n\n\\min_{x \\in X} F(x) = F(x^*)\n\nSimulated Annealing is a probabilistic technique for approximating the global optimum of a given function.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#algorithm",
    "href": "docs/methods/zom/simulated-annealing.html#algorithm",
    "title": "Simulated annealing",
    "section": "2 Algorithm",
    "text": "2 Algorithm\nThe name and inspiration come from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce their defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy. The simulation of annealing can be used to find an approximation of a global minimum for a function with many variables.\n\n2.1 Steps of the Algorithm\nStep 1 Let k = 0 - current iteration, T = T_k - initial temperature.\nStep 2 Let x_k \\in X - some random point from our space\nStep 3 Let decrease the temperature by following rule T_{k+1} = \\alpha T_k where 0 &lt; \\alpha &lt; 1 - some constant that often is closer to 1\nStep 4 Let x_{k+1} = g(x_k) - the next point which was obtained from previous one by some random rule. It is usually assumed that this rule works so that each subsequent approximation should not differ very much.\nStep 5 Calculate \\Delta E = E(x_{k+1}) - E(x_{k}), where E(x) - the function that determines the energy of the system at this point. It is supposed that energy has the minimum in desired value x^*.\nStep 6 If \\Delta E &lt; 0 then the approximation found is better than it was. So accept x_{k+1} as new started point at the next step and go to the step Step 3\nStep 7 If \\Delta E &gt;= 0, then we accept x_{k+1} with the probability of P(\\Delta E) = \\exp^{-\\Delta E / T_k}. If we don’t accept x_{k+1}, then we let k = k+ 1. Go to the step Step 3\nThe algorithm can stop working according to various criteria, for example, achieving an optimal state or lowering the temperature below a predetermined level T_{min}.\n\n\n2.2 Convergence\nAs it mentioned in Simulated annealing: a proof of convergence the algorithm converges almost surely to a global maximum.\n\n\n2.3 Illustration\nA gif from Wikipedia:\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#example",
    "href": "docs/methods/zom/simulated-annealing.html#example",
    "title": "Simulated annealing",
    "section": "3 Example",
    "text": "3 Example\nIn our example we solve the N queens puzzle - the problem of placing N chess queens on an N×N chessboard so that no two queens threaten each other.\n\n\n\nIllustration\n\n\n\n3.1 The Problem\nLet E(x) - the number of intersections, where x - the array of placement queens at the field (the number in array means the column, the index of the number means the row).\nThe problem is to find x^* where E(x^*) = \\min_{x \\in X} E(x) - the global minimum, that is predefined and equals to 0 (no two queens threaten each other).\nIn this code x_0 = [0,1,2,...,N] that means all queens are placed at the board’s diagonal. So at the beginning E = N(N-1), because every queen intersects others.\n\n\n3.2 Results\nResults of applying this algorithm with \\alpha = 0.95 to the N queens puzzle for N = 10 averaged by 100 runs are below:\n\n\n\nIllustration\n\n\nResults of running the code for N from 4 to 40 and measuring the time it takes to find the solution averaged by 100 runs are below:\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#code",
    "href": "docs/methods/zom/simulated-annealing.html#code",
    "title": "Simulated annealing",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Simulated annealing"
    ]
  },
  {
    "objectID": "docs/methods/zom/index.html",
    "href": "docs/methods/zom/index.html",
    "title": "Zero order methods",
    "section": "",
    "text": "Illustration\nNow we have only zero order information from the oracle. Typical speed of convergence of these methods is sublinear. A lot of methods are referred both to zero order methods and global optimization.",
    "crumbs": [
      "Methods",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/methods/zom/index.html#code",
    "href": "docs/methods/zom/index.html#code",
    "title": "Zero order methods",
    "section": "1 Code",
    "text": "1 Code\n\nGlobal optimization illustration - Open In Colab{: .btn }\nNevergrad library - Open In Colab{: .btn }\nOptuna quickstart Open In Colab",
    "crumbs": [
      "Methods",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/methods/line_search/parabola.html",
    "href": "docs/methods/line_search/parabola.html",
    "title": "Successive parabolic interpolation",
    "section": "",
    "text": "Sampling 3 points of a function determines a unique parabola. Using this information we will go directly to its minimum. Suppose, we have 3 points x_1 &lt; x_2 &lt; x_3 such that line segment [x_1, x_3] contains minimum of a function f(x). Then, we need to solve the following system of equations:\n\nax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3\n\nNote that this system is linear since we need to solve it on a,b,c. The minimum of this parabola will be calculated as follows:\n\nu = -\\dfrac{b}{2a} = x_2 - \\dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\\right]}\n\nNote that if f_2 &lt; f_1, f_2 &lt; f_3, then u will lie in [x_1, x_3]",
    "crumbs": [
      "Methods",
      "Line search",
      "Successive parabolic interpolation"
    ]
  },
  {
    "objectID": "docs/methods/line_search/parabola.html#idea",
    "href": "docs/methods/line_search/parabola.html#idea",
    "title": "Successive parabolic interpolation",
    "section": "",
    "text": "Sampling 3 points of a function determines a unique parabola. Using this information we will go directly to its minimum. Suppose, we have 3 points x_1 &lt; x_2 &lt; x_3 such that line segment [x_1, x_3] contains minimum of a function f(x). Then, we need to solve the following system of equations:\n\nax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3\n\nNote that this system is linear since we need to solve it on a,b,c. The minimum of this parabola will be calculated as follows:\n\nu = -\\dfrac{b}{2a} = x_2 - \\dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\\right]}\n\nNote that if f_2 &lt; f_1, f_2 &lt; f_3, then u will lie in [x_1, x_3]",
    "crumbs": [
      "Methods",
      "Line search",
      "Successive parabolic interpolation"
    ]
  },
  {
    "objectID": "docs/methods/line_search/parabola.html#algorithm",
    "href": "docs/methods/line_search/parabola.html#algorithm",
    "title": "Successive parabolic interpolation",
    "section": "2 Algorithm",
    "text": "2 Algorithm\ndef parabola_search(f, x1, x2, x3, epsilon):\n    f1, f2, f3 = f(x1), f(x2), f(x3)\n    while x3 - x1 &gt; epsilon:\n        u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))\n        fu = f(u)\n\n        if x2 &lt;= u:\n            if f2 &lt;= fu:\n                x1, x2, x3 = x1, x2, u\n                f1, f2, f3 = f1, f2, fu\n            else:\n                x1, x2, x3 = x2, u, x3\n                f1, f2, f3 = f2, fu, f3\n        else:\n            if fu &lt;= f2:\n                x1, x2, x3 = x1, u, x2\n                f1, f2, f3 = f1, fu, f2\n            else:\n                x1, x2, x3 = u, x2, x3\n                f1, f2, f3 = fu, f2, f3\n    return (x1 + x3) / 2",
    "crumbs": [
      "Methods",
      "Line search",
      "Successive parabolic interpolation"
    ]
  },
  {
    "objectID": "docs/methods/line_search/parabola.html#bounds",
    "href": "docs/methods/line_search/parabola.html#bounds",
    "title": "Successive parabolic interpolation",
    "section": "3 Bounds",
    "text": "3 Bounds\nThe convergence of this method is superlinear, but local, which means, that you can take profit from using this method only near some neighbour of optimum.",
    "crumbs": [
      "Methods",
      "Line search",
      "Successive parabolic interpolation"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html",
    "href": "docs/methods/line_search/index.html",
    "title": "Line search",
    "section": "",
    "text": "Suppose, we have a problem of minimization of a function f(x): \\mathbb{R} \\to \\mathbb{R} of scalar variable:\n\nf(x) \\to \\min_{x \\in \\mathbb{R}}\n\nSometimes, we refer to a similar problem of finding the minimum on the line segment [a,b]:\n\nf(x) \\to \\min_{x \\in [a,b]}\n\nLine search is one of the simplest formal optimization problems. However, it is an important link in solving more complex tasks, so it is very important to solve it effectively. Let’s restrict the class of problems under consideration to the cases where f(x) is a unimodal function.\nFunction f(x) is called unimodal on [a, b], if there is x_* \\in [a, b], that f(x_1) &gt; f(x_2) \\;\\;\\; \\forall a \\le x_1 &lt; x_2 &lt; x_* and f(x_1) &lt; f(x_2) \\;\\;\\; \\forall x_* &lt; x_1 &lt; x_2 \\leq b\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html#problem",
    "href": "docs/methods/line_search/index.html#problem",
    "title": "Line search",
    "section": "",
    "text": "Suppose, we have a problem of minimization of a function f(x): \\mathbb{R} \\to \\mathbb{R} of scalar variable:\n\nf(x) \\to \\min_{x \\in \\mathbb{R}}\n\nSometimes, we refer to a similar problem of finding the minimum on the line segment [a,b]:\n\nf(x) \\to \\min_{x \\in [a,b]}\n\nLine search is one of the simplest formal optimization problems. However, it is an important link in solving more complex tasks, so it is very important to solve it effectively. Let’s restrict the class of problems under consideration to the cases where f(x) is a unimodal function.\nFunction f(x) is called unimodal on [a, b], if there is x_* \\in [a, b], that f(x_1) &gt; f(x_2) \\;\\;\\; \\forall a \\le x_1 &lt; x_2 &lt; x_* and f(x_1) &lt; f(x_2) \\;\\;\\; \\forall x_* &lt; x_1 &lt; x_2 \\leq b\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html#key-property-of-unimodal-functions",
    "href": "docs/methods/line_search/index.html#key-property-of-unimodal-functions",
    "title": "Line search",
    "section": "2 Key property of unimodal functions",
    "text": "2 Key property of unimodal functions\nLet f(x) be a unimodal function on [a, b]. Then if x_1 &lt; x_2 \\in [a, b]:\n\nif f(x_1) \\leq f(x_2) \\to x_* \\in [a, x_2]\nif f(x_1) \\geq f(x_2) \\to x_* \\in [x_1, b]",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html#code",
    "href": "docs/methods/line_search/index.html#code",
    "title": "Line search",
    "section": "3 Code",
    "text": "3 Code\nOpen In Colab\n\n\n\n\n\n\n\n\nBinary search\n\n\n\n\n\n\n\n\n\n\n\n\n\nGolden search\n\n\n\n\n\n\n\n\n\n\n\n\n\nInexact Line Search\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuccessive parabolic interpolation\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/index.html#references",
    "href": "docs/methods/line_search/index.html#references",
    "title": "Line search",
    "section": "4 References",
    "text": "4 References\n\nCMC seminars (ru)",
    "crumbs": [
      "Methods",
      "Line search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html",
    "href": "docs/methods/line_search/binary_search.html",
    "title": "Binary search",
    "section": "",
    "text": "We divide a segment into two equal parts and choose the one that contains the solution of the problem using the values of functions.",
    "crumbs": [
      "Methods",
      "Line search",
      "Binary search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html#idea",
    "href": "docs/methods/line_search/binary_search.html#idea",
    "title": "Binary search",
    "section": "",
    "text": "We divide a segment into two equal parts and choose the one that contains the solution of the problem using the values of functions.",
    "crumbs": [
      "Methods",
      "Line search",
      "Binary search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html#algorithm",
    "href": "docs/methods/line_search/binary_search.html#algorithm",
    "title": "Binary search",
    "section": "2 Algorithm",
    "text": "2 Algorithm\ndef binary_search(f, a, b, epsilon):\n    c = (a + b) / 2\n    while abs(b - a) &gt; epsilon:\n        y = (a + c) / 2.0\n        if f(y) &lt;= f(c):\n            b = c\n            c = y\n        else:\n            z = (b + c) / 2.0\n            if f(c) &lt;= f(z):\n                a = y\n                b = z\n            else:\n                a = c\n                c = z\n    return c\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Line search",
      "Binary search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html#bounds",
    "href": "docs/methods/line_search/binary_search.html#bounds",
    "title": "Binary search",
    "section": "3 Bounds",
    "text": "3 Bounds\nThe length of the line segment on k+1-th iteration:\n\n\\Delta_{k+1} = b_{k+1} - a_{k+1} = \\dfrac{1}{2^k}(b-a)\n\nFor unimodal functions, this holds if we select the middle of a segment as an output of the iteration x_{k+1}:\n\n|x_{k+1} - x_*| \\leq \\dfrac{\\Delta_{k+1}}{2} \\leq \\dfrac{1}{2^{k+1}}(b-a) \\leq (0.5)^{k+1} \\cdot (b-a)\n\nNote, that at each iteration we ask oracle no more, than 2 times, so the number of function evaluations is N = 2 \\cdot k, which implies:\n\n|x_{k+1} - x_*| \\leq (0.5)^{\\frac{N}{2}+1} \\cdot (b-a) \\leq  (0.707)^{N}  \\frac{b-a}{2}\n\nBy marking the right side of the last inequality for \\varepsilon, we get the number of method iterations needed to achieve \\varepsilon accuracy:\n\nK = \\left\\lceil \\log_2 \\dfrac{b-a}{\\varepsilon} - 1 \\right\\rceil",
    "crumbs": [
      "Methods",
      "Line search",
      "Binary search"
    ]
  },
  {
    "objectID": "docs/methods/fom/index.html",
    "href": "docs/methods/fom/index.html",
    "title": "First order methods",
    "section": "",
    "text": "Now we have only first order information from the oracle.",
    "crumbs": [
      "Methods",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/methods/fom/index.html#materials",
    "href": "docs/methods/fom/index.html#materials",
    "title": "First order methods",
    "section": "1 Materials",
    "text": "1 Materials\n\nVisualization of optimization algorithms.",
    "crumbs": [
      "Methods",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html",
    "href": "docs/methods/fom/SGD.html",
    "title": "Stochastic gradient descent",
    "section": "",
    "text": "Suppose, our target function is the sum of functions.\n\n\\min\\limits_{\\theta \\in \\mathbb{R}^{p}} g(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\theta)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost (calculation of gradient is linear in n).\nThus, we can switch from the full gradient calculation to its unbiased estimator:\n\n\\theta_{k+1} = \\theta_k - \\alpha_k\\nabla f_{i_k} (\\theta),\n\nwhere we randomly choose i_k index of point at each iteration uniformly:\n\n\\mathbb{E}[\\nabla f_{i_k} (\\theta)] = \\sum_{i=1}^n p(i_k=i) \\nabla f_i(\\theta) = \\dfrac{1}{n}\\sum_{i=1}^n \\nabla f_i(\\theta) = \\nabla g(\\theta)\n\nIterations could be n times cheaper! But convergence requires \\alpha_k \\to 0.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#summary",
    "href": "docs/methods/fom/SGD.html#summary",
    "title": "Stochastic gradient descent",
    "section": "",
    "text": "Suppose, our target function is the sum of functions.\n\n\\min\\limits_{\\theta \\in \\mathbb{R}^{p}} g(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\theta)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost (calculation of gradient is linear in n).\nThus, we can switch from the full gradient calculation to its unbiased estimator:\n\n\\theta_{k+1} = \\theta_k - \\alpha_k\\nabla f_{i_k} (\\theta),\n\nwhere we randomly choose i_k index of point at each iteration uniformly:\n\n\\mathbb{E}[\\nabla f_{i_k} (\\theta)] = \\sum_{i=1}^n p(i_k=i) \\nabla f_i(\\theta) = \\dfrac{1}{n}\\sum_{i=1}^n \\nabla f_i(\\theta) = \\nabla g(\\theta)\n\nIterations could be n times cheaper! But convergence requires \\alpha_k \\to 0.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#convergence",
    "href": "docs/methods/fom/SGD.html#convergence",
    "title": "Stochastic gradient descent",
    "section": "2 Convergence",
    "text": "2 Convergence\n\n2.1 General setup\nWe consider classic finite-sample average minimization:\n\n\\min_{x \\in \\mathbb{R}^p} f(x) = \\min_{x \\in \\mathbb{R}^p}\\frac{1}{n} \\sum_{i=1}^n f_i(x)\n\nLet us consider stochastic gradient descent assuming \\nabla f is Lipschitz:\n\n\\tag{SGD}\nx_{k+1} = x_k - \\alpha_k \\nabla f_{i_k}(x_k)\n\nLipschitz continiity implies:\n\nf(x_{k+1}) \\leq f(x_k) + \\langle \\nabla f(x_k), x_{k+1} - x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|^2\n\nusing (\\text{SGD}):\n\nf(x_{k+1}) \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2\n\nNow let’s take expectation with respect to i_k:\n\n\\mathbb{E}[f(x_{k+1})] \\leq \\mathbb{E}[f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2]\n\nUsing linearity of expectation:\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\mathbb{E}[\\nabla f_{i_k}(x_k)]\\rangle + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nSince uniform sampling implies unbiased estimate of gradient: \\mathbb{E}[\\nabla f_{i_k}(x_k)] = \\nabla f(x_k):\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\|\\nabla f(x_k)\\|^2 + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\n\n\n2.2 Polyak-Lojasiewicz conditions\n\n\\tag{PL}\n\\frac{1}{2}\\|\\nabla f(x)\\|_2^2 \\geq \\mu(f(x) - f^*), \\forall x \\in \\mathbb{R}^p\n\nThis inequality simply requires that the gradient grows faster than a quadratic function as we move away from the optimal function value. Note, that strong convexity implies \\text{PL}, but not vice versa. Using \\text{PL} we can write:\n\n\\mathbb{E}[f(x_{k+1})] - f^* \\leq (1 - 2\\alpha_k \\mu) [f(x_k) - f^*] + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nThis bound already indicates, that we have something like linear convergence if far from solution and gradients are similar, but no progress if close to solution or have high variance in gradients at the same time.\n\n\n2.3 Stochastic subgradient descent\n\n\\tag{SSD}\nx_{k+1} = x_k - \\alpha_k g_{i_k}\n\nfor some g_{i_k} \\in \\partial f_{i_k}(x_k).\nFor convex f we have\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 \\mathbb{E}[\\|g_{i_k}\\|^2]\n\nHere we can see, that step-size \\alpha_k controls how fast we move towards solution. And squared step-size \\alpha_k^2 controls how much variance moves us away. Usually, we bound \\mathbb{E}[\\|g_{i_k}\\|^2] by some constant B^2.\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 B^2\n\nIf we also have strong convexity:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu) \\|x_{k-1} - x^*\\|^2 + \\alpha_k^2 B^2\n\nAnd finally, with \\alpha_k = \\alpha &lt; \\frac{2}{\\mu}:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu)^{k} R^2 + \\frac{\\alpha B^2}{2\\mu},\n\nwhere R = \\|x_0- x^*\\|",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#bounds",
    "href": "docs/methods/fom/SGD.html#bounds",
    "title": "Stochastic gradient descent",
    "section": "3 Bounds",
    "text": "3 Bounds\n\n\n\n\n\n\n\n\nConditions\n\\Vert \\mathbb{E} [f(x_k)] - f(x^*)\\Vert \\leq\nType of convergence\n\n\n\n\nConvex, Lipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right)\nSublinear\n\n\n\\mu-Strongly convex, Lipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right)\nSublinear\n\n\nConvex, non-smooth\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right)\nSublinear\n\n\n\\mu-Strongly convex, non-smooth\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right)\nSublinear",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#code",
    "href": "docs/methods/fom/SGD.html#code",
    "title": "Stochastic gradient descent",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SGD.html#references",
    "href": "docs/methods/fom/SGD.html#references",
    "title": "Stochastic gradient descent",
    "section": "5 References",
    "text": "5 References\n\nLecture by Mark Schmidt @ University of British Columbia\nConvergence theorems on major cases of GD, SGD (projected version included)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html",
    "href": "docs/methods/fom/Projected_subgradient_descent.html",
    "title": "Projected subgradient descent",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#intuition",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#intuition",
    "title": "Projected subgradient descent",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#bounds-derivation",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#bounds-derivation",
    "title": "Projected subgradient descent",
    "section": "2 Bounds derivation",
    "text": "2 Bounds derivation\n\n2.1 Introduction\nВ этом разделе мы будем рассматривать работу в рамках какого-то выпуклого множества S \\in \\mathbb{R}^n, так, чтобы x_k \\in S. Запишем для начала соотношение для итераций:\n\n\\begin{align*}\n\\|x_{k+1} - x^*\\|^2 &= \\|(x_{k+1} - x_k) + (x_k - x^*)\\|^2 = \\\\\n&= \\|x_k - x_{k+1}\\|^2 + \\|x_k - x^*\\|^2 - 2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle \\\\\n2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle &=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2\n\\end{align*}\n\nЗаметим, что при работе на ограниченном множестве у нас появилась небольшая проблема: x_{k+1} может не лежать в бюджетном множестве. Сейчас мы увидим, почему это является проблемой для выписывания оценок на число итераций: если мы имеем неравенство, записанное ниже, то процесс получения оценок будет абсолютно совпадать с описанными выше процедурами (потому что в случае субградиентного метода x_k - x_{k+1} = \\alpha_k g_k).\n\n\\tag{Target}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle\n\nОднако, в нашем случае мы можем лишь получить (будет показано ниже) оценки следующего вида:\n\n\\tag{Forward Target}\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nЭто связано с тем, что x_{k+1} нам легче контролировать при построении условного метода, а значит, легче записать на него оценку. К сожалению, привычной телескопической (сворачивающейся) суммы при таком неравенстве не получится. Однако, если неравенство (Forward Target) выполняется, то из него следует следующее неравенство:\n\n\\begin{align*}\n\\tag{Forward Target Fix}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle &\\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\\\\n& - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nДля того, чтобы доказать его, запишем (Forward Target Fix):\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle + \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\n\\leq  \\\\ \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle + \\langle x_k - x_{k+1}, x_{k+1} - x_k \\rangle\n\\end{align*}\n\nПереписывая его еще раз, получаем:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\|x_{k} - x_{k+1}\\|^2 - \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle = \\\\\n&= \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2}\\left(\\|x_{k} - x_{k+1}\\|^2 + 2\\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\\right) \\leq \\\\\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2} \\left( - \\alpha_k^2 g_k^2\\right) = \\\\\n&= \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nИтак, пускай мы имеем неравенство (Forward Target) - напомню, что мы его пока не доказали. Теперь покажем, как с его помощью получить оценки на сходимость метода. Для этого запишем неравенство (Forward Target Fix):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k, x_k &- x^* \\rangle + \\|x_k - x_{k+1}\\|^2 - \\alpha_k^2 g_k^2 \\leq \\\\\n&\\leq 2\\langle x_k - x_{k+1}, x_k - x^* \\rangle \\\\\n&= \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2 \\\\\n&\\quad \\\\\n2 \\langle \\alpha_k g_k, x_k - x^* \\rangle\n&\\leq \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k^2\n\\end{align*}\n\nЕсли внимательно посмотреть на полученный результат, то это в точности совпадает с исходной точкой доказательства для субградиентного метода в безусловном сеттинге.\nМожем сразу получить оценки:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T} \\\\\nf(\\overline{x}) - f^* &\\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nТаким образом, мы показали, что для метода проекции субградиента справедлива точно такая же оценка на число итераций, если выполняется неравенство (Forward Target) :) Давайте разбираться с ним\nНам следует доказать, что:\n\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nВ более общем случае \\forall y \\in S:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle & \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nВспомним из неравенства для проекции (равно как и условия оптимальности первого порядка), что \\forall y \\in S для некоторой гладкой выпуклой минимизируемой функции g(x) в точке оптимума x \\in S:\n\n\\langle \\nabla g(x), x - y \\rangle \\leq 0\n\nВ противном бы случае, можно было бы сделать градиентный шаг в направлении y -x и уменьшить значение функции.\nРассмотрим теперь следующую функцию g(x):\n\ng(x) = \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2, \\quad \\nabla g(x) = \\alpha_k g_k + x - x_k\n\nИ давайте теперь строить условный алгоритм как минимизацию этой функции:\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nТогда из условия оптимальности:\n\n\\begin{align*}\n\\langle \\nabla g(x_{k+1}), x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k + x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k , x_{k+1} - y \\rangle  + \\langle x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nПолученное неравенство в точности совпадает с неравенством (Forward Target), которое нам как раз таки и следовало доказать. Таким образом, мы получаем\n\n\n2.2 Algorithm\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nИнтересные фишки:\n\nТакая же скорость сходимости, как и для безусловного алгоритма. (Однако, стоимость каждой итерации может быть существенно больше из за необходимости решать задачу оптимизации на каждом шаге)\nВ частном случае S = \\mathbb{R}^n в точности совпадает с безусловным алгоритмом (убедитесь)\n\n\n2.2.1 Adaptive stepsize (without T)\nРазберем теперь одну из стратегий того, как избежать знания количества шагов T заранее для подбора длины шага \\alpha_k. Для этого зададим “диаметр” нашего множества D:\n\nD : \\{ \\max\\limits_{x,y \\in S} \\|x - y\\| \\leq D \\}\n\nТеперь зададим длину шага на k- ой итерации, как: \\alpha_k = \\tau \\sqrt{\\dfrac{1}{k+1}}. Константу \\tau \\geq 0 подберем чуть позже.\nДля начала легко заметить, что:\n\n\\begin{align*}\n\\sum\\limits_{k=0}^{T-1} \\alpha_k &= \\tau \\sum\\limits_{k=0}^{T-1} \\dfrac{1}{\\sqrt{k+1}} = \\tau \\left( 1 + \\sum\\limits_{k=1}^{T-1} \\dfrac{1}{\\sqrt{k+1}}\\right) \\leq \\\\\n&\\leq \\tau \\left(1 + \\int\\limits_{0}^{T-1} \\dfrac{1}{\\sqrt{x+1}} dx \\right) = \\tau (2\\sqrt{T}-1)\n\\end{align*}\n\nсм. геометрический смысл неравенства ниже:\n\n\n\nIllustration\n\n\nВозьмем теперь равенство для классического субградиентного метода (БМ) (или неравенство в случае метода проекции субгадиента (УМ)):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k ,x_k - x^*\\rangle\n&=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k ^2 \\\\\n\\sum\\limits_{k=0}^{T-1} \\langle g_k ,x_k - x^*\\rangle\n&= \\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{\\|x_k - x^*\\|^2}{2 \\alpha_k} - \\dfrac{\\|x_{k+1} - x^*\\|^2}{2 \\alpha_k} + \\dfrac{\\alpha_k}{2}g_k^2 \\right) \\\\\n&\\leq \\dfrac{\\|x_0 - x^*\\|^2}{2 \\alpha_0} - \\dfrac{\\|x_T - x^*\\|^2}{2 \\alpha_{T-1}} + \\\\\n&+ \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\|x_k - x^*\\|^2 + \\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2}g_k^2 \\leq \\\\\n& \\leq D^2 \\left( \\dfrac{1}{2 \\alpha_0} + \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\right) + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n& \\leq \\dfrac{D^2}{2 \\alpha_{T-1}} + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n&\\leq \\dfrac{1}{2} \\left( \\dfrac{D^2}{\\tau}\\sqrt{T} + \\tau G^2 \\left(2\\sqrt{T} - 1\\right)\\right) \\leq \\\\\n& \\leq DG \\sqrt{2T}\n\\end{align*}\n\nГде \\tau = \\dfrac{D}{G\\sqrt{2}} - выбран путем минимизации данной оценки по \\tau.\nТаким образом, мы получили, что в случае, когда количество шагов T неизвестно заранее (весьма важное свойство), оценка ухудшается в \\sqrt{2} раз. Такие оценки называют anytime bounds.\n\n\n2.2.2 Online learning:\nPSD - Projected Subgradient Descent\n\n\\begin{align*}\n\\tag{anytime PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{2T} \\\\\n\\tag{PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{T}\n\\end{align*}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#examples",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#examples",
    "title": "Projected subgradient descent",
    "section": "3 Examples",
    "text": "3 Examples\n\n3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R^n}} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\n\n3.1.1 Nonnegativity\n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\geq 0 \\}\n\n\n\n3.1.2 l_2 - ball\n\nS = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}\n\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\n\n\n3.1.3 Linear equality constraints\n\nS = \\{x \\in \\mathbb{R}^n \\mid Ax = b \\}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#bounds",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#bounds",
    "title": "Projected subgradient descent",
    "section": "4 Bounds",
    "text": "4 Bounds\n\n\n\n\n\n\n\n\n\nConditions\nConvergence rate\nIteration complexity\nType of convergence\n\n\n\n\nConvexLipschitz-continious function(G)\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right)\n\\mathcal{O}\\left(\\dfrac{1}{\\varepsilon^2} \\right)\nSublinear\n\n\nStrongly convexLipschitz-continious function(G)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right)\n\\mathcal{O}\\left(\\dfrac{1}{\\varepsilon} \\right)\nSublinear",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#references",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#references",
    "title": "Projected subgradient descent",
    "section": "5 References",
    "text": "5 References\n\nComprehensive presentation on projected subgradient method.\nGreat cheatsheet by Sebastian Pokutta\nLecture on subgradient methods @ Berkley",
    "crumbs": [
      "Methods",
      "First order methods",
      "Projected subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html",
    "href": "docs/methods/fom/Lookahead.html",
    "title": "Lookahead Optimizer: k steps forward, 1 step back",
    "section": "",
    "text": "The lookahead method provides an interesting way to accelerate and stabilize algorithms of stochastic gradient descent family. The main idea is quite simple:\n\nSet some number k. Take initial parameter weights x_0 = \\hat{x}_0\nDo k steps with your favorite optimization algorithm: \\hat{x}_1, \\ldots, \\hat{x}_k\nTake some value between initial x_0 and \\hat{x}_k:\n\n  x_{t+1} = (1 - \\alpha)x_{t} + \\alpha\\hat{x_k}\n  \nUpdate \\hat{x_0} with the last output of the algorithm.\nRepeat profit\n\nAuthors introduced separation on the fast weights and slow weights, which naturally arise in the described procedure. The paper contains proof for optimal step-size of the quadratic loss function and provides understanding why this technique could reduce variance of {% include link.html title=“Stochastic gradient descent” %} in the noisy quadratic case. Moreover, this work compares the convergence rate in dependency of condition number of the squared system.\nIt is worth to say, that author claims significant improvement in practical huge scale settings (ImageNet, CIFAR10,CIFAR100)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Lookahead Optimizer: $k$ steps forward, $1$ step back"
    ]
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html#summary",
    "href": "docs/methods/fom/Lookahead.html#summary",
    "title": "Lookahead Optimizer: k steps forward, 1 step back",
    "section": "",
    "text": "The lookahead method provides an interesting way to accelerate and stabilize algorithms of stochastic gradient descent family. The main idea is quite simple:\n\nSet some number k. Take initial parameter weights x_0 = \\hat{x}_0\nDo k steps with your favorite optimization algorithm: \\hat{x}_1, \\ldots, \\hat{x}_k\nTake some value between initial x_0 and \\hat{x}_k:\n\n  x_{t+1} = (1 - \\alpha)x_{t} + \\alpha\\hat{x_k}\n  \nUpdate \\hat{x_0} with the last output of the algorithm.\nRepeat profit\n\nAuthors introduced separation on the fast weights and slow weights, which naturally arise in the described procedure. The paper contains proof for optimal step-size of the quadratic loss function and provides understanding why this technique could reduce variance of {% include link.html title=“Stochastic gradient descent” %} in the noisy quadratic case. Moreover, this work compares the convergence rate in dependency of condition number of the squared system.\nIt is worth to say, that author claims significant improvement in practical huge scale settings (ImageNet, CIFAR10,CIFAR100)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Lookahead Optimizer: $k$ steps forward, $1$ step back"
    ]
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html#pros",
    "href": "docs/methods/fom/Lookahead.html#pros",
    "title": "Lookahead Optimizer: k steps forward, 1 step back",
    "section": "2 Pros",
    "text": "2 Pros\n\nInteresting idea, costs almost nothing, why not to try?\nWorks with any SGD-like optimizer (SGD, Adam, RmsProp)\nAnalytical approach to quadratic case.\nWide set of empirical tests (Image classification, Neural Translation, LSTM training)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Lookahead Optimizer: $k$ steps forward, $1$ step back"
    ]
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html#cons",
    "href": "docs/methods/fom/Lookahead.html#cons",
    "title": "Lookahead Optimizer: k steps forward, 1 step back",
    "section": "3 Cons",
    "text": "3 Cons\n\nLack of test loss pictures, the majority of them obtained for the train loss/accuracy\nLack of pictures with different batch sizes\nDifficult to analyze the method analytically",
    "crumbs": [
      "Methods",
      "First order methods",
      "Lookahead Optimizer: $k$ steps forward, $1$ step back"
    ]
  },
  {
    "objectID": "docs/methods/fom/ADAM.html",
    "href": "docs/methods/fom/ADAM.html",
    "title": "ADAM: A Method for Stochastic Optimization",
    "section": "",
    "text": "0.1 Summary\nAdam is the stochastic first order optimization algorithm, that uses historical information about stochastic gradients and incorporates it in attempt to estimate second order moment of stochastic gradients.\n\n\\tag{ADAM}\n\\begin{align*}\nx_{k+1} &= x_k - \\alpha_k \\dfrac{\\widehat{m_k}}{\\sqrt{\\widehat{v_k}} + \\epsilon} \\\\\n\\tag{First moment estimation}\n\\widehat{m_k} &= \\dfrac{m_k}{1 - \\beta_1^k} \\\\\nm_k &= \\beta_1 m_{k-1} + (1 - \\beta_1) g_k \\\\\n\\tag{Second moment estimation}\n\\widehat{v_k} &= \\dfrac{v_k}{1 - \\beta_2^k} \\\\\nv_k &= \\beta_2 v_{k-1} + (1 - \\beta_2)g_k^2 \\\\\n\\end{align*}\n\nAll vector operations are element-wise. \\alpha = 0.001, \\beta_1 = 0.9, \\beta_2 = 0.999 - the default values for hyperparameters (\\epsilon here is needed for avoiding zero division problems) and g_k = \\nabla f(x_k, \\xi_k) is the sample of stochastic gradient.\n\nWe can consider this approach as normalization of each parameter by using individual learning rates on $ (0,1)$, since \\mathbb{E}\\_{\\xi_k}[g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{m_k}] and \\mathbb{E}\\_{\\xi_k}[g_k \\odot g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{v_k}].\nThere are some issues with Adam effectiveness and some works, stated, that adaptive metrics methods could lead to worse generalization.\nThe name came from “Adaptive Moment estimation”.\n\n\n\n0.2 Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert \\mathbb{E} [f(x_k)] - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert \\mathbb{E}[x_k] - x^* \\Vert \\leq\n\n\n\n\nConvex\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right)\nSublinear\n\n\n\n\nVersion of Adam for a strongly convex functions is considered in this work. The obtained rate is \\mathcal{O}\\left(\\dfrac{\\log k}{\\sqrt{k}} \\right), while the version for truly linear rate remains undiscovered.",
    "crumbs": [
      "Methods",
      "First order methods",
      "ADAM: A Method for Stochastic Optimization"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "title": "Quasi Newton methods",
    "section": "",
    "text": "For the classic task of unconditional optimization f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} the general scheme of iteration method is written as:\n\nx_{k+1} = x_k + \\alpha_k s_k\n\nIn the Newton method, the s_k direction (Newton’s direction) is set by the linear system solution at each step:\n\ns_k = - B_k\\nabla f(x_k), \\;\\;\\; B_k = f_{xx}^{-1}(x_k)\n\ni.e. at each iteration it is necessary to compensate hessian and gradient and resolve linear system.\nNote here that if we take a single matrix of B_k = I_n as B_k at each step, we will exactly get the gradient descent method.\nThe general scheme of quasi-Newton methods is based on the selection of the B_k matrix so that it tends in some sense at k \\to \\infty to the true value of inverted Hessian in the local optimum f_{xx}^{-1}(x_*). Let’s consider several schemes using iterative updating of B_k matrix in the following way:\n\nB_{k+1} = B_k + \\Delta B_k\n\nThen if we use Taylor’s approximation for the first order gradient, we get it:\n\n\\nabla f(x_k) - \\nabla f(x_{k+1}) \\approx f_{xx}(x_{k+1}) (x_k - x_{k+1}).\n\nNow let’s formulate our method as:\n\n\\Delta x_k = B_{k+1} \\Delta y_k, \\text{ where } \\;\\; \\Delta y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\n\nin case you set the task of finding an update \\Delta B_k:\n\n\\Delta B_k \\Delta y_k = \\Delta x_k - B_k \\Delta y_k",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html#intuition",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html#intuition",
    "title": "Quasi Newton methods",
    "section": "",
    "text": "For the classic task of unconditional optimization f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} the general scheme of iteration method is written as:\n\nx_{k+1} = x_k + \\alpha_k s_k\n\nIn the Newton method, the s_k direction (Newton’s direction) is set by the linear system solution at each step:\n\ns_k = - B_k\\nabla f(x_k), \\;\\;\\; B_k = f_{xx}^{-1}(x_k)\n\ni.e. at each iteration it is necessary to compensate hessian and gradient and resolve linear system.\nNote here that if we take a single matrix of B_k = I_n as B_k at each step, we will exactly get the gradient descent method.\nThe general scheme of quasi-Newton methods is based on the selection of the B_k matrix so that it tends in some sense at k \\to \\infty to the true value of inverted Hessian in the local optimum f_{xx}^{-1}(x_*). Let’s consider several schemes using iterative updating of B_k matrix in the following way:\n\nB_{k+1} = B_k + \\Delta B_k\n\nThen if we use Taylor’s approximation for the first order gradient, we get it:\n\n\\nabla f(x_k) - \\nabla f(x_{k+1}) \\approx f_{xx}(x_{k+1}) (x_k - x_{k+1}).\n\nNow let’s formulate our method as:\n\n\\Delta x_k = B_{k+1} \\Delta y_k, \\text{ where } \\;\\; \\Delta y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\n\nin case you set the task of finding an update \\Delta B_k:\n\n\\Delta B_k \\Delta y_k = \\Delta x_k - B_k \\Delta y_k",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html#broyden-method",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html#broyden-method",
    "title": "Quasi Newton methods",
    "section": "2 Broyden method",
    "text": "2 Broyden method\nThe simplest option is when the amendment \\Delta B_k has a rank equal to one. Then you can look for an amendment in the form\n\n\\Delta B_k = \\mu_k q_k q_k^\\top.\n\nwhere \\mu_k is a scalar and q_k is a non-zero vector. Then mark the right side of the equation to find \\Delta B_k for \\Delta z_k:\n\n\\Delta z_k = \\Delta x_k - B_k \\Delta y_k\n\nWe get it:\n\n\\mu_k q_k q_k^\\top \\Delta y_k = \\Delta z_k\n\n\n\\left(\\mu_k \\cdot q_k^\\top \\Delta y_k\\right) q_k = \\Delta z_k\n\nA possible solution is: q_k = \\Delta z_k, \\mu_k = \\left(q_k^\\top \\Delta y_k\\right)^{-1}.\nThen an iterative amendment to Hessian’s evaluation at each iteration:\n\n\\Delta B_k = \\dfrac{(\\Delta x_k - B_k \\Delta y_k)(\\Delta x_k - B_k \\Delta y_k)^\\top}{\\langle \\Delta x_k - B_k \\Delta y_k , \\Delta y_k\\rangle}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html#davidonfletcherpowell-method",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html#davidonfletcherpowell-method",
    "title": "Quasi Newton methods",
    "section": "3 Davidon–Fletcher–Powell method",
    "text": "3 Davidon–Fletcher–Powell method\n\n\\Delta B_k = \\mu_1 \\Delta x_k (\\Delta x_k)^\\top + \\mu_2 B_k \\Delta y_k (B_k \\Delta y_k)^\\top.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html#broydenfletchergoldfarbshanno-method",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html#broydenfletchergoldfarbshanno-method",
    "title": "Quasi Newton methods",
    "section": "4 Broyden–Fletcher–Goldfarb–Shanno method",
    "text": "4 Broyden–Fletcher–Goldfarb–Shanno method\n\n\\Delta B_k = Q U Q^\\top, \\quad Q = [q_1, q_2], \\quad q_1, q_2 \\in \\mathbb{R}^n, \\quad U = \\begin{pmatrix} a & c\\\\ c & b \\end{pmatrix}.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle} + p_k p_k^\\top.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html#code",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html#code",
    "title": "Quasi Newton methods",
    "section": "5 Code",
    "text": "5 Code\n\nOpen In Colab\nComparison of quasi Newton methods",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Quasi Newton methods"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "title": "Natural gradient descent",
    "section": "",
    "text": "Let’s consider illustrative example of a simple function of 2 variables:\n\nf(x_1, x_2) = 2x_1 + \\frac{1}{3}x_2, \\quad \\nabla_x f = \\begin{pmatrix} 2\\\\ \\frac{1}{3} \\end{pmatrix}\n\nNow, let’s introduce new variables $(y_1, y_2) = (2x_1, x_2) $ or y = Bx, where B = \\begin{pmatrix} 2 & 0\\\\ 0 & \\frac{1}{3} \\end{pmatrix}. The same function, written in the new coordinates, is\n\nf(y_1, y_2) = y_1 + y_2, \\quad \\nabla_y f = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}\n\nLet’s summarize what happened:\n\nWe have a transformation of a vector space described by a coordinate transformation matrix B.\nCoordinate vectors transforms as y = Bx.\nHowever, the partial gradient of a function w.r.t. the coordinates transforms as \\frac{\\partial f}{\\partial y} = B^{-\\top} \\frac{\\partial f}{\\partial x}.\nTherefore, there seems to exist one type of mathematical objects (e.g. coordinate vectors) which transform with B, and a second type of mathematical objects (e.g. the partial gradient of a function w.r.t. coordinates) which transform with B^{-\\top}.\n\nThese two types are called contra-variant and co-variant. This should at least tell us that indeed the so-called “gradient-vector” is somewhat different to a “normal vector”: it behaves inversely under coordinate transformations.\nNice thing here is that steepest descent direction A_x^{-1}\\nabla_x f on a sphere transforms as a covariant vector, since A_y = B^{-\\top} A_x B^{-1}:\n\n\\begin{split}\nA_y^{-1}\\nabla_y f = \\\\\n(B^{-\\top} A_x B^{-1})^{-1} B^{-\\top} \\nabla_x f = \\\\\nB A_x^{-1} B^\\top B^{-\\top} \\nabla_x f = \\\\\nB (A_x^{-1} \\nabla_x f)\n\\end{split}",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html#intuition",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html#intuition",
    "title": "Natural gradient descent",
    "section": "",
    "text": "Let’s consider illustrative example of a simple function of 2 variables:\n\nf(x_1, x_2) = 2x_1 + \\frac{1}{3}x_2, \\quad \\nabla_x f = \\begin{pmatrix} 2\\\\ \\frac{1}{3} \\end{pmatrix}\n\nNow, let’s introduce new variables $(y_1, y_2) = (2x_1, x_2) $ or y = Bx, where B = \\begin{pmatrix} 2 & 0\\\\ 0 & \\frac{1}{3} \\end{pmatrix}. The same function, written in the new coordinates, is\n\nf(y_1, y_2) = y_1 + y_2, \\quad \\nabla_y f = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}\n\nLet’s summarize what happened:\n\nWe have a transformation of a vector space described by a coordinate transformation matrix B.\nCoordinate vectors transforms as y = Bx.\nHowever, the partial gradient of a function w.r.t. the coordinates transforms as \\frac{\\partial f}{\\partial y} = B^{-\\top} \\frac{\\partial f}{\\partial x}.\nTherefore, there seems to exist one type of mathematical objects (e.g. coordinate vectors) which transform with B, and a second type of mathematical objects (e.g. the partial gradient of a function w.r.t. coordinates) which transform with B^{-\\top}.\n\nThese two types are called contra-variant and co-variant. This should at least tell us that indeed the so-called “gradient-vector” is somewhat different to a “normal vector”: it behaves inversely under coordinate transformations.\nNice thing here is that steepest descent direction A_x^{-1}\\nabla_x f on a sphere transforms as a covariant vector, since A_y = B^{-\\top} A_x B^{-1}:\n\n\\begin{split}\nA_y^{-1}\\nabla_y f = \\\\\n(B^{-\\top} A_x B^{-1})^{-1} B^{-\\top} \\nabla_x f = \\\\\nB A_x^{-1} B^\\top B^{-\\top} \\nabla_x f = \\\\\nB (A_x^{-1} \\nabla_x f)\n\\end{split}",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html#steepest-descent-in-distribution-space",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html#steepest-descent-in-distribution-space",
    "title": "Natural gradient descent",
    "section": "2 Steepest descent in distribution space",
    "text": "2 Steepest descent in distribution space\nSuppose, we have a probabilistic model represented by its likelihood $p(x ) $. We want to maximize this likelihood function to find the most likely parameter \\theta with given observations. Equivalent formulation would be to minimize the loss function \\mathcal{L}(\\theta), which is the negative logarithm of likelihood function.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html#example",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html#example",
    "title": "Natural gradient descent",
    "section": "3 Example",
    "text": "3 Example",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html#references",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html#references",
    "title": "Natural gradient descent",
    "section": "4 References",
    "text": "4 References\n\nSome notes on gradient descent\nNatural Gradient Descent",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html#code",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html#code",
    "title": "Natural gradient descent",
    "section": "5 Code",
    "text": "5 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Natural gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html",
    "href": "docs/methods/Simplex.html",
    "title": "Linear Programming and simplex algorithm",
    "section": "",
    "text": "Generally speaking, all problems with linear objective and linear equalities/inequalities constraints could be considered as Linear Programming. However, there are some widely accepted formulations.\n\n\\tag{LP.Basic}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n\\end{align*}\n\n\n\n\nIllustration\n\n\nfor some vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}. Where the inequalities are interpreted component-wise.\nStandard form. This form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#what-is-linear-programming",
    "href": "docs/methods/Simplex.html#what-is-linear-programming",
    "title": "Linear Programming and simplex algorithm",
    "section": "",
    "text": "Generally speaking, all problems with linear objective and linear equalities/inequalities constraints could be considered as Linear Programming. However, there are some widely accepted formulations.\n\n\\tag{LP.Basic}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n\\end{align*}\n\n\n\n\nIllustration\n\n\nfor some vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}. Where the inequalities are interpreted component-wise.\nStandard form. This form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#examples-of-lp-problems",
    "href": "docs/methods/Simplex.html#examples-of-lp-problems",
    "title": "Linear Programming and simplex algorithm",
    "section": "2 Examples of LP problems",
    "text": "2 Examples of LP problems\n\n\n\n\n\n\nDiet problem\n\n\n\n\n\nImagine, that you have to construct a diet plan from some set of products: 🍌🍰🍗🥚🐟. Each of the products has its own vector of nutrients. Thus, all the food information could be processed through the matrix W. Let also assume, that we have the vector of requirements for each of nutrients r \\in \\mathbb{R}^n. We need to find the cheapest configuration of the diet, which meets all the requirements:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^p} c^{\\top}x \\\\\n\\text{s.t. } & Wx \\geq r\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\nIllustration\n\n\n\n\n\n\n\n\n\n\n\n\nTransportation problem\n\n\n\n\n\nSource\nThe prototypical transportation problem deals with the distribution of a commodity from a set of sources to a set of destinations. The object is to minimize total transportation costs while satisfying constraints on the supplies available at each of the sources, and satisfying demand requirements at each of the destinations.\nHere we illustrate the transportation problem using an example from Chapter 5 of Johannes Bisschop, “AIMMS Optimization Modeling”, Paragon Decision Sciences, 1999. In this example there are two factories and six customer sites located in 8 European cities as shown in the following map. The customer sites are labeled in red, the factories are labeled in blue.\n\n\n\nWest Europe Map\n\n\n\n\n\n\n\n\n\n\n\nCustomer\nArnhem [€/ton]\nGouda [€/ton]\nDemand [tons]\n\n\n\n\nLondon\nn/a\n2.5\n125\n\n\nBerlin\n2.5\nn/a\n175\n\n\nMaastricht\n1.6\n2.0\n225\n\n\nAmsterdam\n1.4\n1.0\n250\n\n\nUtrecht\n0.8\n1.0\n225\n\n\nThe Hague\n1.4\n0.8\n200\n\n\nSupply [tons]\n550 tons\n700 tons\n\n\n\n\nThis can be represented as the following graph:\n\n\n\nGraph associated with the problem\n\n\nFor each link we can have a parameter T[c,s] denoting the cost of shipping a ton of goods over the link. What we need to determine is the amount of goods to be shipped over each link, which we will represent as a non-negative decision variable x[c,s].\nThe problem objective is to minimize the total shipping cost to all customers from all sources.\n\\text{minimize:}\\quad \\text{Cost} = \\sum_{c \\in Customers}\\sum_{s \\in Sources} T[c,s] x[c,s]\nShipments from all sources can not exceed the manufacturing capacity of the source.\n\\sum_{c \\in Customers} x[c,s] \\leq \\text{Supply}[s] \\qquad \\forall s \\in Sources\nShipments to each customer must satisfy their demand.\n\\sum_{s\\in Sources} x[c,s] = \\text{Demand}[c] \\qquad \\forall c \\in Customers\nThe code for the problem is available here: 💻\n\n\n\n\n\n\n\n\n\n\nBlending problem\n\n\n\n\n\nSource\nA brewery receives an order for 100 gallons of 4% ABV (alchohol by volume) beer. The brewery has on hand beer Okhota that is 4.5% ABV that cost USD 0.32 per gallon to make, and beer Baltos that is 3.7% ABV and cost USD 0.25 per gallon. Water could also be used as a blending agent at a cost of USD 0.05 per gallon. Find the minimum cost blend that meets the customer requirements.\n\n2.0.1 Model Formulation\n\n2.0.1.1 Objective Function\nIf we let subscript c denote a blending component from the set of blending components C, and denote the volume of c used in the blend as x_c, the cost of the blend is\n\n\\text{cost}  = \\sum_{c\\in C} x_c P_c\n\nwhere P_c is the price per unit volume of c. Using the Python data dictionary defined above, the price P_c is given by data[c]['cost'].\n\n\n2.0.1.2 Volume Constraint\nThe customer requirement is produce a total volume V. Assuming ideal solutions, the constraint is given by\n\nV  = \\sum_{c\\in C} x_c\n\nwhere x_c denotes the volume of component c used in the blend.\n\n\n2.0.1.3 Product Composition Constraint\nThe product composition is specified as 4% alchohol by volume. Denoting this as \\bar{A}, the constraint may be written as\n\n\\bar{A} = \\frac{\\sum_{c\\in C}x_c A_c}{\\sum_{c\\in C} x_c}\n\nwhere A_c is the alcohol by volume for component c. As written, this is a nonlinear constraint. Multiplying both sides of the equation by the denominator yields a linear constraint\n\n\\bar{A}\\sum_{c\\in C} x_c  = \\sum_{c\\in C}x_c A_c\n\nA final form for this constraint can be given in either of two versions. In the first version we subtract the left-hand side from the right to give\n\n0  = \\sum_{c\\in C}x_c \\left(A_c - \\bar{A}\\right)  \\text{ Version 1 of the linear blending constraint}\n\nAlternatively, the summation on the left-hand side corresponds to total volume. Since that is known as part of the problem specification, the blending constraint could also be written as\n\n\\bar{A}V  = \\sum_{c\\in C}x_c A_c   \\text{ Version 2 of the linear blending constraint}\n\nWhich should you use? Either will generally work well. The advantage of version 1 is that it is fully specified by a product requirement \\bar{A}, which is sometimes helpful in writing elegant Python code.\nThe code for the problem is available here: 💻",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#basic-transformations",
    "href": "docs/methods/Simplex.html#basic-transformations",
    "title": "Linear Programming and simplex algorithm",
    "section": "3 Basic transformations",
    "text": "3 Basic transformations\nInequality to equality by increasing the dimension of the problem by m.\n\nAx \\leq b \\leftrightarrow\n\\begin{cases}\nAx + z =  b\\\\\nz \\geq 0\n\\end{cases}\n\nunsigned variables to nonnegative variables.\n\nx \\leftrightarrow\n\\begin{cases}\nx = x_+ - x_-\\\\\nx_+ \\geq 0 \\\\\nx_- \\geq 0\n\\end{cases}\n\n\n\n\n\n\n\nChebyshev approximation problem\n\n\n\n\n\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_\\infty \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\max_{i} |a_i^\\top x - b_i|\n\nCould be equivalently written as a LP with thre replacement of maximum coordinate of a vector:\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}, x \\in \\mathbb{R}^n} t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\n\nl_1 approximation problem\n\n\n\n\n\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_1 \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^n |a_i^\\top x - b_i|\n\nCould be equivalently written as a LP with thre replacement of the sum of coordinates of a vector:\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}^n, x \\in \\mathbb{R}^n} \\mathbf{1}^\\top t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t_i, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t_i, \\; i = 1,\\dots, n\n\\end{align*}",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#duality",
    "href": "docs/methods/Simplex.html#duality",
    "title": "Linear Programming and simplex algorithm",
    "section": "4 Duality",
    "text": "4 Duality\nThere are four possibilities:\n\nBoth the primal and the dual are infeasible.\nThe primal is infeasible and the dual is unbounded.\nThe primal is unbounded and the dual is infeasible.\nBoth the primal and the dual are feasible and their optimal values are equal.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nEnsure, that the following standard form LP:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\nHas the following dual:\n\n\\begin{align*}\n&\\max_{y \\in \\mathbb{R}^n} b^{\\top}y \\\\\n\\text{s.t. } & A^Ty \\preceq c\\\\\n\\end{align*}\n\nFind the dual problem to the problem above (it should be the original LP).",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#idea-of-simplex-algorithm",
    "href": "docs/methods/Simplex.html#idea-of-simplex-algorithm",
    "title": "Linear Programming and simplex algorithm",
    "section": "5 Idea of simplex algorithm",
    "text": "5 Idea of simplex algorithm\n\nThe Simplex Algorithm walks along the edges of the polytope, at every corner choosing the edge that decreases c^\\top x most\nThis either terminates at a corner, or leads to an unconstrained edge (-\\infty optimum)\n\nWe will illustrate simplex algorithm for the simple inequality form of LP:\n\n\\tag{LP.Inequality}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\n\\end{align*}\n\nDefinition: a basis B is a subset of n (integer) numbers between 1 and m, so that \\text{rank} A_B = n. Note, that we can associate submatrix A_B and corresponding right-hand side b_B with the basis B. Also, we can derive a point of intersection of all these hyperplanes from basis: x_B = A^{-1}_B b_B.\nIf A x_B \\leq b, then basis B is feasible.\nA basis B is optimal if x_B is an optimum of the \\text{LP.Inequality}.\n\n\n\nIllustration\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nIf Standartd LP has a nonempty feasible region, then there is at least one basic feasible point\nIf Standartd LP has solutions, then at least one such solution is a basic optimal point.\nIf Standartd LP is feasible and bounded, then it has an optimal solution.\n\n\n\n\n\nSince we have a basis, we can decompose our objective vector c in this basis and find the scalar coefficients \\lambda_B:\n\n\\lambda^\\top_B A_B = c^\\top \\leftrightarrow \\lambda^\\top_B = c^\\top A_B^{-1}\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nIf all components of \\lambda_B are non-positive and B is feasible, then B is optimal.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\\begin{align*}\n\\exists x^*: Ax^* &\\leq b, c^\\top x^* &lt; c^\\top x_B \\\\\nA_B x^* &\\leq b_B \\\\\n\\lambda_B^\\top A_B x^* &\\geq \\lambda_B^\\top b_B \\\\\nc^\\top x^* & \\geq \\lambda_B^\\top A_B x_B \\\\\nc^\\top x^* & \\geq c^\\top  x_B \\\\\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\n\n5.1 Changing basis\nSuppose, some of the coefficients of \\lambda_B are positive. Then we need to go through the edge of the polytope to the new vertex (i.e., switch the basis)\n\n\n\nIllustration\n\n\n\nx_{B'} = x_B + \\mu d = A^{-1}_{B'} b_{B'}\n\n\n\n5.2 Finding an initial basic feasible solution\nLet us consider \\text{LP.Canonical}.\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\nThe proposed algorithm requires an initial basic feasible solution and corresponding basis. To compute this solution and basis, we start by multiplying by −1 any row i of Ax = b such that b_i &lt; 0. This ensures that b \\geq 0. We then introduce artificial variables z \\in \\mathbb{R}^m and consider the following LP:\n\n\\tag{LP.Phase 1}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}^m} 1^{\\top}z \\\\\n\\text{s.t. } & Ax + Iz = b\\\\\n& x_i, z_j \\geq 0, \\; i = 1,\\dots, n \\; j = 1,\\dots, m\n\\end{align*}\n\nwhich can be written in canonical form \\min\\{\\tilde{c}^\\top \\tilde{x} \\mid \\tilde{A}\\tilde{x} = \\tilde{b}, \\tilde{x} \\geq 0\\} by setting\n\n\\tilde{x} = \\begin{bmatrix}x\\\\z\\end{bmatrix}, \\quad \\tilde{A} = [A \\; I], \\quad \\tilde{b} = b, \\quad \\tilde{c} = \\begin{bmatrix}0_n\\\\1_m\\end{bmatrix}\n\nAn initial basis for \\text{LP.Phase 1} is \\tilde{A}_B = I, \\tilde{A}_N = A with corresponding basic feasible solution \\tilde{x}_N = 0, \\tilde{x}_B = \\tilde{A}^{-1}_B \\tilde{b} = \\tilde{b} \\geq 0. We can therefore run the simplex method on \\text{LP.Phase 1}, which will converge to an optimum \\tilde{x}^*. \\tilde{x} = (\\tilde{x}_N \\; \\tilde{x}_B). There are several possible outcomes:\n\n\\tilde{c}^\\top \\tilde{x} &gt; 0. Original primal is infeasible.\n\\tilde{c}^\\top \\tilde{x} = 0 \\to 1^\\top z^* = 0. The obtained solution is a start point for the original problem (probably with slight modification).",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#convergence",
    "href": "docs/methods/Simplex.html#convergence",
    "title": "Linear Programming and simplex algorithm",
    "section": "6 Convergence",
    "text": "6 Convergence\nSince the number of edge point is finite, algorithm should converge (except some degenerate cases, which are not covered here). However, the convergence could be exponentially slow, due to the high number of edges. There is the following iconic example, when simplex algorithm should perform exactly all vertexes.\n\n6.1 Klee Minty example\nIn the following problem simplex algorithm needs to check 2^n - 1 vertexes with x_0 = 0.\n\n\\begin{align*} & \\max_{x \\in \\mathbb{R}^n} 2^{n-1}x_1 + 2^{n-2}x_2 + \\dots + 2x_{n-1} + x_n\\\\\n\\text{s.t. } & x_1 \\leq 5\\\\\n& 4x_1 + x_2 \\leq 25\\\\\n& 8x_1 + 4x_2 + x_3 \\leq 125\\\\\n& \\ldots\\\\\n& 2^n x_1 + 2^{n-1}x_2 + 2^{n-2}x_3 + \\ldots + x_n \\leq 5^n\\\\\n& x \\geq 0\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#summary",
    "href": "docs/methods/Simplex.html#summary",
    "title": "Linear Programming and simplex algorithm",
    "section": "7 Summary",
    "text": "7 Summary\n\nA wide variety of applications could be formulated as the linear programming.\nSimplex algorithm is simple, but could work exponentially long.\nKhachiyan’s ellipsoid method is the first to be proved running at polynomial complexity for LPs. However, it is usually slower than simplex in real problems.\nInterior point methods are the last word in this area. However, good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#code",
    "href": "docs/methods/Simplex.html#code",
    "title": "Linear Programming and simplex algorithm",
    "section": "8 Code",
    "text": "8 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/methods/Simplex.html#materials",
    "href": "docs/methods/Simplex.html#materials",
    "title": "Linear Programming and simplex algorithm",
    "section": "9 Materials",
    "text": "9 Materials\n\nLinear Programming. in V. Lempitsky optimization course.\nSimplex method. in V. Lempitsky optimization course.\nOverview of different LP solvers\nTED talks watching optimization\nOverview of ellipsoid method\nComprehensive overview of linear programming\nConverting LP to a standard form",
    "crumbs": [
      "Methods",
      "Linear Programming and simplex algorithm"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html",
    "href": "docs/materials/tutorials/Colab tutorial.html",
    "title": "Quick start to the Colab",
    "section": "",
    "text": "This tutorial will highlight some\nThis tool perfectly fits 90% of your tasks. * Fast prototyping * Sharing and simultaneous work",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#github-features",
    "href": "docs/materials/tutorials/Colab tutorial.html#github-features",
    "title": "Quick start to the Colab",
    "section": "1 Github features",
    "text": "1 Github features\n\n1.1 Running any jupyter notebook from github\nIf you want to open any notebook, that is already stored in any public github repository, it is enough to paste user/repo name in the field below and just open it. Modified notebook could be downloaded locally or saved to your google drive storage. \n\n\n1.2 Commiting to github from colab\nHowever, you can save any changes directly to the github through the commits. In order to do this, you’ll need to authorize colab to work with your github account. It’s up to you to provide this access or not. All these things work even with private repos.",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#using-different-accelerators-cpugputpu",
    "href": "docs/materials/tutorials/Colab tutorial.html#using-different-accelerators-cpugputpu",
    "title": "Quick start to the Colab",
    "section": "2 Using different accelerators (CPU/GPU/TPU)",
    "text": "2 Using different accelerators (CPU/GPU/TPU)\nRuntime -&gt; Change runtime type -&gt;",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#customization",
    "href": "docs/materials/tutorials/Colab tutorial.html#customization",
    "title": "Quick start to the Colab",
    "section": "3 Customization",
    "text": "3 Customization\n\n3.1 Dark theme\nTools -&gt; Settings -&gt; Site -&gt; Theme -&gt; Dark\n\n\n\n3.2 Monokai\nTools -&gt; Settings -&gt; Site -&gt; Editor -&gt; Editor colorization\n\n\n\n3.3 Bonus\nTools -&gt; Settings -&gt; Miscellaneous -&gt; Kitty mode",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#references",
    "href": "docs/materials/tutorials/Colab tutorial.html#references",
    "title": "Quick start to the Colab",
    "section": "4 References",
    "text": "4 References\n\nOfficial guide",
    "crumbs": [
      "Materials",
      "Tutorials",
      "Quick start to the Colab"
    ]
  },
  {
    "objectID": "docs/exercises/zom.html",
    "href": "docs/exercises/zom.html",
    "title": "Zero order methods",
    "section": "",
    "text": "Implement Rastrigin function f: \\mathbb{R}^d \\to \\mathbb{R} for d = 10. link\n\nf(\\mathbf{x})=10 d+\\sum_{i=1}^{d}\\left[x_{i}^{2}-10 \\cos \\left(2 \\pi x_{i}\\right)\\right]\n\n\nConsider global optimization from here.\nPlot 4 graphs for different d from {10, 100, 1000, 10000}. On each graph you are to plot f from N_{fev} for 5 methods: basinhopping, brute, differential_evolution, shgo, dual_annealing from scipy, where N_{fev} - the number of function evaluations. This information is usually available from specific_optimizer.nfev. If you will need bounds for the optimizer, use x_i \\in [-5, 5].\nNote, that it is crucial to fix seed and to use the same starting point for fair comparison.\n\nMachine learning models often have hyperparameters. To choose optimal one between them one can use GridSearch or RandomSearch. But these algorithms computationally uneffective and don’t use any sort of information about type of optimized function. To overcome this problem one can use bayesian optimization. Using this method we optimize our model by sequentially chosing points based on prior information about function.\n\n\n\nImage\n\n\nIn this task you will use optuna package for hyperparameter optimization RandomForestClassifier. Your task is to find best Random Forest model varying at least 3 hyperparameters on iris dataset. Examples can be find here or here\n!pip install optuna\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\nimport sklearn.svm\n\nimport optuna\n\niris = sklearn.datasets.load_iris()\nx, y = iris.data, iris.target\nTry to perform hyperparameter optimization in context of any metric for imbalanced classification problem with optuna and keras. Open In Colab{: .btn }\nLet’s arrange the base stations of the wireless network optimally! Suppose you have N_{obj} = 10 clusters of 10 subscribers each. Let us use a genetic algorithm to gradually search for the optimal number and location of base stations in order to minimize the cost of arranging such stations.\nBelow is one possible implementation of the genetic algorithm.\nPopulation\nThis is a list of arrays of size [N_stations x 2]. Each individual in this case is a set of station coordinates on the plane. Generation of a random\nMutation\nDefined by the function mutation(). A mutation_rate part is selected from all individuals and a random Gaussian noise is added to the mutation_rate part of its stations. An individual with a random number of stations with random coordinates is then added to the population.\nCrossing\nDefined by children_creation() and breed(). Two sets of stations are matched with a third station, from which the even stations of one parent and the odd stations of the other are taken.\nEstimation of the value of an individual\nDefined by evaluate_generation(). The total cost corresponding to a particular individual is made up of the cost of building base stations (each cost station_cost) minus the profit from each client. The profit from each client is inversely proportional to the distance to “his” base station. Each customer joins only one (closest) base station using find_nearest_station(). In addition, the profit from each subscriber is inversely proportional to the number of subscribers at a given base station (each station has the number of subscribers stations_load connected to it). Note also that, starting from a certain proximity to the subscriber to the base station, the client’s profit ceases to grow (in our algorithm, it is the same in the radius of 0.1 from the base station, then linearly decreases).\nYour task is to come up with any modifications to the proposed procedures within the genetic algorithm so that the final quality of the algorithm is better. Suggest, describe, and test ideas for improving the algorithm.\n%matplotlib notebook\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom random import shuffle, sample\nfrom copy import deepcopy\nimport random\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\ndef generate_problem(N_obj, N_abon_per_cluster):\n    abonents = np.zeros((N_obj*N_abon_per_cluster,2))\n    for i_obj in range(N_obj):\n        center = np.random.random(2)\n        cov    = np.random.random((2,2))*0.1\n        cov    = cov @ cov.T\n        xs, ys = np.random.multivariate_normal(center, cov, N_abon_per_cluster).T\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 0] = xs\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 1] = ys\n    return abonents\n\ndef plot_problem(abonents):\n    plt.figure(figsize=(10,6))\n    plt.plot(abonents[:,0], abonents[:,1], 'go')\n    plt.title('The village')\n#     plt.savefig('bs_problem.svg')\n    plt.show()\n\ndef random_solution(abonents, N_solutions = 100):\n    x_min, x_max = abonents[:,0].min(), abonents[:,0].max()\n    y_min, y_max = abonents[:,1].min(), abonents[:,1].max()\n    population = []\n\n    for i_sol in range(N_solutions):\n        N_stations = int(np.random.random(1)[0]*10)+1\n        stations = np.zeros((N_stations,2))\n        stations[:,0], stations[:,1] = np.random.random(N_stations)*(x_max - x_min), np.random.random(N_stations)*(y_max - y_min)\n        population.append(stations)\n    return population\n\ndef find_nearest_station(dist_matrix):\n    return np.argmin(dist_matrix, axis=1)\n\ndef pairwise_distance(abonents, stations):\n    return cdist(abonents, stations)\n\ndef evaluate_generation(abonents, population, station_cost = 1, abonent_profit_base = 1):  \n    costs = []\n    for creature in population:\n        N_stations, N_users = len(creature), len(abonents)\n        total_cost          = N_stations*station_cost\n        dist_matrix         = pairwise_distance(abonents, creature)\n        stations_assignment = find_nearest_station(dist_matrix)\n        stations_load       = np.ones(N_stations)\n        stations_load       = np.array([1/(sum(stations_assignment == i_st)+1) for i_st, st in enumerate(stations_load)])\n\n        for i_ab, abonent in enumerate(abonents):\n            dist_to_base = dist_matrix[i_ab, stations_assignment[i_ab]]\n            total_cost  -= stations_load[stations_assignment[i_ab]]*abonent_profit_base/(max(0.1, dist_to_base))\n\n        costs.append(total_cost)\n    return np.array(costs)\n\ndef mutation(population, mutation_rate = 0.3):\n    N_creatures = len(population)\n    x_min, x_max = -1, 1\n    y_min, y_max = -1, 1\n    mutated_creatures = sample(range(N_creatures), int(mutation_rate*N_creatures))\n    for i_mut in mutated_creatures:\n        N_stations = len(population[i_mut])\n        mutated_stations = sample(range(N_stations), int(mutation_rate*N_stations))\n        for i_st_mut in mutated_stations:\n            population[i_mut][i_st_mut] += np.random.normal(0, 0.01, 2)\n\n    N_new_stations = max(1, int(random.random()*mutation_rate*N_creatures))\n    for i in range(N_new_stations):\n        new_stations = np.zeros((N_new_stations,2))\n        new_stations[:,0], new_stations[:,1] = np.random.random(N_new_stations)*(x_max - x_min), np.random.random(N_new_stations)*(y_max - y_min)\n        population.append(new_stations)\n    return population\n\ndef children_creation(parent1, parent2):\n    # whoisbatya\n    batya = random.random() &gt; 0.5\n    if batya:\n        child = np.concatenate((parent1[::2], parent2[1::2]))\n    else:\n        child = np.concatenate((parent1[1::2], parent2[::2]))\n    return np.array(child)\n\ndef breed(population):\n    new_population = deepcopy(population)\n    random.shuffle(new_population)\n    N_creatures = len(population)\n    for i in range(N_creatures//2):\n        children = children_creation(population[i], population[i+1])\n        new_population.append(children)\n    return new_population\n\ndef selection(abonents, population, offsprings = 10):\n    scores = evaluate_generation(abonents, population)\n    best = np.array(scores).argsort()[:offsprings].tolist()\n    return [population[i_b] for i_b in best], population[best[0]] \n\n\ndef let_eat_bee(N_creatures, N_generations, N_obj = 10, N_abon_per_cluster = 10):\n    abonents = generate_problem(N_obj, N_abon_per_cluster)\n\n    costs_evolution = np.zeros((N_generations, N_creatures))\n    population = random_solution(abonents, N_creatures)\n    best_creatures = []\n    for generation in range(N_generations):\n        population                = mutation(population)\n        population                = breed(population)\n        population, best_creature = selection(abonents, population, N_creatures)\n        best_creatures.append(best_creature)\n\n        costs_evolution[generation, :] = evaluate_generation(abonents, population)\n\n        # Plotting\n        x_min, x_max = 0, 1\n        y_min, y_max = 0,1\n        cost_min  = [np.min(costs_evolution[i])  for i in range(generation)]\n        cost_max  = [np.max(costs_evolution[i])  for i in range(generation)]\n        cost_mean = [np.mean(costs_evolution[i]) for i in range(generation)]\n\n        fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Topology of the best solution\", \"Cost function\"))\n        fig.update_xaxes(title_text=\"x\", range = [x_min,x_max],  row=1, col=1)\n        fig.update_yaxes(title_text=\"y\", range = [y_min,y_max], row=1, col=1)\n        fig.update_yaxes(title_text=\"Total cost\", row=1, col=2)\n        fig.update_xaxes(title_text=\"Generation\", row=1, col=2)\n\n        fig.add_trace(\n            go.Scatter(x=abonents[:, 0], y=abonents[:, 1], mode='markers', name='abonents',  marker=dict(size=5)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x=best_creatures[generation][:, 0], y=best_creatures[generation][:, 1], mode='markers', name='stations', marker=dict(size=15)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_min, name='best'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_max, name='worst'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_mean, name='mean'),\n            row=1, col=2\n        )\n\n        clear_output(wait=True)\n        fig.show()\n\n    fig.write_html(\"test.html\")    \n    return costs_evolution, abonents, best_creatures\n\n\ncosts_evolution, abonents, best_creatures = let_eat_bee(200, 200)",
    "crumbs": [
      "Exercises",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/exercises/zom.html#zero-order-methods",
    "href": "docs/exercises/zom.html#zero-order-methods",
    "title": "Zero order methods",
    "section": "",
    "text": "Implement Rastrigin function f: \\mathbb{R}^d \\to \\mathbb{R} for d = 10. link\n\nf(\\mathbf{x})=10 d+\\sum_{i=1}^{d}\\left[x_{i}^{2}-10 \\cos \\left(2 \\pi x_{i}\\right)\\right]\n\n\nConsider global optimization from here.\nPlot 4 graphs for different d from {10, 100, 1000, 10000}. On each graph you are to plot f from N_{fev} for 5 methods: basinhopping, brute, differential_evolution, shgo, dual_annealing from scipy, where N_{fev} - the number of function evaluations. This information is usually available from specific_optimizer.nfev. If you will need bounds for the optimizer, use x_i \\in [-5, 5].\nNote, that it is crucial to fix seed and to use the same starting point for fair comparison.\n\nMachine learning models often have hyperparameters. To choose optimal one between them one can use GridSearch or RandomSearch. But these algorithms computationally uneffective and don’t use any sort of information about type of optimized function. To overcome this problem one can use bayesian optimization. Using this method we optimize our model by sequentially chosing points based on prior information about function.\n\n\n\nImage\n\n\nIn this task you will use optuna package for hyperparameter optimization RandomForestClassifier. Your task is to find best Random Forest model varying at least 3 hyperparameters on iris dataset. Examples can be find here or here\n!pip install optuna\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\nimport sklearn.svm\n\nimport optuna\n\niris = sklearn.datasets.load_iris()\nx, y = iris.data, iris.target\nTry to perform hyperparameter optimization in context of any metric for imbalanced classification problem with optuna and keras. Open In Colab{: .btn }\nLet’s arrange the base stations of the wireless network optimally! Suppose you have N_{obj} = 10 clusters of 10 subscribers each. Let us use a genetic algorithm to gradually search for the optimal number and location of base stations in order to minimize the cost of arranging such stations.\nBelow is one possible implementation of the genetic algorithm.\nPopulation\nThis is a list of arrays of size [N_stations x 2]. Each individual in this case is a set of station coordinates on the plane. Generation of a random\nMutation\nDefined by the function mutation(). A mutation_rate part is selected from all individuals and a random Gaussian noise is added to the mutation_rate part of its stations. An individual with a random number of stations with random coordinates is then added to the population.\nCrossing\nDefined by children_creation() and breed(). Two sets of stations are matched with a third station, from which the even stations of one parent and the odd stations of the other are taken.\nEstimation of the value of an individual\nDefined by evaluate_generation(). The total cost corresponding to a particular individual is made up of the cost of building base stations (each cost station_cost) minus the profit from each client. The profit from each client is inversely proportional to the distance to “his” base station. Each customer joins only one (closest) base station using find_nearest_station(). In addition, the profit from each subscriber is inversely proportional to the number of subscribers at a given base station (each station has the number of subscribers stations_load connected to it). Note also that, starting from a certain proximity to the subscriber to the base station, the client’s profit ceases to grow (in our algorithm, it is the same in the radius of 0.1 from the base station, then linearly decreases).\nYour task is to come up with any modifications to the proposed procedures within the genetic algorithm so that the final quality of the algorithm is better. Suggest, describe, and test ideas for improving the algorithm.\n%matplotlib notebook\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom random import shuffle, sample\nfrom copy import deepcopy\nimport random\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\ndef generate_problem(N_obj, N_abon_per_cluster):\n    abonents = np.zeros((N_obj*N_abon_per_cluster,2))\n    for i_obj in range(N_obj):\n        center = np.random.random(2)\n        cov    = np.random.random((2,2))*0.1\n        cov    = cov @ cov.T\n        xs, ys = np.random.multivariate_normal(center, cov, N_abon_per_cluster).T\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 0] = xs\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 1] = ys\n    return abonents\n\ndef plot_problem(abonents):\n    plt.figure(figsize=(10,6))\n    plt.plot(abonents[:,0], abonents[:,1], 'go')\n    plt.title('The village')\n#     plt.savefig('bs_problem.svg')\n    plt.show()\n\ndef random_solution(abonents, N_solutions = 100):\n    x_min, x_max = abonents[:,0].min(), abonents[:,0].max()\n    y_min, y_max = abonents[:,1].min(), abonents[:,1].max()\n    population = []\n\n    for i_sol in range(N_solutions):\n        N_stations = int(np.random.random(1)[0]*10)+1\n        stations = np.zeros((N_stations,2))\n        stations[:,0], stations[:,1] = np.random.random(N_stations)*(x_max - x_min), np.random.random(N_stations)*(y_max - y_min)\n        population.append(stations)\n    return population\n\ndef find_nearest_station(dist_matrix):\n    return np.argmin(dist_matrix, axis=1)\n\ndef pairwise_distance(abonents, stations):\n    return cdist(abonents, stations)\n\ndef evaluate_generation(abonents, population, station_cost = 1, abonent_profit_base = 1):  \n    costs = []\n    for creature in population:\n        N_stations, N_users = len(creature), len(abonents)\n        total_cost          = N_stations*station_cost\n        dist_matrix         = pairwise_distance(abonents, creature)\n        stations_assignment = find_nearest_station(dist_matrix)\n        stations_load       = np.ones(N_stations)\n        stations_load       = np.array([1/(sum(stations_assignment == i_st)+1) for i_st, st in enumerate(stations_load)])\n\n        for i_ab, abonent in enumerate(abonents):\n            dist_to_base = dist_matrix[i_ab, stations_assignment[i_ab]]\n            total_cost  -= stations_load[stations_assignment[i_ab]]*abonent_profit_base/(max(0.1, dist_to_base))\n\n        costs.append(total_cost)\n    return np.array(costs)\n\ndef mutation(population, mutation_rate = 0.3):\n    N_creatures = len(population)\n    x_min, x_max = -1, 1\n    y_min, y_max = -1, 1\n    mutated_creatures = sample(range(N_creatures), int(mutation_rate*N_creatures))\n    for i_mut in mutated_creatures:\n        N_stations = len(population[i_mut])\n        mutated_stations = sample(range(N_stations), int(mutation_rate*N_stations))\n        for i_st_mut in mutated_stations:\n            population[i_mut][i_st_mut] += np.random.normal(0, 0.01, 2)\n\n    N_new_stations = max(1, int(random.random()*mutation_rate*N_creatures))\n    for i in range(N_new_stations):\n        new_stations = np.zeros((N_new_stations,2))\n        new_stations[:,0], new_stations[:,1] = np.random.random(N_new_stations)*(x_max - x_min), np.random.random(N_new_stations)*(y_max - y_min)\n        population.append(new_stations)\n    return population\n\ndef children_creation(parent1, parent2):\n    # whoisbatya\n    batya = random.random() &gt; 0.5\n    if batya:\n        child = np.concatenate((parent1[::2], parent2[1::2]))\n    else:\n        child = np.concatenate((parent1[1::2], parent2[::2]))\n    return np.array(child)\n\ndef breed(population):\n    new_population = deepcopy(population)\n    random.shuffle(new_population)\n    N_creatures = len(population)\n    for i in range(N_creatures//2):\n        children = children_creation(population[i], population[i+1])\n        new_population.append(children)\n    return new_population\n\ndef selection(abonents, population, offsprings = 10):\n    scores = evaluate_generation(abonents, population)\n    best = np.array(scores).argsort()[:offsprings].tolist()\n    return [population[i_b] for i_b in best], population[best[0]] \n\n\ndef let_eat_bee(N_creatures, N_generations, N_obj = 10, N_abon_per_cluster = 10):\n    abonents = generate_problem(N_obj, N_abon_per_cluster)\n\n    costs_evolution = np.zeros((N_generations, N_creatures))\n    population = random_solution(abonents, N_creatures)\n    best_creatures = []\n    for generation in range(N_generations):\n        population                = mutation(population)\n        population                = breed(population)\n        population, best_creature = selection(abonents, population, N_creatures)\n        best_creatures.append(best_creature)\n\n        costs_evolution[generation, :] = evaluate_generation(abonents, population)\n\n        # Plotting\n        x_min, x_max = 0, 1\n        y_min, y_max = 0,1\n        cost_min  = [np.min(costs_evolution[i])  for i in range(generation)]\n        cost_max  = [np.max(costs_evolution[i])  for i in range(generation)]\n        cost_mean = [np.mean(costs_evolution[i]) for i in range(generation)]\n\n        fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Topology of the best solution\", \"Cost function\"))\n        fig.update_xaxes(title_text=\"x\", range = [x_min,x_max],  row=1, col=1)\n        fig.update_yaxes(title_text=\"y\", range = [y_min,y_max], row=1, col=1)\n        fig.update_yaxes(title_text=\"Total cost\", row=1, col=2)\n        fig.update_xaxes(title_text=\"Generation\", row=1, col=2)\n\n        fig.add_trace(\n            go.Scatter(x=abonents[:, 0], y=abonents[:, 1], mode='markers', name='abonents',  marker=dict(size=5)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x=best_creatures[generation][:, 0], y=best_creatures[generation][:, 1], mode='markers', name='stations', marker=dict(size=15)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_min, name='best'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_max, name='worst'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_mean, name='mean'),\n            row=1, col=2\n        )\n\n        clear_output(wait=True)\n        fig.show()\n\n    fig.write_html(\"test.html\")    \n    return costs_evolution, abonents, best_creatures\n\n\ncosts_evolution, abonents, best_creatures = let_eat_bee(200, 200)",
    "crumbs": [
      "Exercises",
      "Zero order methods"
    ]
  },
  {
    "objectID": "docs/exercises/subgradient.html",
    "href": "docs/exercises/subgradient.html",
    "title": "Subgradient and subdifferential",
    "section": "",
    "text": "Prove, that x_0 - is the minimum point of a convex function f(x) if and only if 0 \\in \\partial f(x_0)\nFind \\partial f(x), if f(x) = \\text{ReLU}(x) = \\max \\{0, x\\}\nFind \\partial f(x), if f(x) = \\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0, \\\\\n0.01x & \\text{otherwise}.\n\\end{cases}\nFind \\partial f(x), if f(x) = \\|x\\|_p при p = 1,2, \\infty\nFind \\partial f(x), if f(x) = \\|Ax - b\\|_1^2\nFind \\partial f(x), if f(x) = e^{\\|x\\|}. Try do the task for an arbitrary norm. At least, try \\|\\cdot\\| = \\|\\cdot\\|_{\\{2,1,\\infty\\}}.\nDescribe the connection between subgradient of a scalar function f: \\mathbb{R} \\to \\mathbb{R} and global linear lower bound, which support (tangent) the graph of the function at a point.\nWhat can we say about subdifferential of a convex function in those points, where the function is differentiable?\nDoes the subgradient coincide with the gradient of a function if the function is differentiable? Under which condition it holds?\nIf the function is convex on S, whether \\partial f(x) \\neq \\emptyset  \\;\\;\\; \\forall x \\in S always holds or not?\nFind \\partial f(x), if f(x) = x^3\nFind f(x) = \\lambda_{max} (A(x)) = \\sup\\limits_{\\|y\\|_2 = 1} y^T A(x)y, где A(x) = A_0 + x_1A_1 + \\ldots + x_nA_n, all the matrices A_i \\in \\mathbb{S}^k are symmetric and defined.\nFind subdifferential of a function f(x,y) = x^2 + xy + y^2 + 3\\vert x + y − 2\\vert at points (1,0) and (1,1).\nFind subdifferential of a function f(x) = \\sin x on the set X = [0, \\frac32 \\pi].\nFind subdifferential of a function f(x) = \\vert c^{\\top}x\\vert, \\; x \\in \\mathbb{R}^n.\nFind subdifferential of a function f(x) = \\|x\\|_1, \\; x \\in \\mathbb{R}^n.\nSuppose, that if f(x) = \\|x\\|_\\infty. Prove that \n\\partial f(0) = \\textbf{conv}\\{\\pm e_1, \\ldots , \\pm e_n\\},\n where e_i is i-th canonical basis vector (column of identity matrix).",
    "crumbs": [
      "Exercises",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/exercises/subgradient.html#subgradient-and-subdifferential",
    "href": "docs/exercises/subgradient.html#subgradient-and-subdifferential",
    "title": "Subgradient and subdifferential",
    "section": "",
    "text": "Prove, that x_0 - is the minimum point of a convex function f(x) if and only if 0 \\in \\partial f(x_0)\nFind \\partial f(x), if f(x) = \\text{ReLU}(x) = \\max \\{0, x\\}\nFind \\partial f(x), if f(x) = \\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0, \\\\\n0.01x & \\text{otherwise}.\n\\end{cases}\nFind \\partial f(x), if f(x) = \\|x\\|_p при p = 1,2, \\infty\nFind \\partial f(x), if f(x) = \\|Ax - b\\|_1^2\nFind \\partial f(x), if f(x) = e^{\\|x\\|}. Try do the task for an arbitrary norm. At least, try \\|\\cdot\\| = \\|\\cdot\\|_{\\{2,1,\\infty\\}}.\nDescribe the connection between subgradient of a scalar function f: \\mathbb{R} \\to \\mathbb{R} and global linear lower bound, which support (tangent) the graph of the function at a point.\nWhat can we say about subdifferential of a convex function in those points, where the function is differentiable?\nDoes the subgradient coincide with the gradient of a function if the function is differentiable? Under which condition it holds?\nIf the function is convex on S, whether \\partial f(x) \\neq \\emptyset  \\;\\;\\; \\forall x \\in S always holds or not?\nFind \\partial f(x), if f(x) = x^3\nFind f(x) = \\lambda_{max} (A(x)) = \\sup\\limits_{\\|y\\|_2 = 1} y^T A(x)y, где A(x) = A_0 + x_1A_1 + \\ldots + x_nA_n, all the matrices A_i \\in \\mathbb{S}^k are symmetric and defined.\nFind subdifferential of a function f(x,y) = x^2 + xy + y^2 + 3\\vert x + y − 2\\vert at points (1,0) and (1,1).\nFind subdifferential of a function f(x) = \\sin x on the set X = [0, \\frac32 \\pi].\nFind subdifferential of a function f(x) = \\vert c^{\\top}x\\vert, \\; x \\in \\mathbb{R}^n.\nFind subdifferential of a function f(x) = \\|x\\|_1, \\; x \\in \\mathbb{R}^n.\nSuppose, that if f(x) = \\|x\\|_\\infty. Prove that \n\\partial f(0) = \\textbf{conv}\\{\\pm e_1, \\ldots , \\pm e_n\\},\n where e_i is i-th canonical basis vector (column of identity matrix).",
    "crumbs": [
      "Exercises",
      "Subgradient and subdifferential"
    ]
  },
  {
    "objectID": "docs/exercises/projection.html",
    "href": "docs/exercises/projection.html",
    "title": "Projection",
    "section": "",
    "text": "Let us have two different points a, b \\in \\mathbb{R}^n. Prove that the set of points which in the Euclidean norm are closer to the point a than to b make up a half-space. Is this true for another norm?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S\nIllustrate the geometric inequality that connects \\pi_S(y), y \\notin S, x \\in S, from which it follows that \\pi_S(y) is a projection of the y point onto a convex set of S.\nFor which sets does the projection of the point outside this set exist? Unique?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x \\ge b \\}\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid x = x_0 + X \\alpha, X \\in \\mathbb{R}^{n \\times m}, \\alpha \\in \\mathbb{R}^{m}\\}, y \\in S\nLet S \\subseteq \\mathbb{R}^n be a closed set, and x \\in \\mathbb{R}^n be a point not lying in it. Show that the projection in l_2 norm will be unique, while in l_\\infty norm this statement is not valid.\nFind the projection of the matrix X on a set of matrices of rank k, \\;\\;\\; X \\in \\mathbb{R}^{m \\times n}, k \\leq n \\leq m. In Frobenius norm and spectral norm.\nFind a projection of the X matrix on a set of symmetrical positive semi-definite matrices of X \\in \\mathbb{R}^{n \\times n}. In Frobenius norm and the scalar product associated with it.\nFind the projection \\pi_S(y) of point y onto the set S = \\{x_1, x_2 \\in \\mathbb{R}^2 \\mid \\mid \\vert x_1\\vert + \\vert x_2\\vert = 1 \\} in \\| \\cdot \\|_1 norm. Consider the different positions of y.\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\le x_i \\le \\beta_i, i = 1, \\ldots, n \\}.\nProve that projection is a nonexpansive operator, i.e. prove, that if S \\in \\mathbb{R}^{n} is nonempty, closed and convex set, then for any (x_{1}, x_{2}) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}\n\n\\lVert \\pi_{S}(x_{2}) - \\pi_{S}(x_{1}) \\rVert_{2} \\leq \\lVert x_{2} - x_{1} \\rVert_{2}",
    "crumbs": [
      "Exercises",
      "Projection"
    ]
  },
  {
    "objectID": "docs/exercises/projection.html#projection",
    "href": "docs/exercises/projection.html#projection",
    "title": "Projection",
    "section": "",
    "text": "Let us have two different points a, b \\in \\mathbb{R}^n. Prove that the set of points which in the Euclidean norm are closer to the point a than to b make up a half-space. Is this true for another norm?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S\nIllustrate the geometric inequality that connects \\pi_S(y), y \\notin S, x \\in S, from which it follows that \\pi_S(y) is a projection of the y point onto a convex set of S.\nFor which sets does the projection of the point outside this set exist? Unique?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x \\ge b \\}\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid x = x_0 + X \\alpha, X \\in \\mathbb{R}^{n \\times m}, \\alpha \\in \\mathbb{R}^{m}\\}, y \\in S\nLet S \\subseteq \\mathbb{R}^n be a closed set, and x \\in \\mathbb{R}^n be a point not lying in it. Show that the projection in l_2 norm will be unique, while in l_\\infty norm this statement is not valid.\nFind the projection of the matrix X on a set of matrices of rank k, \\;\\;\\; X \\in \\mathbb{R}^{m \\times n}, k \\leq n \\leq m. In Frobenius norm and spectral norm.\nFind a projection of the X matrix on a set of symmetrical positive semi-definite matrices of X \\in \\mathbb{R}^{n \\times n}. In Frobenius norm and the scalar product associated with it.\nFind the projection \\pi_S(y) of point y onto the set S = \\{x_1, x_2 \\in \\mathbb{R}^2 \\mid \\mid \\vert x_1\\vert + \\vert x_2\\vert = 1 \\} in \\| \\cdot \\|_1 norm. Consider the different positions of y.\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\le x_i \\le \\beta_i, i = 1, \\ldots, n \\}.\nProve that projection is a nonexpansive operator, i.e. prove, that if S \\in \\mathbb{R}^{n} is nonempty, closed and convex set, then for any (x_{1}, x_{2}) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}\n\n\\lVert \\pi_{S}(x_{2}) - \\pi_{S}(x_{1}) \\rVert_{2} \\leq \\lVert x_{2} - x_{1} \\rVert_{2}",
    "crumbs": [
      "Exercises",
      "Projection"
    ]
  },
  {
    "objectID": "docs/exercises/line_search.html",
    "href": "docs/exercises/line_search.html",
    "title": "Line search",
    "section": "",
    "text": "Which function is called unimodal?\nDerive the convergence speed for a dichotomy method for a unimodal function. What type of convergence does this method have?\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0].\n\n\n\nIllustration\n\n\nConsider the following modification of solution localization method, in which the interval [a,b] is divided into 2 parts in a fixed proportion of t: x_t = a + t*(b-a) (maximum twice at iteration - as in the dichotomy method). Experiment with different values of t \\in [0,1] and plot the dependence of N (t) - the number of iterations needed to achieve \\varepsilon - accuracy from the t parameter. Consider \\varepsilon = 10^{-7}. Note that with t = 0.5 this method is exactly the same as the dichotomy method.\nDescribe the idea of successive parabolic interpolation. What type of convergence does this method have?\nWrite down Armijo–Goldstein condition.\nShow that if 0 &lt; c_2 &lt; c_1 &lt; 1, there may be no step lengths that satisfy the Wolfe conditions (sufficient decrease and curvature condition).\nShow that the one-dimensional minimizer of a strongly convex quadratic function always satisfies the Goldstein conditions.\nConsider the Rosenbrock function:\n\nf(x_1, x_2) =  10(x_2 − x_1^2)^2 + (x_1 − 1)^22\n\nYou are given the starting point x_0 = (-1, 2)^\\top. Implement the gradient descent algorithm:\n\nx^{k+1} = x^k - \\alpha^k \\nabla f(x^k),\n\nwhere the stepsize is choosen at each iteration via solution of the following line search problem\n\n\\alpha^k = \\arg\\min\\limits_{\\alpha \\in \\mathbb{R}^+}{f(x^k - \\alpha \\nabla f(x^k))}.\n\nImplement any line search method in this problem and plot 2 graphs: function value from iteration number and function value from the number of function calls (calculate only the function calls, don’t include the gradient calls).\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0] \n\nImplement golden search and binary search methods for this function.\nMinimize the function with these two methods and add Brent method from scipy. Compare 3 methods in terms of iterations, time, number of oracle calls.",
    "crumbs": [
      "Exercises",
      "Line search"
    ]
  },
  {
    "objectID": "docs/exercises/line_search.html#line-search",
    "href": "docs/exercises/line_search.html#line-search",
    "title": "Line search",
    "section": "",
    "text": "Which function is called unimodal?\nDerive the convergence speed for a dichotomy method for a unimodal function. What type of convergence does this method have?\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0].\n\n\n\nIllustration\n\n\nConsider the following modification of solution localization method, in which the interval [a,b] is divided into 2 parts in a fixed proportion of t: x_t = a + t*(b-a) (maximum twice at iteration - as in the dichotomy method). Experiment with different values of t \\in [0,1] and plot the dependence of N (t) - the number of iterations needed to achieve \\varepsilon - accuracy from the t parameter. Consider \\varepsilon = 10^{-7}. Note that with t = 0.5 this method is exactly the same as the dichotomy method.\nDescribe the idea of successive parabolic interpolation. What type of convergence does this method have?\nWrite down Armijo–Goldstein condition.\nShow that if 0 &lt; c_2 &lt; c_1 &lt; 1, there may be no step lengths that satisfy the Wolfe conditions (sufficient decrease and curvature condition).\nShow that the one-dimensional minimizer of a strongly convex quadratic function always satisfies the Goldstein conditions.\nConsider the Rosenbrock function:\n\nf(x_1, x_2) =  10(x_2 − x_1^2)^2 + (x_1 − 1)^22\n\nYou are given the starting point x_0 = (-1, 2)^\\top. Implement the gradient descent algorithm:\n\nx^{k+1} = x^k - \\alpha^k \\nabla f(x^k),\n\nwhere the stepsize is choosen at each iteration via solution of the following line search problem\n\n\\alpha^k = \\arg\\min\\limits_{\\alpha \\in \\mathbb{R}^+}{f(x^k - \\alpha \\nabla f(x^k))}.\n\nImplement any line search method in this problem and plot 2 graphs: function value from iteration number and function value from the number of function calls (calculate only the function calls, don’t include the gradient calls).\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0] \n\nImplement golden search and binary search methods for this function.\nMinimize the function with these two methods and add Brent method from scipy. Compare 3 methods in terms of iterations, time, number of oracle calls.",
    "crumbs": [
      "Exercises",
      "Line search"
    ]
  },
  {
    "objectID": "docs/exercises/gop.html",
    "href": "docs/exercises/gop.html",
    "title": "General optimization problems",
    "section": "",
    "text": "Linear Least squares Write down exact solution of the linear least squares problem:\n\n\\|Ax-b\\|^2 \\to \\min_{x \\in \\mathbb{R}^n}, A \\in \\mathbb{R}^{m \\times n}\n\nConsider three cases:\n\nm &lt; n\nm = n\nm &gt; n\n\nTo successfully write a test on optimization methods, a student must spend at least \\mathrm{K} kilocalories. The evening before the test, he goes to the store to buy food for dinner. There are m items in the store, the unit price of each item is p_i, i = 1, \\ldots , m. It is also known that each item’s i-th unit gives the student energy equal to k_i kilocalories. Formulate the problem of determining the contents of a minimum value basket for the successful writing of a test. Is this a convex problem? Why?\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & a^\\top x ≤ b,\n\\end{split}\n\nwhere a \\neq 0\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & l \\preceq x \\preceq u,\n\\end{split}\n\nwhere l \\preceq u\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered as a simplest portfolio optimization problem.\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = \\alpha, \\\\\n& 0 \\preceq x \\preceq 1,\n\\end{split}\n\nwhere \\alpha is an integer between 0 and n. What happens if \\alpha is not an integer (but satisfies 0 \\leq \\alpha \\leq n)? What if we change the equality to an inequality 1^\\top x \\leq \\alpha?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0. What is the solution if the problem is not convex (A \\notin \\mathbb{S}^n_{++}) (Hint: consider eigendecomposition of the matrix: A = Q \\mathbf{diag}(\\lambda)Q^\\top = \\sum\\limits_{i=1}^n \\lambda_i q_i q_i^\\top) and different cases of \\lambda &gt;0, \\lambda=0, \\lambda&lt;0?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& x^\\top Bx \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, B \\in \\mathbb{S}^n_{+}.\nConsider the equality constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{C}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nLet X \\in \\mathbb{R}^{m \\times n} with \\text{rk} X = n, \\Omega \\in \\mathbb{S}_{++}^n, and W \\in \\mathbb{R}^{k \\times n}. Find matrix G \\in \\mathbb{R}^{k \\times m}, which solves the following optimization problem:\n\nf(G) = \\text{tr} \\left(G \\Omega G^\\top \\right) \\to \\min\\limits_{GX = W}\n 1.Consider the problem of projection some point y \\in \\mathbb{R}^n,  y \\notin \\Delta^n onto the probability simplex \\Delta^n. Find 2 ways to solve the problem numerically and compare them in terms of the total computational time, memory requirements and iteration number for n = 10, 100, 1000.\n\n\\begin{split}\n& \\|x - y \\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}",
    "crumbs": [
      "Exercises",
      "General optimization problems"
    ]
  },
  {
    "objectID": "docs/exercises/gop.html#general-optimization-problems",
    "href": "docs/exercises/gop.html#general-optimization-problems",
    "title": "General optimization problems",
    "section": "",
    "text": "Linear Least squares Write down exact solution of the linear least squares problem:\n\n\\|Ax-b\\|^2 \\to \\min_{x \\in \\mathbb{R}^n}, A \\in \\mathbb{R}^{m \\times n}\n\nConsider three cases:\n\nm &lt; n\nm = n\nm &gt; n\n\nTo successfully write a test on optimization methods, a student must spend at least \\mathrm{K} kilocalories. The evening before the test, he goes to the store to buy food for dinner. There are m items in the store, the unit price of each item is p_i, i = 1, \\ldots , m. It is also known that each item’s i-th unit gives the student energy equal to k_i kilocalories. Formulate the problem of determining the contents of a minimum value basket for the successful writing of a test. Is this a convex problem? Why?\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & a^\\top x ≤ b,\n\\end{split}\n\nwhere a \\neq 0\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & l \\preceq x \\preceq u,\n\\end{split}\n\nwhere l \\preceq u\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered as a simplest portfolio optimization problem.\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = \\alpha, \\\\\n& 0 \\preceq x \\preceq 1,\n\\end{split}\n\nwhere \\alpha is an integer between 0 and n. What happens if \\alpha is not an integer (but satisfies 0 \\leq \\alpha \\leq n)? What if we change the equality to an inequality 1^\\top x \\leq \\alpha?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0. What is the solution if the problem is not convex (A \\notin \\mathbb{S}^n_{++}) (Hint: consider eigendecomposition of the matrix: A = Q \\mathbf{diag}(\\lambda)Q^\\top = \\sum\\limits_{i=1}^n \\lambda_i q_i q_i^\\top) and different cases of \\lambda &gt;0, \\lambda=0, \\lambda&lt;0?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& x^\\top Bx \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, B \\in \\mathbb{S}^n_{+}.\nConsider the equality constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{C}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nLet X \\in \\mathbb{R}^{m \\times n} with \\text{rk} X = n, \\Omega \\in \\mathbb{S}_{++}^n, and W \\in \\mathbb{R}^{k \\times n}. Find matrix G \\in \\mathbb{R}^{k \\times m}, which solves the following optimization problem:\n\nf(G) = \\text{tr} \\left(G \\Omega G^\\top \\right) \\to \\min\\limits_{GX = W}\n 1.Consider the problem of projection some point y \\in \\mathbb{R}^n,  y \\notin \\Delta^n onto the probability simplex \\Delta^n. Find 2 ways to solve the problem numerically and compare them in terms of the total computational time, memory requirements and iteration number for n = 10, 100, 1000.\n\n\\begin{split}\n& \\|x - y \\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}",
    "crumbs": [
      "Exercises",
      "General optimization problems"
    ]
  },
  {
    "objectID": "docs/exercises/duality.html",
    "href": "docs/exercises/duality.html",
    "title": "Duality",
    "section": "",
    "text": "Toy example\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x = 0 \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate.Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& Cx = d \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f_0(x) = \\sum\\limits_{i=1}^n f_i(x_i) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b \\\\\n& f_i(x) - \\text{ differentiable and strictly convex}\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nNew variables. Consider an unconstrained problem of the form:\n\nf_0(Ax + b) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nAnd its equivalent reformulation:\n\n\\begin{split}\n& f_0(y) \\to \\min\\limits_{y \\in \\mathbb{R}^{m} }\\\\\n\\text{s.t. } & y = Ax + b \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problems\nFind the dual functions\nWrite down the dual problems\n\nThe weak duality inequality, d^* ≤ p^* , clearly holds when d^* = -\\infty or p^* = \\infty. Show that it holds in the other two cases as well: If p^* = −\\infty, then we must have d^* = −\\infty, and also, if d^* = \\infty, then we must have p^* = \\infty.\nExpress the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nLeast Squares. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nStandard form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nTwo-way partitioning problem. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top W x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x_i^2 = 1, i = 1, \\ldots, n \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\nCan you reduce this problem to the eigenvalue problem? 🐱\n\nEntropy maximization. Let we have the primal problem:\n\n\\begin{split}\n& \\sum_i x_i \\ln x_i \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& 1^\\top x = 1 \\\\\n& x \\succ 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nMinimum volume covering ellipsoid. Let we have the primal problem:\n\n\\begin{split}\n& \\ln \\text{det} X^{-1} \\to \\min\\limits_{X \\in \\mathbb{S}^{n}_{++} }\\\\\n\\text{s.t. } & a_i^\\top X a_i \\leq 1 , i = 1, \\ldots, m\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nEquality constrained norm minimization. Let we have the primal problem:\n\n\\begin{split}\n& \\|x\\| \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nInequality form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nNonconvex strong duality Let we have the primal problem:\n\n\\begin{split}\n& x^\\top Ax +2b^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1 \\\\\n& A \\in \\mathbb{S}^n, A \\nsucceq 0, b \\in \\mathbb{R}^n\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nA penalty method for equality constraints. We consider the problem minimize\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere $f_0(x): ^n $ is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\|Ax - b\\|_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [ 1,m ] \\}. First introduce new variables y_i and equality constraints y_i = b_i − a^\\top_i x. (The solution of this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [ 1,m ]. Analytic centers have geometric applications, and play an important role in barrier methods.)",
    "crumbs": [
      "Exercises",
      "Duality"
    ]
  },
  {
    "objectID": "docs/exercises/duality.html#duality",
    "href": "docs/exercises/duality.html#duality",
    "title": "Duality",
    "section": "",
    "text": "Toy example\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x = 0 \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate.Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& Cx = d \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f_0(x) = \\sum\\limits_{i=1}^n f_i(x_i) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b \\\\\n& f_i(x) - \\text{ differentiable and strictly convex}\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nNew variables. Consider an unconstrained problem of the form:\n\nf_0(Ax + b) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nAnd its equivalent reformulation:\n\n\\begin{split}\n& f_0(y) \\to \\min\\limits_{y \\in \\mathbb{R}^{m} }\\\\\n\\text{s.t. } & y = Ax + b \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problems\nFind the dual functions\nWrite down the dual problems\n\nThe weak duality inequality, d^* ≤ p^* , clearly holds when d^* = -\\infty or p^* = \\infty. Show that it holds in the other two cases as well: If p^* = −\\infty, then we must have d^* = −\\infty, and also, if d^* = \\infty, then we must have p^* = \\infty.\nExpress the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nLeast Squares. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nStandard form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nTwo-way partitioning problem. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top W x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x_i^2 = 1, i = 1, \\ldots, n \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\nCan you reduce this problem to the eigenvalue problem? 🐱\n\nEntropy maximization. Let we have the primal problem:\n\n\\begin{split}\n& \\sum_i x_i \\ln x_i \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& 1^\\top x = 1 \\\\\n& x \\succ 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nMinimum volume covering ellipsoid. Let we have the primal problem:\n\n\\begin{split}\n& \\ln \\text{det} X^{-1} \\to \\min\\limits_{X \\in \\mathbb{S}^{n}_{++} }\\\\\n\\text{s.t. } & a_i^\\top X a_i \\leq 1 , i = 1, \\ldots, m\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nEquality constrained norm minimization. Let we have the primal problem:\n\n\\begin{split}\n& \\|x\\| \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nInequality form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nNonconvex strong duality Let we have the primal problem:\n\n\\begin{split}\n& x^\\top Ax +2b^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1 \\\\\n& A \\in \\mathbb{S}^n, A \\nsucceq 0, b \\in \\mathbb{R}^n\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nA penalty method for equality constraints. We consider the problem minimize\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere $f_0(x): ^n $ is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\|Ax - b\\|_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [ 1,m ] \\}. First introduce new variables y_i and equality constraints y_i = b_i − a^\\top_i x. (The solution of this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [ 1,m ]. Analytic centers have geometric applications, and play an important role in barrier methods.)",
    "crumbs": [
      "Exercises",
      "Duality"
    ]
  },
  {
    "objectID": "docs/exercises/convex_sets.html",
    "href": "docs/exercises/convex_sets.html",
    "title": "Convex sets",
    "section": "",
    "text": "Show that the set is convex if and only if its intersection with any line is convex.\nShow that the convex hull of the S set is the intersection of all convex sets containing S.\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\alpha &lt; \\mathbb{E} f(x) &lt; \\beta, where \\mathbb{E}f(x) stands for expected value of f(x): \\mathbb{R} \\rightarrow \\mathbb{R}, i.e. \\mathbb{E}f(x) = \\sum\\limits_{i=1}^n p_i f(a_i)\n\\mathbb{E}x^2 \\le \\alpha\n\\mathbb{V}x \\le \\alpha\n\nProve that if the set is convex, its interior is also convex. Is the opposite true?\nProve that if the set is convex, its closure is also convex. Is the opposite true?\nProve that the set of square symmetric positive definite matrices is convex.\nShow that the set of S is convex if and only if\n\n\\forall \\lambda_1, \\lambda_2 \\geq 0, \\quad (\\lambda_1, \\lambda_2) \\neq (0, 0):  \\lambda_1 S + \\lambda_2 S = (\\lambda_1 + \\lambda_2)S\n\nCalculate the Minkowski sum of the line segment and the square on the plane, the line segment and the triangle, the line segment and the circle, the line segment and the disk.\nFind the minimum value of k, at which the set of \\{x \\in \\mathbb{R}^2 \\mid (x_1^2 + 1) x_2\\le 2, x_2 \\ge k\\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nGive an example of two closed convex sets, the sum of which is not closed\nFind convex and conical hulls of the following sets: \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2, x_1 \\ge 0 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1 x_2 = 1 \\}\nShow that the set of directions of the strict local descending of the differentiable function in a point is a convex cone.\nProve that K:\n\nK = \\{ x \\in \\mathbb{R}^3 \\mid x_1^2 - 2x_1x_3 + x_2^2 \\leq 0, x_3 \\geq 0 \\}\n\nis a convex cone.\nFind the convex hulls of the following sets:\n\nx^2 + y^2 \\leq 1, xy = 0\nx^2 + y^2 = 1, x - y = 0\nx^2 + y^2 = 1, \\|x\\| \\leq 1, \\|y\\|\ny \\leq e^x, y \\geq \\|x\\|\n\nFor an arbitrary set of S, let’s say \\tilde{S} consists of all segments of [a,b] with the ends of a,b \\in S. Is it true that \\tilde{S} = \\text{conv}(S) ?\nIs the given set a convex polyhedron (could be written in the form of Ax \\preceq b, Cx = d):\n\nS = \\{ y_1a_1 + y_2a_2 \\mid -1 \\leq y_1, y_2 \\leq 1 \\}; a_1, a_2 \\in \\mathbb{R}^n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, \\mathbf{1}^\\top x = 1, \\sum\\limits_{i=1}^n x_ia_i = b_1, \\sum\\limits_{i=1}^n x_ia_i^2 = b_2 \\}; a_1, \\ldots a_n, b_1, b_2 \\in \\mathbb{R}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\|y\\|_2 = 1 \\}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\sum\\limits_{i=1}^n \\|y_i\\| = 1 \\}\n\nLet S \\subseteq \\mathbb{R}^n is a set of solutions to the quadratic inequality:\n\nS = \\{x \\in \\mathbb{R}^n \\mid x^\\top A x + b^\\top x + c \\leq 0 \\}; A \\in \\mathbb{S}^n, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}\n\n\nShow that if A \\succeq 0, S is convex. Is the opposite true?\nShow that the intersection of S with the hyperplane defined by the g^\\top x + h = 0, g \\neq 0 is convex if A + \\lambda gg^\\top \\succeq 0 for some real \\lambda \\in \\mathbb{R}. Is the opposite true?\n\nShow that the hyperbolic set of \\{x \\in \\mathbb{R}^n_+ | \\prod\\limits_{i=1}^n x_i \\geq 1 \\} is convex. Hint: For 0 \\leq \\theta \\leq 1 it is valid, that a^\\theta b^{1 - \\theta} \\leq \\theta a + (1-\\theta)b with non-negative a,b.\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\|_2 \\leq \\|x-y\\|_2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\|x - a\\|_2 \\leq \\theta\\|xb\\|_2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}\n\nFind the conic hull of the set of rank k matrix products \\{XX^\\top \\mid X \\in \\mathbb{R}^{n \\times k}, \\mathbf{rank} X = k \\}?\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nLet S be such that \\forall x,y \\in S \\to \\frac{1}{2}(x+y) \\in S. Is this set convex?\nFind the conic hull of the following sets in \\mathbb{R}^2:\n\ny = x^2\ny = x^2, x \\geq 0\ny = x^2 + x, x \\geq 0\nxy=1, x &gt; 0\ny = \\sin x, 0 \\leq x \\leq \\pi\ny = e^x\n\nLet S_1 = \\{x^2 + y^2 \\leq 1 \\} is a disk of \\mathbb{R^3} and S_2 is a segment of \\left[(0,0,-1), (0,0,1)\\right]. How their convex combination with \\alpha, \\beta looks like.\nIs the next set convex?\n\n\\{a \\in \\mathbb{R}^k \\mid p(0) = 1, \\|p(t)\\| \\leq 1 \\;\\; \\forall \\alpha \\leq t \\leq \\beta, \\;\\; p(t) = a_1 + a_2t + \\ldots + a_kt^{k-1} \\}\n\nProve that in order for K \\subseteq \\mathbb{R}^n to be a convex cone, it is enough that K contains all possible non-negative combinations of its points.\nProve that in order for S \\subseteq \\mathbb{R}^n to be an affine set it is necessary and sufficient that S contains all possible affine combinations of its points.\nПусть S_1, \\ldots, S_k - произвольные непустые множества в \\mathbb{R}^n. Докажите, что:\n\n\\mathbf{cone} \\left( \\bigcup\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{cone} \\left( S_i\\right)\n\\mathbf{conv} \\left( \\sum\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{conv} \\left( S_i\\right)\n\nProve, that the set S \\subseteq \\mathbb{R}^n is convex if and only if (\\alpha + \\beta)S = \\alpha S + \\beta S for all non-negative \\alpha and \\beta\\quad (\\alpha, \\beta) \\neq (0, 0)\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\n\\mathbb{V}x \\ge \\alpha\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\| \\mathbf{x} - \\mathbf{x}_c \\| \\leq r \\}) - is convex.\nProve, that if S is convex, then S+S = 2S. Give an counterexample in case, when S - is not convex.\nWhich of the following operations does not preserve convexity if X,Y \\subseteq \\mathbb{R}^n are convex sets?\n\nX \\cup Y\nX \\times Y = \\left\\{ (x,y) \\; \\mid \\; x \\in X, y \\in Y \\right\\}\n\\alpha X + \\beta Y = \\{ \\alpha x + \\beta y \\; \\mid \\; x \\in X, \\; y \\in Y, \\; \\alpha,  \\beta \\in \\mathbb{R} \\}\n\\alpha X  = \\{ \\alpha x  \\; \\mid \\; x \\in X, \\; \\alpha  \\in \\mathbb{R_{-}} \\}\nX^{c} = \\{x \\in \\mathbb{R}^n \\; \\mid \\; x \\notin X\\}\n\nShow, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.",
    "crumbs": [
      "Exercises",
      "Convex sets"
    ]
  },
  {
    "objectID": "docs/exercises/convex_sets.html#convex-sets",
    "href": "docs/exercises/convex_sets.html#convex-sets",
    "title": "Convex sets",
    "section": "",
    "text": "Show that the set is convex if and only if its intersection with any line is convex.\nShow that the convex hull of the S set is the intersection of all convex sets containing S.\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\alpha &lt; \\mathbb{E} f(x) &lt; \\beta, where \\mathbb{E}f(x) stands for expected value of f(x): \\mathbb{R} \\rightarrow \\mathbb{R}, i.e. \\mathbb{E}f(x) = \\sum\\limits_{i=1}^n p_i f(a_i)\n\\mathbb{E}x^2 \\le \\alpha\n\\mathbb{V}x \\le \\alpha\n\nProve that if the set is convex, its interior is also convex. Is the opposite true?\nProve that if the set is convex, its closure is also convex. Is the opposite true?\nProve that the set of square symmetric positive definite matrices is convex.\nShow that the set of S is convex if and only if\n\n\\forall \\lambda_1, \\lambda_2 \\geq 0, \\quad (\\lambda_1, \\lambda_2) \\neq (0, 0):  \\lambda_1 S + \\lambda_2 S = (\\lambda_1 + \\lambda_2)S\n\nCalculate the Minkowski sum of the line segment and the square on the plane, the line segment and the triangle, the line segment and the circle, the line segment and the disk.\nFind the minimum value of k, at which the set of \\{x \\in \\mathbb{R}^2 \\mid (x_1^2 + 1) x_2\\le 2, x_2 \\ge k\\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nGive an example of two closed convex sets, the sum of which is not closed\nFind convex and conical hulls of the following sets: \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2, x_1 \\ge 0 \\}, \\{x \\in \\mathbb{R}^2 \\mid x_1 x_2 = 1 \\}\nShow that the set of directions of the strict local descending of the differentiable function in a point is a convex cone.\nProve that K:\n\nK = \\{ x \\in \\mathbb{R}^3 \\mid x_1^2 - 2x_1x_3 + x_2^2 \\leq 0, x_3 \\geq 0 \\}\n\nis a convex cone.\nFind the convex hulls of the following sets:\n\nx^2 + y^2 \\leq 1, xy = 0\nx^2 + y^2 = 1, x - y = 0\nx^2 + y^2 = 1, \\|x\\| \\leq 1, \\|y\\|\ny \\leq e^x, y \\geq \\|x\\|\n\nFor an arbitrary set of S, let’s say \\tilde{S} consists of all segments of [a,b] with the ends of a,b \\in S. Is it true that \\tilde{S} = \\text{conv}(S) ?\nIs the given set a convex polyhedron (could be written in the form of Ax \\preceq b, Cx = d):\n\nS = \\{ y_1a_1 + y_2a_2 \\mid -1 \\leq y_1, y_2 \\leq 1 \\}; a_1, a_2 \\in \\mathbb{R}^n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, \\mathbf{1}^\\top x = 1, \\sum\\limits_{i=1}^n x_ia_i = b_1, \\sum\\limits_{i=1}^n x_ia_i^2 = b_2 \\}; a_1, \\ldots a_n, b_1, b_2 \\in \\mathbb{R}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\|y\\|_2 = 1 \\}\nS = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\sum\\limits_{i=1}^n \\|y_i\\| = 1 \\}\n\nLet S \\subseteq \\mathbb{R}^n is a set of solutions to the quadratic inequality:\n\nS = \\{x \\in \\mathbb{R}^n \\mid x^\\top A x + b^\\top x + c \\leq 0 \\}; A \\in \\mathbb{S}^n, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}\n\n\nShow that if A \\succeq 0, S is convex. Is the opposite true?\nShow that the intersection of S with the hyperplane defined by the g^\\top x + h = 0, g \\neq 0 is convex if A + \\lambda gg^\\top \\succeq 0 for some real \\lambda \\in \\mathbb{R}. Is the opposite true?\n\nShow that the hyperbolic set of \\{x \\in \\mathbb{R}^n_+ | \\prod\\limits_{i=1}^n x_i \\geq 1 \\} is convex. Hint: For 0 \\leq \\theta \\leq 1 it is valid, that a^\\theta b^{1 - \\theta} \\leq \\theta a + (1-\\theta)b with non-negative a,b.\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\|_2 \\leq \\|x-y\\|_2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\|x - a\\|_2 \\leq \\theta\\|xb\\|_2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}\n\nFind the conic hull of the set of rank k matrix products \\{XX^\\top \\mid X \\in \\mathbb{R}^{n \\times k}, \\mathbf{rank} X = k \\}?\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nLet S be such that \\forall x,y \\in S \\to \\frac{1}{2}(x+y) \\in S. Is this set convex?\nFind the conic hull of the following sets in \\mathbb{R}^2:\n\ny = x^2\ny = x^2, x \\geq 0\ny = x^2 + x, x \\geq 0\nxy=1, x &gt; 0\ny = \\sin x, 0 \\leq x \\leq \\pi\ny = e^x\n\nLet S_1 = \\{x^2 + y^2 \\leq 1 \\} is a disk of \\mathbb{R^3} and S_2 is a segment of \\left[(0,0,-1), (0,0,1)\\right]. How their convex combination with \\alpha, \\beta looks like.\nIs the next set convex?\n\n\\{a \\in \\mathbb{R}^k \\mid p(0) = 1, \\|p(t)\\| \\leq 1 \\;\\; \\forall \\alpha \\leq t \\leq \\beta, \\;\\; p(t) = a_1 + a_2t + \\ldots + a_kt^{k-1} \\}\n\nProve that in order for K \\subseteq \\mathbb{R}^n to be a convex cone, it is enough that K contains all possible non-negative combinations of its points.\nProve that in order for S \\subseteq \\mathbb{R}^n to be an affine set it is necessary and sufficient that S contains all possible affine combinations of its points.\nПусть S_1, \\ldots, S_k - произвольные непустые множества в \\mathbb{R}^n. Докажите, что:\n\n\\mathbf{cone} \\left( \\bigcup\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{cone} \\left( S_i\\right)\n\\mathbf{conv} \\left( \\sum\\limits_{i=1}^k S_i\\right) = \\sum\\limits_{i=1}^k \\mathbf{conv} \\left( S_i\\right)\n\nProve, that the set S \\subseteq \\mathbb{R}^n is convex if and only if (\\alpha + \\beta)S = \\alpha S + \\beta S for all non-negative \\alpha and \\beta\\quad (\\alpha, \\beta) \\neq (0, 0)\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\n\\mathbb{V}x \\ge \\alpha\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\| \\mathbf{x} - \\mathbf{x}_c \\| \\leq r \\}) - is convex.\nProve, that if S is convex, then S+S = 2S. Give an counterexample in case, when S - is not convex.\nWhich of the following operations does not preserve convexity if X,Y \\subseteq \\mathbb{R}^n are convex sets?\n\nX \\cup Y\nX \\times Y = \\left\\{ (x,y) \\; \\mid \\; x \\in X, y \\in Y \\right\\}\n\\alpha X + \\beta Y = \\{ \\alpha x + \\beta y \\; \\mid \\; x \\in X, \\; y \\in Y, \\; \\alpha,  \\beta \\in \\mathbb{R} \\}\n\\alpha X  = \\{ \\alpha x  \\; \\mid \\; x \\in X, \\; \\alpha  \\in \\mathbb{R_{-}} \\}\nX^{c} = \\{x \\in \\mathbb{R}^n \\; \\mid \\; x \\notin X\\}\n\nShow, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.",
    "crumbs": [
      "Exercises",
      "Convex sets"
    ]
  },
  {
    "objectID": "docs/exercises/convergence.html",
    "href": "docs/exercises/convergence.html",
    "title": "Rates of convergence",
    "section": "",
    "text": "Show with the definition that the sequence \\left\\{ \\dfrac{1}{k} \\right\\}_{k=1}^\\infty does not have a linear convergence rate (but it converges to zero).\nShow with the definition that the sequence \\left\\{ \\dfrac{1}{k^k} \\right\\}_{k=1}^\\infty does not have a quadratic convergence rate (but it converges to zero).\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^k.\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^{2^k}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k^2}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k!}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^2}, & \\text{if } k\\text{ is odd} \\end{cases}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^{2k}}, & \\text{if } k\\text{ is odd} \\end{cases}.\nShow that the sequence x_k = 1 + (0.5)^{2^k} is quadratically converged to 1.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\left(\\frac{1}{4}\\right)^{2^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{x_{k-1}}{k}, & \\text{if } k\\text{ is odd} \\end{cases}.\nLet \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\nDetermine the convergence type of a given sequence r_k =\\begin{cases} \\frac{1}{2^k}, & \\text{if } k\\text{ is odd} \\\\ \\frac{1}{3^{2k}}, & \\text{if } k\\text{ is even} \\end{cases}.",
    "crumbs": [
      "Exercises",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/exercises/convergence.html#rates-of-convergence",
    "href": "docs/exercises/convergence.html#rates-of-convergence",
    "title": "Rates of convergence",
    "section": "",
    "text": "Show with the definition that the sequence \\left\\{ \\dfrac{1}{k} \\right\\}_{k=1}^\\infty does not have a linear convergence rate (but it converges to zero).\nShow with the definition that the sequence \\left\\{ \\dfrac{1}{k^k} \\right\\}_{k=1}^\\infty does not have a quadratic convergence rate (but it converges to zero).\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^k.\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^{2^k}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k^2}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k!}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^2}, & \\text{if } k\\text{ is odd} \\end{cases}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^{2k}}, & \\text{if } k\\text{ is odd} \\end{cases}.\nShow that the sequence x_k = 1 + (0.5)^{2^k} is quadratically converged to 1.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\left(\\frac{1}{4}\\right)^{2^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{x_{k-1}}{k}, & \\text{if } k\\text{ is odd} \\end{cases}.\nLet \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\nDetermine the convergence type of a given sequence r_k =\\begin{cases} \\frac{1}{2^k}, & \\text{if } k\\text{ is odd} \\\\ \\frac{1}{3^{2k}}, & \\text{if } k\\text{ is even} \\end{cases}.",
    "crumbs": [
      "Exercises",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_functions.html",
    "href": "docs/exercises/conjugate_functions.html",
    "title": "Conjugate functions",
    "section": "",
    "text": "Find f^*(y), if f(x) = ax + b\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = e^x\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, \\;\\;\\; f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) =\\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n\nFind f^*(y), if f(x) = -\\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = -0,5 - \\log x, \\;\\; x&gt;0\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nFind f^*(y), if f(x) = - (a^2 - x^2)^{1/2}, \\;\\;\\; \\vert x\\vert \\le a, \\;\\;\\; a&gt;0\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) = \\|x\\|\nFind f^*(y), if f(x) = \\dfrac{1}{2}\\|x\\|^2\nName any 3 non-trivial facts about conjugate function.\nFind conjugate function to the f(x) = \\dfrac{1}{x}, \\;\\; x \\in \\mathbb{R}_{++}\nFind conjugate function to the f(x) = x^p, \\;\\; x \\in \\mathbb{R}_{++}, \\;\\; p&gt;1\nProve, that if f(x_1, x_2) = g_1(x_1) + g_2(x_2), then f^*(y_1, y_2) = g_1^*(y_1) + g_2^*(y_2)\nProve, that if f(x) = g(x-b), then f^*(y) = b^\\top y + g^*(y)\nProve, that if f(x) = \\alpha g(x) and $ &gt; 0 $, then f^*(y) = \\alpha g^*(y/\\alpha)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y)",
    "crumbs": [
      "Exercises",
      "Conjugate functions"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_functions.html#conjugate-functions",
    "href": "docs/exercises/conjugate_functions.html#conjugate-functions",
    "title": "Conjugate functions",
    "section": "",
    "text": "Find f^*(y), if f(x) = ax + b\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = e^x\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, \\;\\;\\; f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) =\\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n\nFind f^*(y), if f(x) = -\\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = -0,5 - \\log x, \\;\\; x&gt;0\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nFind f^*(y), if f(x) = - (a^2 - x^2)^{1/2}, \\;\\;\\; \\vert x\\vert \\le a, \\;\\;\\; a&gt;0\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) = \\|x\\|\nFind f^*(y), if f(x) = \\dfrac{1}{2}\\|x\\|^2\nName any 3 non-trivial facts about conjugate function.\nFind conjugate function to the f(x) = \\dfrac{1}{x}, \\;\\; x \\in \\mathbb{R}_{++}\nFind conjugate function to the f(x) = x^p, \\;\\; x \\in \\mathbb{R}_{++}, \\;\\; p&gt;1\nProve, that if f(x_1, x_2) = g_1(x_1) + g_2(x_2), then f^*(y_1, y_2) = g_1^*(y_1) + g_2^*(y_2)\nProve, that if f(x) = g(x-b), then f^*(y) = b^\\top y + g^*(y)\nProve, that if f(x) = \\alpha g(x) and $ &gt; 0 $, then f^*(y) = \\alpha g^*(y/\\alpha)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y)",
    "crumbs": [
      "Exercises",
      "Conjugate functions"
    ]
  },
  {
    "objectID": "docs/benchmarks/linear_least_squares.html",
    "href": "docs/benchmarks/linear_least_squares.html",
    "title": "Linear Least Squares",
    "section": "",
    "text": "In a least-squares, or linear regression, problem, we have measurements A \\in \\mathbb{R}^{m \\times n} and b \\in \\mathbb{R}^{m} and seek a vector x \\in \\mathbb{R}^{n} such that A x is close to b. Closeness is defined as the sum of the squared differences:\n\nf(x) = \\|Ax - b\\|_2^2 \\to \\min_{x \\in \\mathbb{R^n}}",
    "crumbs": [
      "Benchmarks",
      "Linear Least Squares"
    ]
  },
  {
    "objectID": "docs/benchmarks/linear_least_squares.html#problem",
    "href": "docs/benchmarks/linear_least_squares.html#problem",
    "title": "Linear Least Squares",
    "section": "",
    "text": "In a least-squares, or linear regression, problem, we have measurements A \\in \\mathbb{R}^{m \\times n} and b \\in \\mathbb{R}^{m} and seek a vector x \\in \\mathbb{R}^{n} such that A x is close to b. Closeness is defined as the sum of the squared differences:\n\nf(x) = \\|Ax - b\\|_2^2 \\to \\min_{x \\in \\mathbb{R^n}}",
    "crumbs": [
      "Benchmarks",
      "Linear Least Squares"
    ]
  },
  {
    "objectID": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "href": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "title": "CNN on FashionMNIST",
    "section": "",
    "text": "This chapter is WIP. We will make interactive graphs with benchmarx library, which allows to benchmark different optimizers in a convenient, reproducible way.",
    "crumbs": [
      "Benchmarks",
      "CNN on FashionMNIST"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html",
    "href": "docs/applications/total_variation_inpainting.html",
    "title": "Total variation in-painting",
    "section": "",
    "text": "Illustration\n\n\n\n\nA grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:\n\n\n\nIllustration\n\n\n\n\n\nFor the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIt is not that easy, right?\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nOnly 5% of all pixels are left:\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nWhat about 1% of all pixels?\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#problem",
    "href": "docs/applications/total_variation_inpainting.html#problem",
    "title": "Total variation in-painting",
    "section": "",
    "text": "Illustration\n\n\n\n\nA grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:\n\n\n\nIllustration\n\n\n\n\n\nFor the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIt is not that easy, right?\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nOnly 5% of all pixels are left:\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nWhat about 1% of all pixels?\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#code",
    "href": "docs/applications/total_variation_inpainting.html#code",
    "title": "Total variation in-painting",
    "section": "2 Code",
    "text": "2 Code\nOpen In Colab{: .btn } ## References\n\nCVXPY documentation\nInteractive demo",
    "crumbs": [
      "Applications",
      "Total variation in-painting"
    ]
  },
  {
    "objectID": "docs/applications/rendezvous.html",
    "href": "docs/applications/rendezvous.html",
    "title": "Rendezvous problem",
    "section": "",
    "text": "Illustration\n\n\nWe have two bodies in discrete time: the first is described by its coordinate x_i and its speed v_i, the second has coordinate z_i and speed u_i. Each body has its own dynamics, which we denote as linear systems with matrices A, B, C, D:\n\n\\begin{align*}\nx_{i+1} = Ax_i + Bu_i \\\\\nz_{i+1} = Cz_i + Dv_i\n\\end{align*}\n\nWe want these bodies to meet in future at some point T in such a way, that preserve minimum energy through the path. We will consider only kinetic energy, which is proportional to the squared speed at each point of time, that’s why optimization problem takes the following form:\n\n\\begin{align*}\n& \\min \\sum_{i=1}^T \\|u_i\\|_2^2 + \\|v_i\\|_2^2 \\\\\n\\text{s.t. } & x_{t+1} = Ax_t + Bu_t, \\; t = 1,\\ldots,T-1\\\\\n& z_{t+1} = Cz_t + Dv_t, \\; t = 1,\\ldots,T-1\\\\\n& x_T = z_T\n\\end{align*}\n\nProblem of this type arise in space engineering - just imagine, that the first body is the spaceship, while the second, say, Mars.",
    "crumbs": [
      "Applications",
      "Rendezvous problem"
    ]
  },
  {
    "objectID": "docs/applications/rendezvous.html#problem",
    "href": "docs/applications/rendezvous.html#problem",
    "title": "Rendezvous problem",
    "section": "",
    "text": "Illustration\n\n\nWe have two bodies in discrete time: the first is described by its coordinate x_i and its speed v_i, the second has coordinate z_i and speed u_i. Each body has its own dynamics, which we denote as linear systems with matrices A, B, C, D:\n\n\\begin{align*}\nx_{i+1} = Ax_i + Bu_i \\\\\nz_{i+1} = Cz_i + Dv_i\n\\end{align*}\n\nWe want these bodies to meet in future at some point T in such a way, that preserve minimum energy through the path. We will consider only kinetic energy, which is proportional to the squared speed at each point of time, that’s why optimization problem takes the following form:\n\n\\begin{align*}\n& \\min \\sum_{i=1}^T \\|u_i\\|_2^2 + \\|v_i\\|_2^2 \\\\\n\\text{s.t. } & x_{t+1} = Ax_t + Bu_t, \\; t = 1,\\ldots,T-1\\\\\n& z_{t+1} = Cz_t + Dv_t, \\; t = 1,\\ldots,T-1\\\\\n& x_T = z_T\n\\end{align*}\n\nProblem of this type arise in space engineering - just imagine, that the first body is the spaceship, while the second, say, Mars.",
    "crumbs": [
      "Applications",
      "Rendezvous problem"
    ]
  },
  {
    "objectID": "docs/applications/rendezvous.html#code",
    "href": "docs/applications/rendezvous.html#code",
    "title": "Rendezvous problem",
    "section": "2 Code",
    "text": "2 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Rendezvous problem"
    ]
  },
  {
    "objectID": "docs/applications/rendezvous.html#references",
    "href": "docs/applications/rendezvous.html#references",
    "title": "Rendezvous problem",
    "section": "3 References",
    "text": "3 References\n\nJupyter notebook by A. Katrutsa",
    "crumbs": [
      "Applications",
      "Rendezvous problem"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html",
    "href": "docs/applications/least_squares.html",
    "title": "Linear least squares",
    "section": "",
    "text": "Illustration\n\n\nIn a least-squares, or linear regression, problem, we have measurements X \\in \\mathbb{R}^{m \\times n} and y \\in \\mathbb{R}^{m} and seek a vector \\theta \\in \\mathbb{R}^{n} such that $X $ is close to y. Closeness is defined as the sum of the squared differences:\n\n\\sum\\limits_{i=1}^m (x_i^\\top \\theta - y_i)^2\n\nalso known as the l_2-norm squared, \\|X \\theta - y\\|^2_2\nFor example, we might have a dataset of m users, each represented by n features. Each row x_i^\\top of X is the features for user i, while the corresponding entry y_i of y is the measurement we want to predict from x_i^\\top, such as ad spending. The prediction is given by x_i^\\top \\theta.\nWe find the optimal \\theta by solving the optimization problem\n\n\\|X \\theta - y\\|^2_2 \\to \\min_{\\theta \\in \\mathbb{R}^{n}}\n\nLet \\theta^* denote the optimal $ $. The quantity $ r=X ^* - y $ is known as the residual. If $ |r|_2 = 0 $, we have a perfect fit.\nNote, that the function needn’t be linear in the argument x but only in the parameters \\theta that are to be determined in the best fit.",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#problem",
    "href": "docs/applications/least_squares.html#problem",
    "title": "Linear least squares",
    "section": "",
    "text": "Illustration\n\n\nIn a least-squares, or linear regression, problem, we have measurements X \\in \\mathbb{R}^{m \\times n} and y \\in \\mathbb{R}^{m} and seek a vector \\theta \\in \\mathbb{R}^{n} such that $X $ is close to y. Closeness is defined as the sum of the squared differences:\n\n\\sum\\limits_{i=1}^m (x_i^\\top \\theta - y_i)^2\n\nalso known as the l_2-norm squared, \\|X \\theta - y\\|^2_2\nFor example, we might have a dataset of m users, each represented by n features. Each row x_i^\\top of X is the features for user i, while the corresponding entry y_i of y is the measurement we want to predict from x_i^\\top, such as ad spending. The prediction is given by x_i^\\top \\theta.\nWe find the optimal \\theta by solving the optimization problem\n\n\\|X \\theta - y\\|^2_2 \\to \\min_{\\theta \\in \\mathbb{R}^{n}}\n\nLet \\theta^* denote the optimal $ $. The quantity $ r=X ^* - y $ is known as the residual. If $ |r|_2 = 0 $, we have a perfect fit.\nNote, that the function needn’t be linear in the argument x but only in the parameters \\theta that are to be determined in the best fit.",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#approaches",
    "href": "docs/applications/least_squares.html#approaches",
    "title": "Linear least squares",
    "section": "2 Approaches",
    "text": "2 Approaches\n\n2.1 Moore–Penrose inverse\nIf the matrix X is relatively small, we can write down and calculate exact solution:\n\n\\theta^* = (X^\\top X)^{-1} X^\\top y = X^\\dagger y,\n\nwhere X^\\dagger is called pseudo-inverse matrix. However, this approach squares the condition number of the problem, which could be an obstacle in case of ill-conditioned huge scale problem.\n\n\n2.2 QR decomposition\nFor any matrix X \\in \\mathbb{R}^{m \\times n} there is exists QR decomposition:\n\nX = Q \\cdot R,\n\nwhere Q is an orthogonal matrix (its columns are orthogonal unit vectors meaning Q^\\top Q=QQ^\\top=I and R is an upper triangular matrix. It is important to notice, that since Q^{-1} = Q^\\top, we have:\n\nQR\\theta = y \\quad \\longrightarrow \\quad R \\theta = Q^\\top y\n\nNow, process of finding theta consists of two steps:\n\nFind the QR decomposition of X.\nSolve triangular system R \\theta = Q^\\top y, which is triangular and, therefore, easy to solve.\n\n\n\n2.3 Cholesky decomposition\nFor any positive definite matrix A \\in \\mathbb{R}^{n \\times n} there is exists Cholesky decomposition:\n\nX^\\top X = A = L^\\top \\cdot L,\n\nwhere L is an lower triangular matrix. We have:\n\nL^\\top L\\theta = y \\quad \\longrightarrow \\quad L^\\top z_\\theta = y\n\nNow, process of finding theta consists of two steps:\n\nFind the Cholesky decomposition of X^\\top X.\nFind the z_\\theta = L\\theta by solving triangular system L^\\top z_\\theta = y\nFind the \\theta by solving triangular system L\\theta = z_\\theta\n\nNote, that in this case the error stil proportional to the squared condition number.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/least_squares.html#code",
    "href": "docs/applications/least_squares.html#code",
    "title": "Linear least squares",
    "section": "3 Code",
    "text": "3 Code\nOpen In Colab{: .btn } ## References * CVXPY documentation * Interactive example * Jupyter notebook by A. Katrutsa",
    "crumbs": [
      "Applications",
      "Linear least squares"
    ]
  },
  {
    "objectID": "docs/applications/index.html",
    "href": "docs/applications/index.html",
    "title": "Applications",
    "section": "",
    "text": "This section contains self-sufficient examples of numerical applications with Python code. Most of the examples are covered in the Boyd book. There are also some very useful link about applications with code:\n\nCVXOPT examples\nCVXPY examples\nAlexandr Katrutsa demos\n\n\n\n\n\n\n\n\n\nA^* algorithm for path finding\n\n\n\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nKnapsack problem\n\n\n\n\n\n\n\n\n\n\nLinear least squares\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood estimation\n\n\n\n\n\n\n\n\n\n\nMinimum volume ellipsoid\n\n\n\n\n\n\n\n\n\n\nNeural Network Loss Surface Visualization\n\n\n\n\n\n\n\n\n\n\nNeural network Lipschitz constant\n\n\n\n\n\n\n\n\n\n\nPrincipal component analysis\n\n\n\n\n\n\n\n\n\n\nRendezvous problem\n\n\n\n\n\n\n\n\n\n\nTotal variation in-painting\n\n\n\n\n\n\n\n\n\n\nTravelling salesman problem\n\n\n\n\n\n\n\n\n\n\nTwo way partitioning problem\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html",
    "href": "docs/applications/deep_learning.html",
    "title": "Deep learning",
    "section": "",
    "text": "Illustration\n\n\nA lot of practical tasks nowadays are being solved using the deep learning approach, which is usually implies finding local minimum of a non-convex function, that generalizes well (enough 😉). The goal of this short text is to show you the importance of the optimization behind neural network training.\n\n\nOne of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html#problem",
    "href": "docs/applications/deep_learning.html#problem",
    "title": "Deep learning",
    "section": "",
    "text": "Illustration\n\n\nA lot of practical tasks nowadays are being solved using the deep learning approach, which is usually implies finding local minimum of a non-convex function, that generalizes well (enough 😉). The goal of this short text is to show you the importance of the optimization behind neural network training.\n\n\nOne of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html#code",
    "href": "docs/applications/deep_learning.html#code",
    "title": "Deep learning",
    "section": "2 Code",
    "text": "2 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/deep_learning.html#references",
    "href": "docs/applications/deep_learning.html#references",
    "title": "Deep learning",
    "section": "3 References",
    "text": "3 References\n\nOptimization for Deep Learning Highlights in 2017\nAn overview of gradient descent optimization algorithms",
    "crumbs": [
      "Applications",
      "Deep learning"
    ]
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html",
    "href": "docs/applications/NN_Loss_Surface.html",
    "title": "Neural Network Loss Surface Visualization",
    "section": "",
    "text": "Let’s consider the training of our neural network by solving the following optimization problem:\n\n\\mathcal{L} (\\theta) \\to \\min_{\\theta \\in \\mathbb{R}^p}\n\nWe denote the initial point as \\theta_0, representing the weights of the neural network at initialization. The weights after training are denoted as \\hat{\\theta}.\nIn the given example, we have p = 105,866, which implies that we are seeking a minimum in a 105,866-dimensional space. Exploring this space is intriguing, and the underlying concept is as follows.\nInitially, we generate a random Gaussian direction w_1 \\in \\mathbb{R}^p, which inherits the magnitude of the original neural network weights for each parameter group. Subsequently, we sample the training and testing loss surfaces at points along the direction w_1, situated close to either \\theta_0 or \\hat{\\theta}.\nMathematically, this involves evaluating:\n\n\\mathcal{L} (\\alpha) = \\mathcal{L} (\\theta_0 + \\alpha w_1), \\text{ where } \\alpha \\in [-b, b].\n\nHere, \\alpha plays the role of a coordinate along the w_1 direction, and b stands for the bounds of interpolation. Visualizing \\mathcal{L} (\\alpha) enables us to project the p-dimensional surface onto a one-dimensional axis.\nIt is important to note that the characteristics of the resulting graph heavily rely on the chosen projection direction. It’s not feasible to maintain the entirety of the informationWhen transforming a space with 100,000 dimensions into a one-dimensional line through projection. However, certain properties can still be established. For instance, if \\mathcal{L} (\\alpha) \\mid_{\\alpha=0} is decreasing, this indicates that the point lies on a slope. Additionally, if the projection is non-convex, it implies that the original surface was not convex.\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Neural Network Loss Surface Visualization"
    ]
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html#scalar-projection",
    "href": "docs/applications/NN_Loss_Surface.html#scalar-projection",
    "title": "Neural Network Loss Surface Visualization",
    "section": "",
    "text": "Let’s consider the training of our neural network by solving the following optimization problem:\n\n\\mathcal{L} (\\theta) \\to \\min_{\\theta \\in \\mathbb{R}^p}\n\nWe denote the initial point as \\theta_0, representing the weights of the neural network at initialization. The weights after training are denoted as \\hat{\\theta}.\nIn the given example, we have p = 105,866, which implies that we are seeking a minimum in a 105,866-dimensional space. Exploring this space is intriguing, and the underlying concept is as follows.\nInitially, we generate a random Gaussian direction w_1 \\in \\mathbb{R}^p, which inherits the magnitude of the original neural network weights for each parameter group. Subsequently, we sample the training and testing loss surfaces at points along the direction w_1, situated close to either \\theta_0 or \\hat{\\theta}.\nMathematically, this involves evaluating:\n\n\\mathcal{L} (\\alpha) = \\mathcal{L} (\\theta_0 + \\alpha w_1), \\text{ where } \\alpha \\in [-b, b].\n\nHere, \\alpha plays the role of a coordinate along the w_1 direction, and b stands for the bounds of interpolation. Visualizing \\mathcal{L} (\\alpha) enables us to project the p-dimensional surface onto a one-dimensional axis.\nIt is important to note that the characteristics of the resulting graph heavily rely on the chosen projection direction. It’s not feasible to maintain the entirety of the informationWhen transforming a space with 100,000 dimensions into a one-dimensional line through projection. However, certain properties can still be established. For instance, if \\mathcal{L} (\\alpha) \\mid_{\\alpha=0} is decreasing, this indicates that the point lies on a slope. Additionally, if the projection is non-convex, it implies that the original surface was not convex.\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Neural Network Loss Surface Visualization"
    ]
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html#two-dimensional-projection",
    "href": "docs/applications/NN_Loss_Surface.html#two-dimensional-projection",
    "title": "Neural Network Loss Surface Visualization",
    "section": "2 Two dimensional projection",
    "text": "2 Two dimensional projection\nWe can explore this idea further and draw the projection of the loss surface to the plane, which is defined by 2 random vectors. Note, that with 2 random gaussian vectors in the huge dimensional space are almost certainly orthogonal.\nSo, as previously, we generate random normalized gaussian vectors w_1, w_2 \\in \\mathbb{R}^p and evaluate the loss function\n\n\\mathcal{L} (\\alpha, \\beta) = \\mathcal{L} (\\theta_0 + \\alpha w_1 + \\beta w_2), \\text{ where } \\alpha, \\beta \\in [-b, b]^2.\n\nwhich immediately leads us to the following nice pictures:",
    "crumbs": [
      "Applications",
      "Neural Network Loss Surface Visualization"
    ]
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html#code",
    "href": "docs/applications/NN_Loss_Surface.html#code",
    "title": "Neural Network Loss Surface Visualization",
    "section": "3 Code",
    "text": "3 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Neural Network Loss Surface Visualization"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html",
    "href": "docs/applications/A-Star.html",
    "title": "A^* algorithm for path finding",
    "section": "",
    "text": "The graph is one of the most significant structures in the algorithms, because this structure can represent many real life cases, from streets to networks.\nAnd one is the most popular problem is: Find the least sum of graph edges for given start and end points\nGenerally, we need determine input and output data:\n- Input data: graph map and end or start point/node (or both for certain path) - Output data: paths (or intermediate points/nodes) with the least sum of graph edges as result",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#problem",
    "href": "docs/applications/A-Star.html#problem",
    "title": "A^* algorithm for path finding",
    "section": "",
    "text": "The graph is one of the most significant structures in the algorithms, because this structure can represent many real life cases, from streets to networks.\nAnd one is the most popular problem is: Find the least sum of graph edges for given start and end points\nGenerally, we need determine input and output data:\n- Input data: graph map and end or start point/node (or both for certain path) - Output data: paths (or intermediate points/nodes) with the least sum of graph edges as result",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#solutions",
    "href": "docs/applications/A-Star.html#solutions",
    "title": "A^* algorithm for path finding",
    "section": "2 Solutions",
    "text": "2 Solutions\nToday there is a variety of algorithms for solving this problem, and solutions have their own advantages and disadvantages regarding the task, so let’s consider main of them:\n\n2.1 Breadth First Search\nThis is the simplest algorithm for graph traversing. It starts at the tree root (it may be start/end node) and explores all the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n\n\n\n\n\n\n\n\nOrigin Graph\nResult Tree\nAnimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously this algorithm has low performance: O(\\vert V \\vert + \\vert E\\vert) = O(b^d), where b is branch factor (average quantity of children nodes in tree, e.g. for binary tree b=2) and d is depth/distance from root.\n\n\n2.2 Dijkstra’s algorithm\n\n\n\nDescription\nAnimation\n\n\n\n\nDijkstra’s Algorithm (also called Uniform Cost Search) lets us prioritize which paths to explore. Instead of exploring all possible paths equally (like in Breadth First Search), it favors lower cost paths.\n_________________________\n\n\n\n\n\n2.3 Greedy Best-First-Search\nWith Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. Let’s make the frontier expand towards the goal more than it expands in other directions. First, we’ll define a heuristic function that tells us how close we are to the goal. E.g. on flat map we can use function like H(A, B) = |A.x - B.x| + |A.y - B.y| , where A and B are nodes with coordinates {x, y}. Let’s consider not only shortest edges, but also use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first.\n\n\n\n\n\n\n\nResult of Heuristic function\nAnimation\n\n\n\n\nWe can see that firstly nodes, that are closer to target are considered at first. But when algorithm finds a barrier, then it tries to find the path to walk around, but this path is best from the corner, not from the start position, so the result path is not the shortest. This is a result of the heuristic function. To solve this problem let’s consider next algorithm\n_________________________\n\n\n\n\n\n2.4 A-Star Algorithm\nDijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal.\n\n\n\n\n\n\n\nResult of Cost and Heuristic function\nAnimation\n\n\n\n\nBecause of considering both cost and result of heuristic functuion as result metric for Dijkstra’s algorithm, we can find the shortest path faster, than raw Dijkstra’s algorithm, and precisely, than Greedy Best-First-Search\n_________________________\n\n\n\n\n\n2.5 A-Star Implementation\nLet’s take a closer look at this algorithm and analyze it with code example. First of all you need to create a Priority Queue because you should consider points, which are closer to destination from start position. Priority does not equal cost. This Queue contains possible points, that are to be considered as possible shortest way to destination.\n## Only main methods\nclass PriorityQueue:\n    # Puts item in collection, sorted by priority.\n    def put(self, item, priority):\n    # Returns the most priority item.\n    def get(self):\nAlso you need a class, that describes your Graph Model with 2 methods. First finds neighbors of current node, and second returns cost between current node and next. This methods allows to implement any structure, be neither grid, hexagonal map or graph.\n## Only main methods\nclass SquareGrid:\n    # Returns neigbours of 'id' cell\n    # according to map and 'walls'.\n    def neighbors(self, id):\n    # Returns cost (or distance) between 2 cells.\n    # Applicable for neighbors only.\n    def cost(self, current, next):\nAlso you need to add your heuristic function too. Because, e.i. on a grid, a cost is always equals to 1 (if you don’t use diagonals), so it would be like a Breadth First Search, but you know a destination point, so you can use direction.\ndef heuristic(a, b):\n    (x1, y1) = a\n    (x2, y2) = b\n    return ((x1 - x2)**2 + (y1 - y2)**2)**.5\nNow we can implement our A-Star algorithm. First of all we need to init our algorithm: frontier stores points according to priority. We will store information in dictionaries:\n\ncame_from like pair &lt;TO point : FROM point&gt;\ncost_so_far like pair &lt;Point : Distance from start&gt;\n\nFirstly add our start point to them. Than, for each point (temporary as origin we find its neighbors and for each calculate the cost as: cost from origin to neighbor. If there is no information about this node in Queue or the cost is less than before, that add this point to queue with priority = cost + heuristic. Last step allows to consider more closer point to destination at first.\ndef  a_star_search(graph, start, goal):\n    ## Create a queue and add start point\n    frontier = PriorityQueue()\n    frontier.put(start,  0)\n    # Dictionaries with init for start point\n    came_from = {}\n    cost_so_far = {}\n    came_from[start] = None\n    cost_so_far[start] = 0\n    # Not all neighbors are visited\n    while  not frontier.empty():\n        # Get next node (firstly it is start one) \n        current = frontier.get()\n        if current == goal:\n            break\n        # Find all neighbor nodes\n        for  next  in graph.neighbors(current):\n            new_cost = cost_so_far[current] + graph.cost(current,  next)\n            # Not visited or cost to it is less\n            if  next  not  in cost_so_far or new_cost &lt; cost_so_far[next]:\n                cost_so_far[next] = new_cost\n                priority = new_cost + heuristic(goal,  next)\n                frontier.put(next, priority)\n                came_from[next] = current\n    return came_from, cost_so_far",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#results",
    "href": "docs/applications/A-Star.html#results",
    "title": "A^* algorithm for path finding",
    "section": "3 Results",
    "text": "3 Results\nNow let’s try to compare Dijkstra’s algorithm with A-Star. For this task we will generate map with size from 5 to 50 with step equal 3. Start position is in left top corner, and End position is opposite. Also, we will generate corners (the quantity is SIZE^0.4), with random length for one side and other side to the end of the map. Generated Maps you can find below, there is only example and comparison plot of iterations depending on the map size.\n\n\n\n\n\n\n\nDijkstras\nA-Star\n\n\n\n\n\n\n\n\nDue to the fact that the algorithm does not know the final position, it considers all possible directions, including dead ends, which affects the number of iterations\nDue to the fact that the algorithm knows the final position, it first considers those points that are closest to the target, so in some cases it does not go into false dead ends\n\n\n\n\n\n\nComparison\n\n\nIt is seen that in most cases A^* finds faster. However, there are situations where heuristics do not help, and in this case A-Star works the same way as Dijkstra’s.",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#code",
    "href": "docs/applications/A-Star.html#code",
    "title": "A^* algorithm for path finding",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/A-Star.html#references",
    "href": "docs/applications/A-Star.html#references",
    "title": "A^* algorithm for path finding",
    "section": "5 References",
    "text": "5 References\n\nArtificial Intelligence: A New Synthesis\nIntroduction_to_algorithms\nCS627 lecture\nTutorial\nWikipedia\nStanford Game Programming 1\nStanford Game Programming 2\nHabr",
    "crumbs": [
      "Applications",
      "$A^*$ algorithm for path finding"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html",
    "href": "docs/applications/MLE.html",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "We need to estimate probability density p(x) of a random variable from observed values.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html#problem",
    "href": "docs/applications/MLE.html#problem",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "We need to estimate probability density p(x) of a random variable from observed values.\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html#approach",
    "href": "docs/applications/MLE.html#approach",
    "title": "Maximum likelihood estimation",
    "section": "2 Approach",
    "text": "2 Approach\nWe will use idea of parametric distribution estimation, which involves choosing the best parameters, of a chosen family of densities p_\\theta(x), indexed by a parameter \\theta. The idea is very natural: we choose such parameters, which maximizes the probability (or logarithm of probability) of observed values.\n\n\\arg \\max\\limits_{\\theta} \\log p_\\theta(x) = \\theta^*\n\n\n2.1 Linear measurements with i.i.d. noise\nSuppose, we are given the set of observations:\n\nx_i = \\theta^\\top a_i + \\xi_i, \\quad i = [1,m],\n\nwhere\n\n\\theta \\in \\mathbb{R}^n - unknown vector of parameters\n\\xi_i are IID noise random variables with density p(z)\nx_i - measurements, x \\in \\mathbb{R}^m\n\nWhich implies the following optimization problem:\n\n\\max\\limits_{\\theta} \\log p(x) = \\max_\\theta \\sum\\limits_{i=1}^m \\log p (x_i - \\theta^\\top a_i) = \\max_\\theta L(\\theta)\n\nWhere the sum goes from the fact, that all observation are independent, which leads to the fact, that p(\\xi) = \\prod\\limits_{i=1}^m p(\\xi_i). The target function is called log-likelihood function L(\\theta).\n\n2.1.1 Gaussian noise\n\np(z) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{z^2}{2 \\sigma^2}}\n\n\n\\log p(z) = - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{z^2}{2 \\sigma^2}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{(x_i - \\theta^\\top a_i)^2}{2 \\sigma^2} \\right] \\\\\n&= - \\dfrac{m}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{1}{2 \\sigma^2} \\sum\\limits_{i=1}^m (x_i - \\theta^\\top a_i)^2\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of gaussian noise is a least squares solution.\n\n\n2.1.2 Laplacian noise\n\np(z) = \\dfrac{1}{2a} e^{-\\frac{|z|}{a}}\n\n\n\\log p(z) = -  \\log (2a) - -\\dfrac{|z|}{a}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\log (2a) - -\\dfrac{|(x_i - \\theta^\\top a_i)|}{a} \\right] \\\\\n&= - m \\log (2 a) - \\dfrac{1}{a} \\sum\\limits_{i=1}^m |x_i - \\theta^\\top a_i|\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of Laplacian noise is a l_1-norm solution.\n\n\n2.1.3 Uniform noise\n\np(z) = \\begin{cases}\n  \\frac{1}{2a}, & -a \\leq z \\leq a, \\\\\n  0, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n\n\\log p(z) =  \\begin{cases}\n  - \\log(2a), & -a \\leq z \\leq a, \\\\\n  -\\infty, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n$$\nL() =\n\\begin{cases}\n  - m\\log(2a), & |x_i - \\theta^\\top a_i| \\leq a, \\\\\n  -\\infty, &  \\text{ otherwise }\n  \\end{cases}\n$$\nWhich means, the maximum likelihood estimation in case of uniform noise is any vector \\theta, which satisfies \\vert x_i - \\theta^\\top a_i \\vert \\leq a.\n\n\n\n2.2 Binary logistic regression\nSuppose, we are given a set of binary random variables y_i \\in \\{0,1\\}. Let us parametrize the distribution function as a sigmoid, using linear transformation of the input as an argument of a sigmoid.\n\n\n\nPicture from Wikipedia\n\n\n\n\\begin{split}\np(y_i = 1) &= \\dfrac{\\text{exp}(\\theta_0^\\top x_i + \\theta_1)}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)} \\\\\np(y_i = 0) &= \\dfrac{1}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)}\n\\end{split}\n\nLet’s assume, that first k observations are ones: y_1, \\ldots, y_k =1, y_{k+1}, \\ldots, y_m = 0. Then, log-likelihood function will be written as follows:\n\nL(\\theta_0, \\theta_1) = \\sum\\limits_{i=1}^k (\\theta_0^\\top x_i + \\theta_1) - \\sum\\limits_{i=1}^m \\log(1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1))",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/MLE.html#references",
    "href": "docs/applications/MLE.html#references",
    "title": "Maximum likelihood estimation",
    "section": "3 References",
    "text": "3 References\n\nConvex Optimization @ UCLA by Prof. L. Vandenberghe\nNumerical explanation",
    "crumbs": [
      "Applications",
      "Maximum likelihood estimation"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html",
    "href": "docs/applications/Neural_Lipschitz_constant.html",
    "title": "Neural network Lipschitz constant",
    "section": "",
    "text": "It was observed, that small perturbation in Neural Network input could lead to significant errors, i.e. misclassifications.\n\n\n\nTypical illustration of adversarial attacks in image domain. Source\n\n\nLipschitz constant bounds the magnitude of the output of a function, so it cannot change drastically with a slight change in the input\n\n\\|\\mathcal{NN}(image) - \\mathcal{NN}(image+\\varepsilon)\\| \\leq L_{\\mathcal{NN}}\\|\\varepsilon\\|\n\nNote, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, \\text{ReLU }(x)):\n\n\\mathcal{NN}(x) = f_L \\circ w_L \\circ \\ldots \\circ f_1 \\circ w_1 \\circ x,\n\nwhere L is the number of layers, f_i - non-linear activation function, w_i = W_i x + b_i - linear layer.",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#lipschitz-constant-and-robustness-to-a-small-perturbation",
    "href": "docs/applications/Neural_Lipschitz_constant.html#lipschitz-constant-and-robustness-to-a-small-perturbation",
    "title": "Neural network Lipschitz constant",
    "section": "",
    "text": "It was observed, that small perturbation in Neural Network input could lead to significant errors, i.e. misclassifications.\n\n\n\nTypical illustration of adversarial attacks in image domain. Source\n\n\nLipschitz constant bounds the magnitude of the output of a function, so it cannot change drastically with a slight change in the input\n\n\\|\\mathcal{NN}(image) - \\mathcal{NN}(image+\\varepsilon)\\| \\leq L_{\\mathcal{NN}}\\|\\varepsilon\\|\n\nNote, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, \\text{ReLU }(x)):\n\n\\mathcal{NN}(x) = f_L \\circ w_L \\circ \\ldots \\circ f_1 \\circ w_1 \\circ x,\n\nwhere L is the number of layers, f_i - non-linear activation function, w_i = W_i x + b_i - linear layer.",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#estimating-lipschitz-constant-of-a-neural-network",
    "href": "docs/applications/Neural_Lipschitz_constant.html#estimating-lipschitz-constant-of-a-neural-network",
    "title": "Neural network Lipschitz constant",
    "section": "2 Estimating Lipschitz constant of a neural network",
    "text": "2 Estimating Lipschitz constant of a neural network\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nAn everywhere differentiable function f: \\mathbb{R} \\to \\mathbb{R} is Lipschitz continuous with L = \\sup |\\nabla f(x)| if and only if it has bounded first derivative.\n\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nIf f = g_1 \\circ g_2, then L_f \\leq L_{g_1} L_{g_2}\n\n\n\n\nTherefore, we can bound the Lipschitz constant of a neural network:\n\nL_{\\mathcal{NN}} \\leq L_{f_1} \\ldots L_{f_L} L_{w_1} \\ldots L_{w_L}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet’s consider one the simplest non-liear activation function.\n\nf(x) = \\text{ReLU}(x)\n\n\nIts Lipschitz L_f constant equals to 1, because:\n\n\\|f(x) - f(y)\\| \\leq 1 \\|x-y\\|\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat is the Lipschitz constant of a linear layer of neural network\n\nw(x) = Wx + b\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\\|w(x) - w(y)\\| = \\|Wx + b - (Wy + b)\\|_2 = \\|W(x-y)\\|_2 \\leq \\|W\\|_2 \\|x-y\\|_2\n Therefore, L_w = \\|W\\|_2",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#how-to-compute-w_2",
    "href": "docs/applications/Neural_Lipschitz_constant.html#how-to-compute-w_2",
    "title": "Neural network Lipschitz constant",
    "section": "3 How to compute \\|W\\|_2?",
    "text": "3 How to compute \\|W\\|_2?\nLet W = U \\Sigma V^T – SVD of the matrix W \\in \\mathbb{R}^{m \\times n}. Then\n\n\\|W\\|_2 = \\sup_{x \\ne 0} \\frac{\\| W x \\|_2}{\\| x \\|_{2}} = \\sigma_1(W) = \\sqrt{\\lambda_\\text{max} (W^*W)}\n\nFor m = n, computing SVD is \\mathcal{O}(n^3).\n\nTime measurements with jax and Google colab CPU.\n\n\nn\n10\n100\n1000\n5000\n\n\n\n\nTime\n38.7 µs\n3.04 ms\n717 ms\n1min 21s\n\n\nMemory for W in fp32\n0.4 KB\n40 KB\n4 MB\n95 MB\n\n\n\nWorks only for small linear layers.\nIn this notebook we will try to estimate Lipschitz constant of some convolutional layer of a Neural Network.",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#convolutional-layer",
    "href": "docs/applications/Neural_Lipschitz_constant.html#convolutional-layer",
    "title": "Neural network Lipschitz constant",
    "section": "4 Convolutional layer",
    "text": "4 Convolutional layer\n\n\n\nAnimation of Convolution operation. Source\n\n\nSuppose, that we have an input X and the convolutional layer C with the filter size k \\times k. Here we assume, that p_1, p_2 - are the indices of pixels of the kernel, while q_1, q_2 are the indices of pixels of the output.\n\nC \\circ X = \\sum_{p_1 = 0}^{k-1}\\sum_{p_2 = 0}^{k-1} C_{p_1, p_2} X_{p_1 + q_1, p_2 + q_2}\n\nWhile multichannel convolution could be written in the following form:\n\nY_j = \\sum_{i = 1}^{m_{in}} C_{:, :, i, j} \\circ X_{:, :, i},\n\nwhere\n\nC \\in  \\mathbb{R}^{k \\times k \\times m_{in} \\times m_{out}} – convolution kernel\nk – filter size\nm_{in} – number of input channels (e.g., 3 for RGB)\nm_{out} – number of output channels.\n\nIt is easy to see, that the output of the multichannel convolution operation is linear w.r.t. the input X, which actually means, that one can write it as matvec:\n\n\\text{vec }(Y) = W \\text{vec }(X)\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat is the size of the matrix W in this case, if the input image is square of size n?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nW \\in \\mathbb{R}^{m_{out}n^2 \\times m_{in}n^2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nIf the image size is n = 224, number if input and output channels are m_{in} = m_{out} = 64, then W \\in \\mathbb{R}^{3 211 264 \\times 3 211 264}. Storing this matrix in fp32 requW^T W x_kires approximately 40 Gb of RAM.\n\n\n\n\nIt seems, that computing \\|W\\|_2 is almost impossible, isn’t it?",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#power-method-for-computing-the-largest-singular-value",
    "href": "docs/applications/Neural_Lipschitz_constant.html#power-method-for-computing-the-largest-singular-value",
    "title": "Neural network Lipschitz constant",
    "section": "5 Power method for computing the largest singular value",
    "text": "5 Power method for computing the largest singular value\nSince we are interested in the largest singular value and\n\n\\|W\\|_2 = \\sigma_1(W) = \\sqrt{\\lambda_\\text{max} (W^*W)}\n\nwe can apply the power method\n\nx_{k+1} = \\dfrac{W^T W x_k}{\\|W^T W x_k\\|_2}\n\n\n\\sigma_{k+1} = \\dfrac{W x_{k+1}}{\\|x_{k+1}\\|_2}",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#code",
    "href": "docs/applications/Neural_Lipschitz_constant.html#code",
    "title": "Neural network Lipschitz constant",
    "section": "6 Code",
    "text": "6 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html#references",
    "href": "docs/applications/Neural_Lipschitz_constant.html#references",
    "title": "Neural network Lipschitz constant",
    "section": "7 References",
    "text": "7 References\n\nMaxim Rakhuba’s lecture on Matrix and tensor methods in ML.",
    "crumbs": [
      "Applications",
      "Neural network Lipschitz constant"
    ]
  },
  {
    "objectID": "docs/applications/ellipsoid.html",
    "href": "docs/applications/ellipsoid.html",
    "title": "Minimum volume ellipsoid",
    "section": "",
    "text": "Illustration\n\n\nLet x_1, \\ldots, x_n be the points in \\mathbb{R}^2. Given these points we need to find an ellipsoid, that contains all points with the minimum volume (in 2d case volume of an ellipsoid is just the square).\nAn invertible linear transformation applied to a unit sphere produces an ellipsoid with the square, that is \\det A^{-1} times bigger, than the unit sphere square, that’s why we parametrize the interior of ellipsoid in the following way:\n\nS = \\{x \\in \\mathbb{R}^2 \\; | \\; u = Ax + b, \\|u\\|_2^2 \\leq 1\\}\n\nSadly, the determinant is the function, which is relatively hard to minimize explicitly. However, the function \\log \\det A^{-1} = -\\log \\det A is actually convex, which provides a great opportunity to work with it. As soon as we need to cover all the points with ellipsoid of minimum volume, we pose an optimization problem on the convex function with convex restrictions:\n\n\\begin{align*}\n& \\min_{A \\in \\mathbb{R}^{2 \\times 2}, b \\in \\mathbb{R}^{2}} -\\log\\det(A)\\\\\n\\text{s.t. } & \\|Ax_i + b\\| \\leq 1, i = 1, \\ldots, n\\\\\n& A \\succ 0\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Minimum volume ellipsoid"
    ]
  },
  {
    "objectID": "docs/applications/ellipsoid.html#problem",
    "href": "docs/applications/ellipsoid.html#problem",
    "title": "Minimum volume ellipsoid",
    "section": "",
    "text": "Illustration\n\n\nLet x_1, \\ldots, x_n be the points in \\mathbb{R}^2. Given these points we need to find an ellipsoid, that contains all points with the minimum volume (in 2d case volume of an ellipsoid is just the square).\nAn invertible linear transformation applied to a unit sphere produces an ellipsoid with the square, that is \\det A^{-1} times bigger, than the unit sphere square, that’s why we parametrize the interior of ellipsoid in the following way:\n\nS = \\{x \\in \\mathbb{R}^2 \\; | \\; u = Ax + b, \\|u\\|_2^2 \\leq 1\\}\n\nSadly, the determinant is the function, which is relatively hard to minimize explicitly. However, the function \\log \\det A^{-1} = -\\log \\det A is actually convex, which provides a great opportunity to work with it. As soon as we need to cover all the points with ellipsoid of minimum volume, we pose an optimization problem on the convex function with convex restrictions:\n\n\\begin{align*}\n& \\min_{A \\in \\mathbb{R}^{2 \\times 2}, b \\in \\mathbb{R}^{2}} -\\log\\det(A)\\\\\n\\text{s.t. } & \\|Ax_i + b\\| \\leq 1, i = 1, \\ldots, n\\\\\n& A \\succ 0\n\\end{align*}\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Minimum volume ellipsoid"
    ]
  },
  {
    "objectID": "docs/applications/ellipsoid.html#code",
    "href": "docs/applications/ellipsoid.html#code",
    "title": "Minimum volume ellipsoid",
    "section": "2 Code",
    "text": "2 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Minimum volume ellipsoid"
    ]
  },
  {
    "objectID": "docs/applications/ellipsoid.html#references",
    "href": "docs/applications/ellipsoid.html#references",
    "title": "Minimum volume ellipsoid",
    "section": "3 References",
    "text": "3 References\n\nJupyter notebook by A. Katrutsa\nhttps://cvxopt.org/examples/book/ellipsoids.html",
    "crumbs": [
      "Applications",
      "Minimum volume ellipsoid"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html",
    "href": "docs/applications/knapsack_problem.html",
    "title": "Knapsack problem",
    "section": "",
    "text": "The knapsack problem or rucksack problem is a problem in combinatorial optimization. Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\n\n\nWikipedia\n\n\n\n\nThe most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases.",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#introduction",
    "href": "docs/applications/knapsack_problem.html#introduction",
    "title": "Knapsack problem",
    "section": "",
    "text": "The knapsack problem or rucksack problem is a problem in combinatorial optimization. Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\n\n\nWikipedia\n\n\n\n\nThe most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases.",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#approaches",
    "href": "docs/applications/knapsack_problem.html#approaches",
    "title": "Knapsack problem",
    "section": "2 Approaches",
    "text": "2 Approaches\n\n2.1 Exact solutions\n\n2.1.1 Full search\nAs for other discrete tasks, the backpack problem can be solved by completely sorting through all possible solutions. Suppose there are n items that can be packed in a backpack. It is necessary to determine the maximum value of the cargo, whose weight does not exceed W. For each item, there are 2 options: the item is either put in a backpack or not. Then enumeration of all possible options has time complexity O(2^n), which allows it to be used only for a small number of objects. With an increase in the number of objects, the task becomes unsolvable by this method in an acceptable time.\n\n\n2.1.2 Dynamic programming algorithm\nA similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume w_{1},\\,w_{2},\\,\\ldots ,\\,w_{n}, W are strictly positive integers. Define m(i,w) to be the maximum value that can be attained with weight less than or equal to w using items up to i (first i items). We can define m(i,w) recursively as follows:\n\nm(i, w) =\n\\begin{cases}\n   m(0,w)=0  \\\\\n   m(i,w)=m(i-1,w) &\\text{$w_{i}&gt;w$ } \\\\\n   m(i,w)=max(m(i-1,w), m(i-1,w-w_{i})+w_{i}) &\\text{$w_{i} \\le w$ }\n\\end{cases}\n\nThe solution can then be found by calculating m(n,W). To do this efficiently, we can use a table to store previous computations. This solution will therefore run in O(nW) time and O(nW) space.\n\n\n\n2.2 Approximation algorithms\n\n2.2.1 Greedy algorithm\nTo solve the problem by the greedy algorithm, it is necessary to sort things by their specific value (that is, the ratio of the value of an item to its weight), and put items with the highest specific value in a backpack. The running time of this algorithm is the sum of the sorting time and stacking time. The difficulty in sorting items is O(N \\ log (N)). Next, the calculation of how many items fit in a backpack for the total time O(N). Total complexity O(N \\ log (N)) if necessary sorting and O(N) if already sorted data. It should be understood that a greedy algorithm can lead to an answer arbitrarily far from optimal. For example, if one item has a weight of 1 and a cost of 2, and another has a weight of W and a cost of W, then the greedy algorithm will pick up the final cost of 2 with the optimal answer W.\n\n\n2.2.2 Probabilistic algorithm\nIt is a modification of greedy algorithm. In this algorithm decision to include an item with index j in the knapsack is taken based on th probability of \\frac{\\lambda_j}{\\sum_{i=1}^n\\lambda_i}, where \\lambda_i is the ratio of the value of an item to its weight. This algorithm is run several times and the best solution is selected. If Algorithm starts as many times as you like, then the probability of getting it as a result the work of the optimal solution tends to 1. Total complexity is equal to O(mN  log(N)) operations. Calculating experiment The above algorithms were implemented in the python programming language, their source codes are available at the link below. Initial data for the task, namely the values (v_{i}) and weights (w_{i}) of items were randomly generated. The following ranges of values have been selected v_{i}\\in [0, 100] and w_{i}\\in [100, 200]. Maximum weight capacity W generated randomly from interval W \\in [0.5\\sum w_i, 0.75\\sum w_i].",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#results",
    "href": "docs/applications/knapsack_problem.html#results",
    "title": "Knapsack problem",
    "section": "3 Results",
    "text": "3 Results\nDynamic programming algorithm vs probabilistic algorithm:\n\n\n\nknapsack_problem_1\n\n\n\n\n\nknapsack_problem_2\n\n\nProbabilistic algorithm in case when number of items is fixed (150 items):\n\n\n\nknapsack_problem_3",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#code",
    "href": "docs/applications/knapsack_problem.html#code",
    "title": "Knapsack problem",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#references",
    "href": "docs/applications/knapsack_problem.html#references",
    "title": "Knapsack problem",
    "section": "5 References",
    "text": "5 References\n\nScheduling theory\nWiki",
    "crumbs": [
      "Applications",
      "Knapsack problem"
    ]
  },
  {
    "objectID": "docs/applications/pca.html",
    "href": "docs/applications/pca.html",
    "title": "Principal component analysis",
    "section": "",
    "text": "Imagine, that you have a dataset of points. Your goal is to choose orthogonal axes, that describe your data the most informative way. To be precise, we choose first axis in such a way, that maximize the variance (expressiveness) of the projected data. All the following axes have to be orthogonal to the previously chosen ones, while satisfy largest possible variance of the projections.\nLet’s take a look at the simple 2d data. We have a set of blue points on the plane. We can easily see that the projections on the first axis (red dots) have maximum variance at the final position of the animation. The second (and the last) axis should be orthogonal to the previous one.\n source\nThis idea could be used in a variety of ways. For example, it might happen, that projection of complex data on the principal plane (only 2 components) bring you enough intuition for clustering. The picture below plots projection of the labeled dataset onto the first to principal components (PCs), we can clearly see, that only two vectors (these PCs) would be enough to differ Finnish people from Italian in particular dataset (celiac disease (Dubois et al. 2010))  source",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#intuition",
    "href": "docs/applications/pca.html#intuition",
    "title": "Principal component analysis",
    "section": "",
    "text": "Imagine, that you have a dataset of points. Your goal is to choose orthogonal axes, that describe your data the most informative way. To be precise, we choose first axis in such a way, that maximize the variance (expressiveness) of the projected data. All the following axes have to be orthogonal to the previously chosen ones, while satisfy largest possible variance of the projections.\nLet’s take a look at the simple 2d data. We have a set of blue points on the plane. We can easily see that the projections on the first axis (red dots) have maximum variance at the final position of the animation. The second (and the last) axis should be orthogonal to the previous one.\n source\nThis idea could be used in a variety of ways. For example, it might happen, that projection of complex data on the principal plane (only 2 components) bring you enough intuition for clustering. The picture below plots projection of the labeled dataset onto the first to principal components (PCs), we can clearly see, that only two vectors (these PCs) would be enough to differ Finnish people from Italian in particular dataset (celiac disease (Dubois et al. 2010))  source",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#problem",
    "href": "docs/applications/pca.html#problem",
    "title": "Principal component analysis",
    "section": "2 Problem",
    "text": "2 Problem\nThe first component should be defined in order to maximize variance. Suppose, we’ve already normalized the data, i.e. \\sum\\limits_i a_i = 0, then sample variance will become the sum of all squared projections of data points to our vector {\\mathbf{w}}_{(1)}, which implies the following optimization problem:\n\n\\mathbf{w}_{(1)}={\\underset  {\\Vert {\\mathbf{w}}\\Vert =1}{\\operatorname{\\arg \\,max}}}\\,\\left\\{\\sum _{i}\\left({\\mathbf{a}}^{\\top}_{(i)}\\cdot {\\mathbf{w}}\\right)^{2}\\right\\}\n\nor\n\n\\mathbf{w}_{(1)}={\\underset {\\Vert \\mathbf{w} \\Vert =1}{\\operatorname{\\arg \\,max} }}\\,\\{\\Vert \\mathbf{Aw} \\Vert ^{2}\\}={\\underset {\\Vert \\mathbf{w} \\Vert =1}{\\operatorname{\\arg \\,max} }}\\,\\left\\{\\mathbf{w}^{\\top}\\mathbf{A^{\\top}} \\mathbf{Aw} \\right\\}\n\nsince we are looking for the unit vector, we can reformulate the problem:\n\n\\mathbf{w} _{(1)}={\\operatorname{\\arg \\,max} }\\,\\left\\{ \\frac{\\mathbf{w}^{\\top}\\mathbf{A^{\\top}} \\mathbf{Aw} }{\\mathbf{w}^{\\top}\\mathbf{w} }\\right\\}\n\nIt is known, that for positive semidefinite matrix A^\\top A such vector is nothing else, but eigenvector of A^\\top A, which corresponds to the largest eigenvalue. The following components will give you the same results (eigenvectors).\nSo, we can conclude, that the following mapping:\n\n\\underset{n \\times k}{\\Pi} = \\underset{n \\times d}{A} \\cdot \\underset{d \\times k}{W}\n\ndescribes the projection of data onto the k principal components, where W contains first (by the size of eigenvalues) k eigenvectors of A^\\top A.\nNow we’ll briefly derive how SVD decomposition could lead us to the PCA.\nFirstly, we write down SVD decomposition of our matrix:\n\nA = U \\Sigma W^\\top\n\nand to its transpose:\n\n\\begin{align*}\nA^\\top\n&= (U \\Sigma W^\\top)^\\top \\\\\n&= (W^\\top)^\\top \\Sigma^\\top U^\\top \\\\\n&= W \\Sigma^\\top U^\\top \\\\\n&= W \\Sigma U^\\top\n\\end{align*}\n\nThen, consider matrix A A^\\top:\n\n\\begin{align*}\nA^\\top A\n&= (W \\Sigma U^\\top)(U \\Sigma V^\\top)  \\\\\n&= W \\Sigma I \\Sigma W^\\top \\\\\n&= W \\Sigma \\Sigma W^\\top \\\\\n&= W \\Sigma^2 W^\\top\n\\end{align*}\n\nWhich corresponds to the eigendecomposition of matrix A^\\top A, where W stands for the matrix of eigenvectors of A^\\top A, while \\Sigma^2 contains eigenvalues of A^\\top A.\nAt the end:\n\n\\begin{align*}\n\\Pi &= A \\cdot W =\\\\\n&= U \\Sigma W^\\top W = U \\Sigma\n\\end{align*}\n\nThe latter formula provide us with easy way to compute PCA via SVD with any number of principal components:\n\n\\Pi_r = U_r \\Sigma_r",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#examples",
    "href": "docs/applications/pca.html#examples",
    "title": "Principal component analysis",
    "section": "3 Examples",
    "text": "3 Examples\n\n3.1 🌼 Iris dataset\nConsider the classical Iris dataset\n\n\n\nIllustration\n\n\nsource\nWe have the dataset matrix A \\in \\mathbb{R}^{150 \\times 4}\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#code",
    "href": "docs/applications/pca.html#code",
    "title": "Principal component analysis",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/pca.html#related-materials",
    "href": "docs/applications/pca.html#related-materials",
    "title": "Principal component analysis",
    "section": "5 Related materials",
    "text": "5 Related materials\n\nWikipedia\nBlog post\nBlog post",
    "crumbs": [
      "Applications",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html",
    "href": "docs/applications/salesman_problem.html",
    "title": "Travelling salesman problem",
    "section": "",
    "text": "Suppose, we have N points in \\mathbb{R}^d Euclidian space (for simplicity, we’ll consider and plot case with d=2). Let’s imagine, that these points are nothing else but houses in some 2d village. Salesman should find the shortest way to go through the all houses only once.\n\n\n\nIllustration\n\n\nThat is, very simple formulation, however, implies NP - hard problem with the factorial growth of possible combinations. The goal is to minimize the following cumulative distance:\n\nd = \\sum_{i=1}^{N-1} \\| x_{y(i+1)}  - x_{y(i)}\\|_2 \\to \\min_{y},\n\nwhere x_k is the k-th point from N and y stands for the N- dimensional vector of indicies, which describes the order of path. Actually, the problem could be formulated as an LP problem, which is easier to solve.",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#problem",
    "href": "docs/applications/salesman_problem.html#problem",
    "title": "Travelling salesman problem",
    "section": "",
    "text": "Suppose, we have N points in \\mathbb{R}^d Euclidian space (for simplicity, we’ll consider and plot case with d=2). Let’s imagine, that these points are nothing else but houses in some 2d village. Salesman should find the shortest way to go through the all houses only once.\n\n\n\nIllustration\n\n\nThat is, very simple formulation, however, implies NP - hard problem with the factorial growth of possible combinations. The goal is to minimize the following cumulative distance:\n\nd = \\sum_{i=1}^{N-1} \\| x_{y(i+1)}  - x_{y(i)}\\|_2 \\to \\min_{y},\n\nwhere x_k is the k-th point from N and y stands for the N- dimensional vector of indicies, which describes the order of path. Actually, the problem could be formulated as an LP problem, which is easier to solve.",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#genetic-evolution-algorithm",
    "href": "docs/applications/salesman_problem.html#genetic-evolution-algorithm",
    "title": "Travelling salesman problem",
    "section": "2 🧬Genetic (evolution) algorithm",
    "text": "2 🧬Genetic (evolution) algorithm\nOur approach is based on the famous global optimization algorithm, known as evolution algorithm. ### Population and individuals Firstly, we need to generate the set of random solutions as an initialization. We will call a set of solutions \\{y_k\\}_{k=1}^n as population, while each solution is called individual (or creature).\nEach creature contains integer numbers 1, \\ldots, N, which indicates the order of bypassing all the houses. The creature, that reflects the shortest path length among the others will be used as an output of an algorithm at the current iteration (generation).\n\n2.1 Crossing procedure\nEach iteration of the algorithm starts with the crossing (breed) procedure. Formally speaking, we should formulate the mapping, that takes two creature vectors as an input and returns its offspring, which inherits parents properties, while remaining consistent. We will use ordered crossover as such procedure.\n\n\n\nIllustration\n\n\n\n\n2.2 Mutation\nIn order to give our algorithm some ability to escape local minima, we provide it with mutation procedure. We simply swap some houses in an individual vector. To be more accurate, we define mutation rate (say, 0.05). On the one hand, the higher the rate, the less stable the population is, on the other, the smaller the rate, the more often algorithm gets stuck in the local minima. We choose \\text{mutation rate} \\cdot n individuals and in each case swap random \\text{mutation rate} \\cdot N digits.\n\n\n2.3 Selection\nAt the end of the iteration we have increased population (due to crossing results), then we just calculate total path distance to each individual and select top n of them.\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\nIn general, for any c &gt; 0, where d is the number of dimensions in the Euclidean space, there is a polynomial-time algorithm that finds a tour of length at most (1 + \\frac{1}{c}) times the optimal for geometric instances of TSP in\n\n\\mathcal{O}\\left(N(\\log N)^{(\\mathcal{O}(c{\\sqrt {d}}))^{d-1}}\\right)",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#code",
    "href": "docs/applications/salesman_problem.html#code",
    "title": "Travelling salesman problem",
    "section": "3 Code",
    "text": "3 Code\nOpen In Colab",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/salesman_problem.html#references",
    "href": "docs/applications/salesman_problem.html#references",
    "title": "Travelling salesman problem",
    "section": "4 References",
    "text": "4 References\n\nGeneral information about genetic algorithms\nWiki",
    "crumbs": [
      "Applications",
      "Travelling salesman problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html",
    "href": "docs/applications/two_way_partitioning.html",
    "title": "Two way partitioning problem",
    "section": "",
    "text": "Illustration\n\n\nSuppose, we have a set of n objects, which are needed to be split into two groups. Moreover, we have information about the preferences of all possible pairs of objects to be in the same group. This information could be presented in the matrix form: W \\in \\mathbb{R}^{n \\times n}, where \\{w_{ij}\\} is the cost of having i-th and j-th object in the same partitions. It is easy to see, that the total number of partitions is finite and equals to 2^n. So this problem can in principle be solved by simply checking the objective value of each feasible point. Since the number of feasible points grows exponentially, however, this is possible only for small problems (say, with n \\leq 30). In general (and for n larger than, say, 50) the problem is very difficult to solve.\nFor example, bruteforce solution on MacBook Air with M1 processor without any explicit parallelization will take more, than a universe lifetime for n=62.\n\n\n\nIllustration\n\n\nDespite the hardness of the problems, there are several ways to approach it.",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#intuition",
    "href": "docs/applications/two_way_partitioning.html#intuition",
    "title": "Two way partitioning problem",
    "section": "",
    "text": "Illustration\n\n\nSuppose, we have a set of n objects, which are needed to be split into two groups. Moreover, we have information about the preferences of all possible pairs of objects to be in the same group. This information could be presented in the matrix form: W \\in \\mathbb{R}^{n \\times n}, where \\{w_{ij}\\} is the cost of having i-th and j-th object in the same partitions. It is easy to see, that the total number of partitions is finite and equals to 2^n. So this problem can in principle be solved by simply checking the objective value of each feasible point. Since the number of feasible points grows exponentially, however, this is possible only for small problems (say, with n \\leq 30). In general (and for n larger than, say, 50) the problem is very difficult to solve.\nFor example, bruteforce solution on MacBook Air with M1 processor without any explicit parallelization will take more, than a universe lifetime for n=62.\n\n\n\nIllustration\n\n\nDespite the hardness of the problems, there are several ways to approach it.",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#problem",
    "href": "docs/applications/two_way_partitioning.html#problem",
    "title": "Two way partitioning problem",
    "section": "2 Problem",
    "text": "2 Problem\nWe consider the (nonconvex) problem\n\n\\begin{align*}\n& \\min_{x \\in \\mathbb{R}^n} x^\\top W x,\\\\\n\\text{s.t. } & x_i^2 = 1,  \\; i= 1, \\ldots, n\\\\\n\\end{align*}\n\nwhere W \\in \\mathbb{R}^n is the symetric matrix. The constraints restrict the values of x_i to 1 or −1, so the problem is equivalent to finding the vector with components \\pm 1 that minimizes x^\\top W x. The feasible set here is finite (it contains 2^n points), thus, is non-convex.\nThe objective is the total cost, over all pairs of elements, and the problem is to find the partition with least total cost.\n\n2.1 Simple lower bound with duality\nWe now derive the dual function for this problem. The Lagrangian is\n\nL(x, \\nu) = x^\\top W x + \\sum\\limits_{i=1}^n \\nu_i (x^2_i − 1) = x^\\top (W + \\text{diag}(\\nu))x − \\mathbf{1}^\\top \\nu.\n\nWe obtain the Lagrange dual function by minimizing over x:\n\n\\begin{split}\ng(\\nu) &= \\inf_{x \\in\\mathbb{R}^n} x^\\top (W + diag(\\nu))x − \\mathbf{1}^\\top \\nu = \\\\\n&= \\begin{cases}\n\\mathbf{1}^\\top \\nu,  &W + \\text{diag}(\\nu) \\succeq 0 \\\\\n-\\infty, &\\text{ otherwise} \\end{cases}\n\\end{split}\n\nThis dual function provides lower bounds on the optimal value of the difficult problem. For example, we can take any specific value of the dual variable\n\n\\nu = −\\lambda_{min}(W)\\mathbf{1},\n\nThis yields the bound on the optimal value p^*:\n\np^* \\geq g(\\nu) \\geq −\\mathbf{1}^\\top \\nu = n \\lambda_{min}(W)\n\nQuestion Can you obtain the same lower bound without knowledge of duality, but using the idea of eigenvalues?",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#code",
    "href": "docs/applications/two_way_partitioning.html#code",
    "title": "Two way partitioning problem",
    "section": "3 Code",
    "text": "3 Code\nOpen In Colab{: .btn }",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#references",
    "href": "docs/applications/two_way_partitioning.html#references",
    "title": "Two way partitioning problem",
    "section": "4 References",
    "text": "4 References\n\nConvex Optimization book by Stephen Boyd and Lieven Vandenberghe.",
    "crumbs": [
      "Applications",
      "Two way partitioning problem"
    ]
  },
  {
    "objectID": "docs/benchmarks/index.html",
    "href": "docs/benchmarks/index.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Here you can find comparison of different algorithms with respect to the different hyperparameter choice.",
    "crumbs": [
      "Benchmarks"
    ]
  },
  {
    "objectID": "docs/exercises/automatic_differentiation.html",
    "href": "docs/exercises/automatic_differentiation.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Calculate the gradient of a Taylor series of a \\cos (x) using autograd library:\nimport autograd.numpy as np # Thinly-wrapped version of Numpy \nfrom autograd import grad \n\ndef taylor_cosine(x): # Taylor approximation to cosine function \n  # Your np code here\n  return ans \nIn the following code for the gradient descent for linear regression change the manual gradient computation to the PyTorch/jax autograd way. Compare those two approaches in time.\nIn order to do this, set the tolerance rate for the function value \\varepsilon = 10^{-9}. Compare the total time required to achieve the specified value of the function for analytical and automatic differentiation. Perform measurements for different values of n from np.logspace(1,4).\nFor each n value carry out at least 3 runs.\nimport numpy as np \n\n# Compute every step manually\n\n# Linear regression\n# f = w * x \n\n# here : f = 2 * x\nX = np.array([1, 2, 3, 4], dtype=np.float32)\nY = np.array([2, 4, 6, 8], dtype=np.float32)\n\nw = 0.0\n\n# model output\ndef forward(x):\n    return w * x\n\n# loss = MSE\ndef loss(y, y_pred):\n    return ((y_pred - y)**2).mean()\n\n# J = MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(w*x - y)\ndef gradient(x, y, y_pred):\n    return np.dot(2*x, y_pred - y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 20\n\nfor epoch in range(n_iters):\n    # predict = forward pass\n    y_pred = forward(X)\n\n    # loss\n    l = loss(Y, y_pred)\n\n    # calculate gradients\n    dw = gradient(X, Y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 2 == 0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')\nCalculate the 4th derivative of hyperbolic tangent function using Jax autograd.\nCompare analytic and autograd (with any framework) approach for the hessian of:\n\nf(x) = \\dfrac{1}{2}x^TAx + b^Tx + c\n\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = tr(AXB)\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\dfrac{1}{2} \\|Ax - b\\|^2_2\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nYou will work with the following function for this exercise,\n\nf(x,y)=e^{−\\left(sin(x)−cos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your own way to visualise it.\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nSuppose, we have the following function f(x) = \\frac{1}{2}\\|x\\|^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0:\n\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n\nYour goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose, \\alpha_0 = \\mathbb{1}^{10}.\n\n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n\n\\frac{\\partial L}{\\partial \\alpha} should be computed at each step using automatic differentiation. Choose any \\beta and the number of steps your need. Describe obtained results.\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x",
    "crumbs": [
      "Exercises",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/exercises/automatic_differentiation.html#automatic-differentiation",
    "href": "docs/exercises/automatic_differentiation.html#automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Calculate the gradient of a Taylor series of a \\cos (x) using autograd library:\nimport autograd.numpy as np # Thinly-wrapped version of Numpy \nfrom autograd import grad \n\ndef taylor_cosine(x): # Taylor approximation to cosine function \n  # Your np code here\n  return ans \nIn the following code for the gradient descent for linear regression change the manual gradient computation to the PyTorch/jax autograd way. Compare those two approaches in time.\nIn order to do this, set the tolerance rate for the function value \\varepsilon = 10^{-9}. Compare the total time required to achieve the specified value of the function for analytical and automatic differentiation. Perform measurements for different values of n from np.logspace(1,4).\nFor each n value carry out at least 3 runs.\nimport numpy as np \n\n# Compute every step manually\n\n# Linear regression\n# f = w * x \n\n# here : f = 2 * x\nX = np.array([1, 2, 3, 4], dtype=np.float32)\nY = np.array([2, 4, 6, 8], dtype=np.float32)\n\nw = 0.0\n\n# model output\ndef forward(x):\n    return w * x\n\n# loss = MSE\ndef loss(y, y_pred):\n    return ((y_pred - y)**2).mean()\n\n# J = MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(w*x - y)\ndef gradient(x, y, y_pred):\n    return np.dot(2*x, y_pred - y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 20\n\nfor epoch in range(n_iters):\n    # predict = forward pass\n    y_pred = forward(X)\n\n    # loss\n    l = loss(Y, y_pred)\n\n    # calculate gradients\n    dw = gradient(X, Y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 2 == 0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')\nCalculate the 4th derivative of hyperbolic tangent function using Jax autograd.\nCompare analytic and autograd (with any framework) approach for the hessian of:\n\nf(x) = \\dfrac{1}{2}x^TAx + b^Tx + c\n\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = tr(AXB)\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\dfrac{1}{2} \\|Ax - b\\|^2_2\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nYou will work with the following function for this exercise,\n\nf(x,y)=e^{−\\left(sin(x)−cos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your own way to visualise it.\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nSuppose, we have the following function f(x) = \\frac{1}{2}\\|x\\|^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0:\n\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n\nYour goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose, \\alpha_0 = \\mathbb{1}^{10}.\n\n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n\n\\frac{\\partial L}{\\partial \\alpha} should be computed at each step using automatic differentiation. Choose any \\beta and the number of steps your need. Describe obtained results.\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x",
    "crumbs": [
      "Exercises",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_sets.html",
    "href": "docs/exercises/conjugate_sets.html",
    "title": "Conjugate sets",
    "section": "",
    "text": "Prove that S^* = \\left(\\overline{S}\\right)^*\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*\nProve that if B(0,r) is a ball of radius r at some norm with the center in zero, then \\left( B(0,r) \\right)^* = B(0,1/r)\nFind a dual cone for a monotonous non-negative cone:\n\nK = \\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\}\n\nFind and sketch on the plane a conjugate set to a multi-faceted cone: S = \\mathbf{cone} \\{ (-3,1), (2,3), (4,5)\\}\nDerive the definition of the cone from the definition of the conjugate set.\nName any 3 non-trivial facts about conjugate sets.\nHow to write down a set conjugate to the polyhedron?\nDraw a conjugate set by hand for simple sets. Conjugate to zero, conjugate to the halfline, to two random points, to their convex hull, etc.\nGive examples of self-conjugate sets.\nUsing a lemma about a cone conjugate, conjugate to the sum of cones and a lemma about a cone, conjugate to the intersection of closed convex cones, prove that cones\n\nK_1 = \\{x \\in \\mathbb{R}^n \\mid x = Ay, y \\ge 0, y \\in \\mathbb{R}^m, A \\in \\mathbb{R}^{n \\times}, \\}, \\;\\; K_2 = \\{p \\in \\mathbb{R}^n \\mid A^Tp \\ge 0\\}\n\nare inter conjugated.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -4, \\;\\; -2x_1 + x_2 \\ge -4\\}\n\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge -1, \\;\\; 2x_1 - x_2 \\ge 0, \\;\\; -x_1 + 2x_2 \\ge -2\\}\n\nFind conjugate cone for the cone of positive definite (semi-definite) matrices.\nFind the conjugate cone for the exponential cone:\n\nK = \\{(x, y, z) \\mid y &gt; 0, y e^{x/y} \\leq z\\}\n\nProve that’s fair for closed convex cones:\n\n(K_1 \\cap K_2)^* = K_1^* + K_2^*\n\nFind the dual cone for the following cones:\n\nK = \\{0\\}\nK = \\mathbb{R}^2\nK = \\{(x_1, x_2) \\mid \\vert x_1\\vert \\leq x_2\\}\nK = \\{(x_1, x_2) \\mid x_1 + x_2 = 0\\}\n\nFind and sketch on the plane a conjugate set to a multifaced cone:\n\n  S = \\mathbf{conv} \\left\\{ (-4,-1), (-2,-1), (-2,1)\\right\\} + \\mathbf{cone} \\left\\{ (1,0), (2,1)\\right\\}\n\nFind and sketch on the plane a conjugate set to a polyhedra:\n\nS = \\left\\{ x \\in \\mathbb{R}^2 \\mid -3x_1 + 2x_2 \\le 7, x_1 + 5x_2 \\le 9, x_1 - x_2 \\le 3, -x_2 \\le 1\\right\\}\n\nProve that if we define the conjugate set to S as follows:\n\nS^* = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\le 1 \\;\\; \\forall x \\in S\\},\n\nthen unit ball with the zero point as the center is the only self conjugate set in \\mathbb{R}^n.\nFind the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\nLet L be the subspace of a Euclidian space X. Prove that L^* = L^\\bot, where L^\\bot - orthogonal complement to L.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices. Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nProve, that B_p and B_{p_*} are inter-conjugate, i.e. (B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nProve, that K_p and K_{p_*} are inter-conjugate, i.e. (K_p)^* = K_{p_*}, (K_{p_*})^* = K_p, where K_p = \\left\\{ [x, \\mu] \\in \\mathbb{R}^{n+1} : \\|x\\|_p \\leq \\mu \\right\\}, \\; 1 &lt; p &lt; \\infty is the norm cone (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nSuppose, S = S^*. Could the set S be anything, but a unit ball? If it can, provide an example of another self-conjugate set. If it couldn’t, prove it.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{\\star}, S^{\\star\\star}, S^{\\star\\star\\star}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_\\star} are inter-conjugate, i.e. (B_p)^\\star = B_{p_\\star}, (B_{p_\\star})^\\star = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_\\star are conjugated, i.e. p^{-1} + p^{-1}\\_\\star = 1. You can assume, that p_\\star = \\infty if p = 1 and vice versa.",
    "crumbs": [
      "Exercises",
      "Conjugate sets"
    ]
  },
  {
    "objectID": "docs/exercises/conjugate_sets.html#conjugate-sets",
    "href": "docs/exercises/conjugate_sets.html#conjugate-sets",
    "title": "Conjugate sets",
    "section": "",
    "text": "Prove that S^* = \\left(\\overline{S}\\right)^*\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*\nProve that if B(0,r) is a ball of radius r at some norm with the center in zero, then \\left( B(0,r) \\right)^* = B(0,1/r)\nFind a dual cone for a monotonous non-negative cone:\n\nK = \\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\}\n\nFind and sketch on the plane a conjugate set to a multi-faceted cone: S = \\mathbf{cone} \\{ (-3,1), (2,3), (4,5)\\}\nDerive the definition of the cone from the definition of the conjugate set.\nName any 3 non-trivial facts about conjugate sets.\nHow to write down a set conjugate to the polyhedron?\nDraw a conjugate set by hand for simple sets. Conjugate to zero, conjugate to the halfline, to two random points, to their convex hull, etc.\nGive examples of self-conjugate sets.\nUsing a lemma about a cone conjugate, conjugate to the sum of cones and a lemma about a cone, conjugate to the intersection of closed convex cones, prove that cones\n\nK_1 = \\{x \\in \\mathbb{R}^n \\mid x = Ay, y \\ge 0, y \\in \\mathbb{R}^m, A \\in \\mathbb{R}^{n \\times}, \\}, \\;\\; K_2 = \\{p \\in \\mathbb{R}^n \\mid A^Tp \\ge 0\\}\n\nare inter conjugated.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -4, \\;\\; -2x_1 + x_2 \\ge -4\\}\n\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge -1, \\;\\; 2x_1 - x_2 \\ge 0, \\;\\; -x_1 + 2x_2 \\ge -2\\}\n\nFind conjugate cone for the cone of positive definite (semi-definite) matrices.\nFind the conjugate cone for the exponential cone:\n\nK = \\{(x, y, z) \\mid y &gt; 0, y e^{x/y} \\leq z\\}\n\nProve that’s fair for closed convex cones:\n\n(K_1 \\cap K_2)^* = K_1^* + K_2^*\n\nFind the dual cone for the following cones:\n\nK = \\{0\\}\nK = \\mathbb{R}^2\nK = \\{(x_1, x_2) \\mid \\vert x_1\\vert \\leq x_2\\}\nK = \\{(x_1, x_2) \\mid x_1 + x_2 = 0\\}\n\nFind and sketch on the plane a conjugate set to a multifaced cone:\n\n  S = \\mathbf{conv} \\left\\{ (-4,-1), (-2,-1), (-2,1)\\right\\} + \\mathbf{cone} \\left\\{ (1,0), (2,1)\\right\\}\n\nFind and sketch on the plane a conjugate set to a polyhedra:\n\nS = \\left\\{ x \\in \\mathbb{R}^2 \\mid -3x_1 + 2x_2 \\le 7, x_1 + 5x_2 \\le 9, x_1 - x_2 \\le 3, -x_2 \\le 1\\right\\}\n\nProve that if we define the conjugate set to S as follows:\n\nS^* = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\le 1 \\;\\; \\forall x \\in S\\},\n\nthen unit ball with the zero point as the center is the only self conjugate set in \\mathbb{R}^n.\nFind the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\nLet L be the subspace of a Euclidian space X. Prove that L^* = L^\\bot, where L^\\bot - orthogonal complement to L.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices. Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nProve, that B_p and B_{p_*} are inter-conjugate, i.e. (B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nProve, that K_p and K_{p_*} are inter-conjugate, i.e. (K_p)^* = K_{p_*}, (K_{p_*})^* = K_p, where K_p = \\left\\{ [x, \\mu] \\in \\mathbb{R}^{n+1} : \\|x\\|_p \\leq \\mu \\right\\}, \\; 1 &lt; p &lt; \\infty is the norm cone (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nSuppose, S = S^*. Could the set S be anything, but a unit ball? If it can, provide an example of another self-conjugate set. If it couldn’t, prove it.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{\\star}, S^{\\star\\star}, S^{\\star\\star\\star}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_\\star} are inter-conjugate, i.e. (B_p)^\\star = B_{p_\\star}, (B_{p_\\star})^\\star = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_\\star are conjugated, i.e. p^{-1} + p^{-1}\\_\\star = 1. You can assume, that p_\\star = \\infty if p = 1 and vice versa.",
    "crumbs": [
      "Exercises",
      "Conjugate sets"
    ]
  },
  {
    "objectID": "docs/exercises/convex_functions.html",
    "href": "docs/exercises/convex_functions.html",
    "title": "Convex functions",
    "section": "",
    "text": "Show, that f(x) = \\|x\\| is convex on \\mathbb{R}^n.\nShow, that f(x) = c^\\top x + b is convex and concave.\nShow, that f(x) = x^\\top Ax, where A\\succeq 0 - is convex on \\mathbb{R}^n.\nShow, that f(A) = \\lambda_{max}(A) - is convex, if A \\in S^n.\nProve, that -\\log\\det X is convex on X \\in S^n_{++}.\nShow, that f(x) is convex, using first and second order criteria, if f(x) = \\sum\\limits_{i=1}^n x_i^4.\nFind the set of x \\in \\mathbb{R}^n, where the function f(x) = \\dfrac{-1}{2(1 + x^\\top x)} is convex, strictly convex, strongly convex?\n\nFind the values of a,b,c, where f(x,y,z) = x^2 + 2axy + by^2 + cz^2 is convex, strictly convex, strongly convex?\nВыпуклы ли следующие функции: f(x) = e^x - 1, \\; x \\in \\mathbb{R};\\;\\;\\; f(x_1, x_2) = x_1x_2, \\; x \\in \\mathbb{R}^2_{++};\\;\\;\\; f(x_1, x_2) = 1/(x_1x_2), \\; x \\in \\mathbb{R}^2_{++}?\nДокажите, что множество S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\prod\\limits_{i=1}^n x_i \\geq 1 \\right\\} выпукло.\nProve, that function f(X) = \\mathbf{tr}(X^{-1}), X \\in S^n_{++} is convex, while g(X) = (\\det X)^{1/n}, X \\in S^n_{++} is concave.\nKullback–Leibler divergence between p,q \\in \\mathbb{R}^n_{++} is:\n\nD(p,q) = \\sum\\limits_{i=1}^n (p_i \\log(p_i/q_i) - p_i + q_i)\n\nProve, that D(p,q) \\geq 0 \\forall p,q \\in \\mathbb{R}^n_{++} and D(p,q) = 0 \\leftrightarrow p = q\nHint: \nD(p,q) = f(p) - f(q) - \\nabla f(q)^\\top (p-q), \\;\\;\\;\\; f(p) = \\sum\\limits_{i=1}^n p_i \\log p_i\n\nLet x be a real variable with the values a_1 &lt; a_2 &lt; \\ldots &lt; a_n with probabilities \\mathbb{P}(x = a_i) = p_i. Derive the convexity or concavity of the following functions from p on the set of \\left\\{p \\mid \\sum\\limits_{i=1}^n p_i = 1, p_i \\ge 0 \\right\\}\n\n\\mathbb{E}x\n\\mathbb{P}\\{x \\ge \\alpha\\}\n\\mathbb{P}\\{\\alpha \\le x \\le \\beta\\}\n\\sum\\limits_{i=1}^n p_i \\log p_i​\n\\mathbb{V}x = \\mathbb{E}(x - \\mathbb{E}x)^2\n\\mathbf{quartile}(x) = {\\operatorname{inf}}\\left\\{ \\beta \\mid \\mathbb{P}\\{x \\le \\beta\\} \\ge 0.25 \\right\\}\n\nОпределения выпуклости и сильной выпуклости. Критерии выпуклости и сильной выпуклости первого и второго порядков\nГеометрическая интерпретация выпуклости и сильной выпуклости. (подпирание прямой и параболой)\nПриведите различные три операции, сохраняющие выпуклость функции.\nДоказать, что для a,b \\ge 0; \\;\\;\\; \\theta \\in [0,1]\n\n- \\log \\left( \\dfrac{a+b}{2}\\right) \\le -\\dfrac{\\log a + \\log b}{2}\na^\\theta b^{1-\\theta} \\le \\theta a + (1 - \\theta)b\nHölder’s inequality: \\sum\\limits_{i=1}^n x_i y_i \\le \\left( \\sum\\limits_{i=1}^n \\vert x_i\\vert ^p\\right)^{1/p} \\left( \\sum\\limits_{i=1}^n \\vert y_i\\vert^p\\right)^{1/p}. For p &gt;1, \\;\\; \\dfrac{1}{p} + \\dfrac{1}{q} = 1.\n\nFor x, y \\in \\mathbb{R}^n\nДоказать, что матричная норма f(X) = \\|X\\|_2 = \\sup\\limits_{y \\in \\mathbb{R}^n} \\dfrac{\\|Xy\\|_2}{\\|y\\|_2} выпукла.\nДоказать, что:\n\nесли f(x) - выпукла, то \\exp f(x) также выпукла.\nесли f(x) - выпукла, то g(x)^p выпукла для p \\ge 1, f(x) \\ge 0.\nесли f(x) - вогнута, то 1/f(x) выпукла для f(x) &gt; 0.\n\nВыпукла ли функция f(X, y) = y^T X^{-1}y на множестве \\mathbf{dom} f = \\{X, y \\mid X + X^T \\succeq 0\\} ? Известно, что эта функция выпукла, если X - симметричная матрица (упражнение - доказать). Докажите выпуклость или приведите простой контрпример.\nПусть функция h(x) - выпуклая на \\mathbb{R} неубывающая функция, кроме того: h(x) = 0 при x \\le 0. Докажите, что функция h\\left(\\|x\\|_2\\right) выпукла на \\mathbb{R}^n.\nIs the function returning the arithmetic mean of vector coordinates is a convex one: a(x) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i, what about geometric mean: g(x) = \\prod\\limits_{i=1}^n \\left(x_i \\right)^{1/n}?\nShow, that the following function is convex on the set of all positive denominators\n\nf(x) = \\dfrac{1}{x_1 - \\dfrac{1}{x_2 - \\dfrac{1}{x_3 - \\dfrac{1}{\\ldots}}}}, x \\in \\mathbb{R}^n\n\nВлияют ли линейные члены квадратичной функции на ее выпуклость? Сильную выпуклость?\nПусть f(x) : \\mathbb{R}^n \\to \\mathbb{R} такова, что \\forall x,y \\to f\\left( \\dfrac{x+y}{2}\\right) \\leq \\dfrac{1}{2}(f(x)+f(y)). Является ли такая функция выпуклой?\nFind the set, on which the function f(x,y) = e^{xy} will be convex.\nStudy the following function of two variables f(x,y) = e^{xy}.\n\nIs this function convex?\nProve, that this function will be convex on the line x = y.\nFind another set in \\mathbb{R}^2, on which this function will be convex.\n\nIs f(x) = -x \\ln x - (1-x) \\ln (1-x) convex?\nProve, that adding \\lambda \\|x\\|_2^2 to any convex function g(x) ensures strong convexity of a resulting function f(x) = g(x) + \\lambda \\|x\\|_2^2. Find the constant of the strong convexity \\mu.\nProve, that function\n\nf(x) = \\log\\left( \\sum\\limits_{i=1}^n e^{x_i}\\right)\n\nis convex using any differential criterion.\nProve, that a function f is strongly convex with parameter \\mu if and only if the function \nx \\mapsto f(x)- \\frac{\\mu}{2} \\|x\\|^{2}\n is convex.\nGive an example of a function, that satisfies Polyak Lojasiewicz condition, but doesn’t have convexity property.\nProve, that if g(x) - convex function, then f(x) = g(x) + \\dfrac{\\lambda}{2}\\|x\\|^2_2 will be strongly convex function.\nFind then f(x) = x^T A x is strongly convex and find strong convexity constant.\nLet f: \\mathbb{R}^n \\to \\mathbb{R} be the following function: \nf(x) = \\sum\\limits_{i=1}^k x_{\\lfloor i \\rfloor},\n where 1 \\leq k \\leq n, while the symbol x_{\\lfloor i \\rfloor} stands for the i-th component of sorted (x_{\\lfloor 1 \\rfloor} - maximum component of x and x_{\\lfloor n \\rfloor} - minimum component of x) vector of x. Show, that f is a convex function.\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\mu \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.",
    "crumbs": [
      "Exercises",
      "Convex functions"
    ]
  },
  {
    "objectID": "docs/exercises/convex_functions.html#convex-functions",
    "href": "docs/exercises/convex_functions.html#convex-functions",
    "title": "Convex functions",
    "section": "",
    "text": "Show, that f(x) = \\|x\\| is convex on \\mathbb{R}^n.\nShow, that f(x) = c^\\top x + b is convex and concave.\nShow, that f(x) = x^\\top Ax, where A\\succeq 0 - is convex on \\mathbb{R}^n.\nShow, that f(A) = \\lambda_{max}(A) - is convex, if A \\in S^n.\nProve, that -\\log\\det X is convex on X \\in S^n_{++}.\nShow, that f(x) is convex, using first and second order criteria, if f(x) = \\sum\\limits_{i=1}^n x_i^4.\nFind the set of x \\in \\mathbb{R}^n, where the function f(x) = \\dfrac{-1}{2(1 + x^\\top x)} is convex, strictly convex, strongly convex?\n\nFind the values of a,b,c, where f(x,y,z) = x^2 + 2axy + by^2 + cz^2 is convex, strictly convex, strongly convex?\nВыпуклы ли следующие функции: f(x) = e^x - 1, \\; x \\in \\mathbb{R};\\;\\;\\; f(x_1, x_2) = x_1x_2, \\; x \\in \\mathbb{R}^2_{++};\\;\\;\\; f(x_1, x_2) = 1/(x_1x_2), \\; x \\in \\mathbb{R}^2_{++}?\nДокажите, что множество S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\prod\\limits_{i=1}^n x_i \\geq 1 \\right\\} выпукло.\nProve, that function f(X) = \\mathbf{tr}(X^{-1}), X \\in S^n_{++} is convex, while g(X) = (\\det X)^{1/n}, X \\in S^n_{++} is concave.\nKullback–Leibler divergence between p,q \\in \\mathbb{R}^n_{++} is:\n\nD(p,q) = \\sum\\limits_{i=1}^n (p_i \\log(p_i/q_i) - p_i + q_i)\n\nProve, that D(p,q) \\geq 0 \\forall p,q \\in \\mathbb{R}^n_{++} and D(p,q) = 0 \\leftrightarrow p = q\nHint: \nD(p,q) = f(p) - f(q) - \\nabla f(q)^\\top (p-q), \\;\\;\\;\\; f(p) = \\sum\\limits_{i=1}^n p_i \\log p_i\n\nLet x be a real variable with the values a_1 &lt; a_2 &lt; \\ldots &lt; a_n with probabilities \\mathbb{P}(x = a_i) = p_i. Derive the convexity or concavity of the following functions from p on the set of \\left\\{p \\mid \\sum\\limits_{i=1}^n p_i = 1, p_i \\ge 0 \\right\\}\n\n\\mathbb{E}x\n\\mathbb{P}\\{x \\ge \\alpha\\}\n\\mathbb{P}\\{\\alpha \\le x \\le \\beta\\}\n\\sum\\limits_{i=1}^n p_i \\log p_i​\n\\mathbb{V}x = \\mathbb{E}(x - \\mathbb{E}x)^2\n\\mathbf{quartile}(x) = {\\operatorname{inf}}\\left\\{ \\beta \\mid \\mathbb{P}\\{x \\le \\beta\\} \\ge 0.25 \\right\\}\n\nОпределения выпуклости и сильной выпуклости. Критерии выпуклости и сильной выпуклости первого и второго порядков\nГеометрическая интерпретация выпуклости и сильной выпуклости. (подпирание прямой и параболой)\nПриведите различные три операции, сохраняющие выпуклость функции.\nДоказать, что для a,b \\ge 0; \\;\\;\\; \\theta \\in [0,1]\n\n- \\log \\left( \\dfrac{a+b}{2}\\right) \\le -\\dfrac{\\log a + \\log b}{2}\na^\\theta b^{1-\\theta} \\le \\theta a + (1 - \\theta)b\nHölder’s inequality: \\sum\\limits_{i=1}^n x_i y_i \\le \\left( \\sum\\limits_{i=1}^n \\vert x_i\\vert ^p\\right)^{1/p} \\left( \\sum\\limits_{i=1}^n \\vert y_i\\vert^p\\right)^{1/p}. For p &gt;1, \\;\\; \\dfrac{1}{p} + \\dfrac{1}{q} = 1.\n\nFor x, y \\in \\mathbb{R}^n\nДоказать, что матричная норма f(X) = \\|X\\|_2 = \\sup\\limits_{y \\in \\mathbb{R}^n} \\dfrac{\\|Xy\\|_2}{\\|y\\|_2} выпукла.\nДоказать, что:\n\nесли f(x) - выпукла, то \\exp f(x) также выпукла.\nесли f(x) - выпукла, то g(x)^p выпукла для p \\ge 1, f(x) \\ge 0.\nесли f(x) - вогнута, то 1/f(x) выпукла для f(x) &gt; 0.\n\nВыпукла ли функция f(X, y) = y^T X^{-1}y на множестве \\mathbf{dom} f = \\{X, y \\mid X + X^T \\succeq 0\\} ? Известно, что эта функция выпукла, если X - симметричная матрица (упражнение - доказать). Докажите выпуклость или приведите простой контрпример.\nПусть функция h(x) - выпуклая на \\mathbb{R} неубывающая функция, кроме того: h(x) = 0 при x \\le 0. Докажите, что функция h\\left(\\|x\\|_2\\right) выпукла на \\mathbb{R}^n.\nIs the function returning the arithmetic mean of vector coordinates is a convex one: a(x) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i, what about geometric mean: g(x) = \\prod\\limits_{i=1}^n \\left(x_i \\right)^{1/n}?\nShow, that the following function is convex on the set of all positive denominators\n\nf(x) = \\dfrac{1}{x_1 - \\dfrac{1}{x_2 - \\dfrac{1}{x_3 - \\dfrac{1}{\\ldots}}}}, x \\in \\mathbb{R}^n\n\nВлияют ли линейные члены квадратичной функции на ее выпуклость? Сильную выпуклость?\nПусть f(x) : \\mathbb{R}^n \\to \\mathbb{R} такова, что \\forall x,y \\to f\\left( \\dfrac{x+y}{2}\\right) \\leq \\dfrac{1}{2}(f(x)+f(y)). Является ли такая функция выпуклой?\nFind the set, on which the function f(x,y) = e^{xy} will be convex.\nStudy the following function of two variables f(x,y) = e^{xy}.\n\nIs this function convex?\nProve, that this function will be convex on the line x = y.\nFind another set in \\mathbb{R}^2, on which this function will be convex.\n\nIs f(x) = -x \\ln x - (1-x) \\ln (1-x) convex?\nProve, that adding \\lambda \\|x\\|_2^2 to any convex function g(x) ensures strong convexity of a resulting function f(x) = g(x) + \\lambda \\|x\\|_2^2. Find the constant of the strong convexity \\mu.\nProve, that function\n\nf(x) = \\log\\left( \\sum\\limits_{i=1}^n e^{x_i}\\right)\n\nis convex using any differential criterion.\nProve, that a function f is strongly convex with parameter \\mu if and only if the function \nx \\mapsto f(x)- \\frac{\\mu}{2} \\|x\\|^{2}\n is convex.\nGive an example of a function, that satisfies Polyak Lojasiewicz condition, but doesn’t have convexity property.\nProve, that if g(x) - convex function, then f(x) = g(x) + \\dfrac{\\lambda}{2}\\|x\\|^2_2 will be strongly convex function.\nFind then f(x) = x^T A x is strongly convex and find strong convexity constant.\nLet f: \\mathbb{R}^n \\to \\mathbb{R} be the following function: \nf(x) = \\sum\\limits_{i=1}^k x_{\\lfloor i \\rfloor},\n where 1 \\leq k \\leq n, while the symbol x_{\\lfloor i \\rfloor} stands for the i-th component of sorted (x_{\\lfloor 1 \\rfloor} - maximum component of x and x_{\\lfloor n \\rfloor} - minimum component of x) vector of x. Show, that f is a convex function.\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\mu \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.",
    "crumbs": [
      "Exercises",
      "Convex functions"
    ]
  },
  {
    "objectID": "docs/exercises/cvxpy.html",
    "href": "docs/exercises/cvxpy.html",
    "title": "CVXPY library",
    "section": "",
    "text": "Constrained linear least squares Solve the following problem with cvxpy library.\n\n\\begin{split} &\\|X \\theta - y\\|^2_2 \\to \\min\\limits_{\\theta \\in \\mathbb{R}^{n} } \\\\ \\text{s.t. } & 0_n \\leq \\theta \\leq 1_n \\end{split}\n\nLinear programming A linear program is an optimization problem with a linear objective and affine inequality constraints. A common standard form is the following:\n  \n     \\begin{array}{ll}\n     \\text{minimize}   & c^Tx \\\\\n     \\text{subject to} & Ax \\leq b.\n     \\end{array}\n\nHere A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, and c \\in \\mathbb{R}^n are problem data and x \\in \\mathbb{R}^{n} is the optimization variable. The inequality constraint Ax \\leq b is elementwise. Solve this problem with cvxpy library.\nList the installed solvers in cvxpy using cp.installed_solvers() method.\nSolve the following optimization problem using CVXPY:\n\n\\begin{array}{ll}\n\\text{minimize} & |x| - 2\\sqrt{y}\\\\\n\\text{subject to} & 2 \\geq e^x \\\\\n& x + y = 5,\n\\end{array}\n\nwhere x,y \\in \\mathbb{R} are variables. Find the optimal values of x and y.\nRisk budget allocation Suppose an amount x_i&gt;0 is invested in n assets, labeled i=1,..., n, with asset return covariance matrix \\Sigma \\in \\mathcal{S}_{++}^n. We define the risk of the investments as the standard deviation of the total return\n\nR(x) = (x^T\\Sigma x)^{1/2}.\n\nWe define the (relative) risk contribution of asset i (in the portfolio x) as\n\n\\rho_i = \\frac{\\partial \\log R(x)}{\\partial \\log x_i} =\n\\frac{\\partial R(x)}{R(x)} \\frac{x_i}{\\partial x_i}, \\quad i=1, \\ldots, n.\n\nThus \\rho_i gives the fractional increase in risk per fractional increase in investment i. We can express the risk contributions as\n\n\\rho_i = \\frac{x_i (\\Sigma x)_i} {x^T\\Sigma x}, \\quad i=1, \\ldots, n,\n\nfrom which we see that \\sum_{i=1}^n \\rho_i = 1. For general x, we can have \\rho_i &lt;0, which means that a small increase in investment i decreases the risk. Desirable investment choices have \\rho_i&gt;0, in which case we can interpret \\rho_i as the fraction of the total risk contributed by the investment in asset i. Note that the risk contributions are homogeneous, i.e., scaling x by a positive constant does not affect \\rho_i.\n\nProblem statement: In the risk budget allocation problem, we are given \\Sigma and a set of desired risk contributions \\rho_i^\\mathrm{des}&gt;0 with \\bf{1}^T \\rho^\\mathrm{des}=1; the goal is to find an investment mix x\\succ 0, \\bf{1}^Tx =1, with these risk contributions. When \\rho^\\mathrm{des} = (1/n)\\bf{1}, the problem is to find an investment mix that achieves so-called risk parity.\na) Explain how to solve the risk budget allocation problem using convex optimization. Hint. Minimize (1/2)x^T\\Sigma x - \\sum_{i=1}^n \\rho_i^\\mathrm{des} \\log x_i.\nb) Find the investment mix that achieves risk parity for the return covariance matrix \\Sigma below.\nimport numpy as np\nimport cvxpy as cp\nSigma = np.array(np.matrix(\"\"\"6.1  2.9  -0.8  0.1;\n                     2.9  4.3  -0.3  0.9;\n                    -0.8 -0.3   1.2 -0.7;\n                     0.1  0.9  -0.7  2.3\"\"\"))\nrho = np.ones(4)/4",
    "crumbs": [
      "Exercises",
      "CVXPY library"
    ]
  },
  {
    "objectID": "docs/exercises/cvxpy.html#cvxpy-library",
    "href": "docs/exercises/cvxpy.html#cvxpy-library",
    "title": "CVXPY library",
    "section": "",
    "text": "Constrained linear least squares Solve the following problem with cvxpy library.\n\n\\begin{split} &\\|X \\theta - y\\|^2_2 \\to \\min\\limits_{\\theta \\in \\mathbb{R}^{n} } \\\\ \\text{s.t. } & 0_n \\leq \\theta \\leq 1_n \\end{split}\n\nLinear programming A linear program is an optimization problem with a linear objective and affine inequality constraints. A common standard form is the following:\n  \n     \\begin{array}{ll}\n     \\text{minimize}   & c^Tx \\\\\n     \\text{subject to} & Ax \\leq b.\n     \\end{array}\n\nHere A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, and c \\in \\mathbb{R}^n are problem data and x \\in \\mathbb{R}^{n} is the optimization variable. The inequality constraint Ax \\leq b is elementwise. Solve this problem with cvxpy library.\nList the installed solvers in cvxpy using cp.installed_solvers() method.\nSolve the following optimization problem using CVXPY:\n\n\\begin{array}{ll}\n\\text{minimize} & |x| - 2\\sqrt{y}\\\\\n\\text{subject to} & 2 \\geq e^x \\\\\n& x + y = 5,\n\\end{array}\n\nwhere x,y \\in \\mathbb{R} are variables. Find the optimal values of x and y.\nRisk budget allocation Suppose an amount x_i&gt;0 is invested in n assets, labeled i=1,..., n, with asset return covariance matrix \\Sigma \\in \\mathcal{S}_{++}^n. We define the risk of the investments as the standard deviation of the total return\n\nR(x) = (x^T\\Sigma x)^{1/2}.\n\nWe define the (relative) risk contribution of asset i (in the portfolio x) as\n\n\\rho_i = \\frac{\\partial \\log R(x)}{\\partial \\log x_i} =\n\\frac{\\partial R(x)}{R(x)} \\frac{x_i}{\\partial x_i}, \\quad i=1, \\ldots, n.\n\nThus \\rho_i gives the fractional increase in risk per fractional increase in investment i. We can express the risk contributions as\n\n\\rho_i = \\frac{x_i (\\Sigma x)_i} {x^T\\Sigma x}, \\quad i=1, \\ldots, n,\n\nfrom which we see that \\sum_{i=1}^n \\rho_i = 1. For general x, we can have \\rho_i &lt;0, which means that a small increase in investment i decreases the risk. Desirable investment choices have \\rho_i&gt;0, in which case we can interpret \\rho_i as the fraction of the total risk contributed by the investment in asset i. Note that the risk contributions are homogeneous, i.e., scaling x by a positive constant does not affect \\rho_i.\n\nProblem statement: In the risk budget allocation problem, we are given \\Sigma and a set of desired risk contributions \\rho_i^\\mathrm{des}&gt;0 with \\bf{1}^T \\rho^\\mathrm{des}=1; the goal is to find an investment mix x\\succ 0, \\bf{1}^Tx =1, with these risk contributions. When \\rho^\\mathrm{des} = (1/n)\\bf{1}, the problem is to find an investment mix that achieves so-called risk parity.\na) Explain how to solve the risk budget allocation problem using convex optimization. Hint. Minimize (1/2)x^T\\Sigma x - \\sum_{i=1}^n \\rho_i^\\mathrm{des} \\log x_i.\nb) Find the investment mix that achieves risk parity for the return covariance matrix \\Sigma below.\nimport numpy as np\nimport cvxpy as cp\nSigma = np.array(np.matrix(\"\"\"6.1  2.9  -0.8  0.1;\n                     2.9  4.3  -0.3  0.9;\n                    -0.8 -0.3   1.2 -0.7;\n                     0.1  0.9  -0.7  2.3\"\"\"))\nrho = np.ones(4)/4",
    "crumbs": [
      "Exercises",
      "CVXPY library"
    ]
  },
  {
    "objectID": "docs/exercises/cvxpy.html#materials",
    "href": "docs/exercises/cvxpy.html#materials",
    "title": "CVXPY library",
    "section": "1 Materials",
    "text": "1 Materials\n\nCVXPY exercises\nAdditional Exercises for Convex Optimization",
    "crumbs": [
      "Exercises",
      "CVXPY library"
    ]
  },
  {
    "objectID": "docs/exercises/fom.html",
    "href": "docs/exercises/fom.html",
    "title": "First order methods",
    "section": "",
    "text": "A function is said to belong to the class f \\in C^{k,p}_L (Q) if it k times is continuously differentiable on Q and the pth derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} on \\mathbb{R}^n. Note that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher the order of the derivative, the stronger the constraint (fewer functions belong to the class)\n\nProve that the function belongs to the class C_L^{2,1} \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\||\\nabla^2 f(x)\\| \\leq L\n\nProve also that the last condition can be rewritten, without generality restriction, as follows:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nNote: by default the Euclidean norm is used for vectors and the spectral norm is used for matrices.\nПокажите, что с помощью следующих стратегий подбора шага в градиентному спуске:\n\nПостоянный шаг h_k = \\dfrac{1}{L}\nУбывающая последовательность h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0\n\nможно получить оценку убывания функции на итерации вида:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - некоторая константа, L - константа Липщица градиента функции\nРассмотрим функцию двух переменных:\n\nf(x_1, x_2) = x_1^2 + k x_2^2,\n\nгде k - некоторый параметр. Постройте график количества итераций, необходимых для сходимости алгоритма наискорейшего спуска (до выполнения условия \\|\\nabla f(x_k)\\| \\leq \\varepsilon = 10^{-7}) в зависимости от значения k. Рассмотрите интервал k \\in [10^{-3}; 10^3] (будет удобно использовать функцию ks = np.logspace(-3,3)) и строить график по оси абсцисс в логарифмическом масштабе plt.semilogx() или plt.loglog() для двойного лог. масштаба.\nСделайте те же графики для функции:\n\nf(x) = \\ln(1 + e^{x^\\top A x}) + \\mathbf{1}^\\top x\n\nОбъясните полученную зависимость.\nДля наглядности можете пользоваться кодом отрисовки картинок:\ndef f_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    x_1, x_2 = x\n    return x_1**2 + k*x_2**2\n\ndef df_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    return np.array([2*x[0], 2*k*x[1]])\n\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\ndef plot_3d_function(x1, x2, f, title, *f_params, minima = None, iterations = None):\n    '''\n    '''\n    low_lim_1 = x1.min()\n    low_lim_2 = x2.min()\n    up_lim_1  = x1.max()\n    up_lim_2  = x2.max()\n\n    X1,X2 = np.meshgrid(x1, x2) # grid of point\n    Z = f((X1, X2), *f_params) # evaluation of the function on the grid\n\n    # set up a figure twice as wide as it is tall\n    fig = plt.figure(figsize=(16,7))\n    fig.suptitle(title)\n\n    #===============\n    #  First subplot\n    #===============\n    # set up the axes for the first plot\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n\n    # plot a 3D surface like in the example mplot3d/surface3d_demo\n    surf = ax.plot_surface(X1, X2, Z, rstride=1, cstride=1, \n                        cmap=cm.RdBu,linewidth=0, antialiased=False)\n\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, f(minima_), 'r*', markersize=10)\n\n\n\n    #===============\n    # Second subplot\n    #===============\n    # set up the axes for the second plot\n    ax = fig.add_subplot(1, 2, 2)\n\n    # plot a 3D wireframe like in the example mplot3d/wire3d_demo\n    im = ax.imshow(Z,cmap=plt.cm.RdBu,  extent=[low_lim_1, up_lim_1, low_lim_2, up_lim_2])\n    cset = ax.contour(x1, x2,Z,linewidths=2,cmap=plt.cm.Set2)\n    ax.clabel(cset,inline=True,fmt='%1.1f',fontsize=10)\n    fig.colorbar(im)\n    ax.set_xlabel(f'$x_1$')\n    ax.set_ylabel(f'$x_2$')\n\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, 'r*', markersize=10)\n\n    if iterations is not None:\n        for point in iterations:\n            ax.plot(*point, 'go', markersize=3)\n        iterations = np.array(iterations).T\n        ax.quiver(iterations[0,:-1], iterations[1,:-1], iterations[0,1:]-iterations[0,:-1], iterations[1,1:]-iterations[1,:-1], scale_units='xy', angles='xy', scale=1, color='blue')\n\n    plt.show()\n\nup_lim  = 4\nlow_lim = -up_lim\nx1 = np.arange(low_lim, up_lim, 0.1)\nx2 = np.arange(low_lim, up_lim, 0.1)\nk=0.5\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0])\n\nfrom scipy.optimize import minimize_scalar\n\ndef steepest_descent(x_0, f, df, *f_params, df_eps = 1e-2, max_iter = 1000):\n    iterations = []\n    x = np.array(x_0)\n    iterations.append(x)\n    while np.linalg.norm(df(x, *f_params)) &gt; df_eps and len(iterations) &lt;= max_iter:\n        res = minimize_scalar(lambda alpha: f(x - alpha * df(x, *f_params), *f_params))\n        alpha_opt = res.x\n        x = x - alpha_opt * df(x, *f_params)\n        iterations.append(x)\n    print(f'Finished with {len(iterations)} iterations')\n    return iterations\n\nx_0 = [10,1]\nk = 30\niterations = steepest_descent(x_0, f_6, df_6, k, df_eps = 1e-9)\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0], iterations = iterations)\nSolve the Hobbit Village problem. Open In Colab\nSolve the problem of constrained optimization using projected gradient descent Open In Colab",
    "crumbs": [
      "Exercises",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/exercises/fom.html#first-order-methods",
    "href": "docs/exercises/fom.html#first-order-methods",
    "title": "First order methods",
    "section": "",
    "text": "A function is said to belong to the class f \\in C^{k,p}_L (Q) if it k times is continuously differentiable on Q and the pth derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} on \\mathbb{R}^n. Note that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher the order of the derivative, the stronger the constraint (fewer functions belong to the class)\n\nProve that the function belongs to the class C_L^{2,1} \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\||\\nabla^2 f(x)\\| \\leq L\n\nProve also that the last condition can be rewritten, without generality restriction, as follows:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nNote: by default the Euclidean norm is used for vectors and the spectral norm is used for matrices.\nПокажите, что с помощью следующих стратегий подбора шага в градиентному спуске:\n\nПостоянный шаг h_k = \\dfrac{1}{L}\nУбывающая последовательность h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0\n\nможно получить оценку убывания функции на итерации вида:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - некоторая константа, L - константа Липщица градиента функции\nРассмотрим функцию двух переменных:\n\nf(x_1, x_2) = x_1^2 + k x_2^2,\n\nгде k - некоторый параметр. Постройте график количества итераций, необходимых для сходимости алгоритма наискорейшего спуска (до выполнения условия \\|\\nabla f(x_k)\\| \\leq \\varepsilon = 10^{-7}) в зависимости от значения k. Рассмотрите интервал k \\in [10^{-3}; 10^3] (будет удобно использовать функцию ks = np.logspace(-3,3)) и строить график по оси абсцисс в логарифмическом масштабе plt.semilogx() или plt.loglog() для двойного лог. масштаба.\nСделайте те же графики для функции:\n\nf(x) = \\ln(1 + e^{x^\\top A x}) + \\mathbf{1}^\\top x\n\nОбъясните полученную зависимость.\nДля наглядности можете пользоваться кодом отрисовки картинок:\ndef f_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    x_1, x_2 = x\n    return x_1**2 + k*x_2**2\n\ndef df_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    return np.array([2*x[0], 2*k*x[1]])\n\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\ndef plot_3d_function(x1, x2, f, title, *f_params, minima = None, iterations = None):\n    '''\n    '''\n    low_lim_1 = x1.min()\n    low_lim_2 = x2.min()\n    up_lim_1  = x1.max()\n    up_lim_2  = x2.max()\n\n    X1,X2 = np.meshgrid(x1, x2) # grid of point\n    Z = f((X1, X2), *f_params) # evaluation of the function on the grid\n\n    # set up a figure twice as wide as it is tall\n    fig = plt.figure(figsize=(16,7))\n    fig.suptitle(title)\n\n    #===============\n    #  First subplot\n    #===============\n    # set up the axes for the first plot\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n\n    # plot a 3D surface like in the example mplot3d/surface3d_demo\n    surf = ax.plot_surface(X1, X2, Z, rstride=1, cstride=1, \n                        cmap=cm.RdBu,linewidth=0, antialiased=False)\n\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, f(minima_), 'r*', markersize=10)\n\n\n\n    #===============\n    # Second subplot\n    #===============\n    # set up the axes for the second plot\n    ax = fig.add_subplot(1, 2, 2)\n\n    # plot a 3D wireframe like in the example mplot3d/wire3d_demo\n    im = ax.imshow(Z,cmap=plt.cm.RdBu,  extent=[low_lim_1, up_lim_1, low_lim_2, up_lim_2])\n    cset = ax.contour(x1, x2,Z,linewidths=2,cmap=plt.cm.Set2)\n    ax.clabel(cset,inline=True,fmt='%1.1f',fontsize=10)\n    fig.colorbar(im)\n    ax.set_xlabel(f'$x_1$')\n    ax.set_ylabel(f'$x_2$')\n\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, 'r*', markersize=10)\n\n    if iterations is not None:\n        for point in iterations:\n            ax.plot(*point, 'go', markersize=3)\n        iterations = np.array(iterations).T\n        ax.quiver(iterations[0,:-1], iterations[1,:-1], iterations[0,1:]-iterations[0,:-1], iterations[1,1:]-iterations[1,:-1], scale_units='xy', angles='xy', scale=1, color='blue')\n\n    plt.show()\n\nup_lim  = 4\nlow_lim = -up_lim\nx1 = np.arange(low_lim, up_lim, 0.1)\nx2 = np.arange(low_lim, up_lim, 0.1)\nk=0.5\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0])\n\nfrom scipy.optimize import minimize_scalar\n\ndef steepest_descent(x_0, f, df, *f_params, df_eps = 1e-2, max_iter = 1000):\n    iterations = []\n    x = np.array(x_0)\n    iterations.append(x)\n    while np.linalg.norm(df(x, *f_params)) &gt; df_eps and len(iterations) &lt;= max_iter:\n        res = minimize_scalar(lambda alpha: f(x - alpha * df(x, *f_params), *f_params))\n        alpha_opt = res.x\n        x = x - alpha_opt * df(x, *f_params)\n        iterations.append(x)\n    print(f'Finished with {len(iterations)} iterations')\n    return iterations\n\nx_0 = [10,1]\nk = 30\niterations = steepest_descent(x_0, f_6, df_6, k, df_eps = 1e-9)\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0], iterations = iterations)\nSolve the Hobbit Village problem. Open In Colab\nSolve the problem of constrained optimization using projected gradient descent Open In Colab",
    "crumbs": [
      "Exercises",
      "First order methods"
    ]
  },
  {
    "objectID": "docs/exercises/index.html",
    "href": "docs/exercises/index.html",
    "title": "Exercises",
    "section": "",
    "text": "Files and links should be added.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "docs/exercises/matrix_calculus.html",
    "href": "docs/exercises/matrix_calculus.html",
    "title": "Matrix calculus",
    "section": "",
    "text": "Find the derivatives of f(x) = Ax, \\quad \\nabla_x f(x) = ?, \\nabla_A f(x) = ?\nFind \\nabla f(x), if f(x) = c^Tx.\nFind \\nabla f(x), if f(x) = \\dfrac{1}{2}x^TAx + b^Tx + c.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = -e^{-x^Tx}.\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert ^2_2.\nFind \\nabla f(x), if f(x) = \\Vert x\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), if f(x) = \\Vert Ax\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = \\dfrac{-1}{1 + x^T x}.\nCalculate df(x) and \\nabla f(x) for the function f(x) = \\log(x^{T}\\mathrm{A}x).\nFind f^\\prime(X), if f(X) = \\det X\nNote: here under f^\\prime(X) assumes first order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X)\n\nFind f^{\\prime\\prime}(X), if f(X) = \\log \\det X\nNote: here under f^{\\prime\\prime}(X) assumes second order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X) + \\frac{1}{2}\\mathbf{tr}(\\Delta X^T f^{\\prime\\prime}(X) \\Delta X)\n\nFind gradient and hessian of f : \\mathbb{R}^n \\to \\mathbb{R}, if:\n\nf(x) = \\log \\sum\\limits_{i=1}^m \\exp (a_i^T x + b_i), \\;\\;\\;\\; a_1, \\ldots, a_m \\in \\mathbb{R}^n; \\;\\;\\;  b_1, \\ldots, b_m  \\in \\mathbb{R}\n\nWhat is the gradient, Jacobian, Hessian? Is there any connection between those three definitions?\nCalculate: \\dfrac{\\partial }{\\partial X} \\sum \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X} \\prod \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X}\\text{tr}(X), \\;\\; \\dfrac{\\partial }{\\partial X} \\text{det}(X)\nCalculate the Frobenious norm derivative: \\dfrac{\\partial}{\\partial X}\\Vert X\\Vert _F^2\nCalculate the gradient of the softmax regression \\nabla_\\theta L in binary case (K = 2) n - dimensional objects:\n\nh_\\theta(x) = \\begin{bmatrix} P(y = 1 \\vert x; \\theta) \\\\ P(y = 2 \\vert x; \\theta) \\\\ \\vdots \\\\ P(y = K \\vert x; \\theta) \\end{bmatrix} = \\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)T} x) }} \\begin{bmatrix} \\exp(\\theta^{(1)T} x ) \\\\ \\exp(\\theta^{(2)T} x ) \\\\ \\vdots \\\\ \\exp(\\theta^{(K)T} x ) \\\\ \\end{bmatrix}\n\n\nL(\\theta) = - \\left[ \\sum_{i=1}^n  (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right]\n\nFind \\nabla f(X), if f(X) = \\text{tr } AX\nFind \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X\nFind \\nabla f(X), if f(X) = \\ln \\langle Ax, x\\rangle, A \\in \\mathbb{S^n_{++}}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{3}\\Vert x\\Vert _2^3\nCalculate \\nabla f(X), if f(X) = \\Vert  AX - B\\Vert _F, X \\in \\mathbb{R}^{k \\times n}, A \\in \\mathbb{R}^{m \\times k}, B \\in \\mathbb{R}^{m \\times n}\nCalculate the derivatives of the loss function with respect to parameters \\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial b} for the single object x_i (or, n = 1)\n\n\n\nIllustration\n\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\langle x, x\\rangle^{\\langle x, x\\rangle}, x \\in \\mathbb{R}^p \\setminus \\{0\\}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{\\langle Ax, x\\rangle}{\\Vert x\\Vert _2^2}, x \\in \\mathbb{R}^p \\setminus \\{0\\}, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert ^2_F, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\Vert xx^T\\Vert _2\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac1n \\sum\\limits_{i=1}^n \\log \\left( 1 + \\exp(a_i^{T}x)  \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i \\in \\mathbb R^n, \\; \\mu&gt;0.\nMatch functions with their gradients:\n\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}^{-1}\nf(\\mathrm{X}) = \\det \\mathrm{X}\nf(\\mathrm{X}) = \\ln \\det \\mathrm{X}\n\n\n\\nabla f(\\mathrm{X}) = \\mathrm{X}^{-1}\n\\nabla f(\\mathrm{X}) = \\mathrm{I}\n\\nabla f(\\mathrm{X}) = \\det (\\mathrm{X})\\cdot (\\mathrm{X}^{-1})^{T}\n\\nabla f(\\mathrm{X}) = -\\left(\\mathrm{X}^{-2}\\right)^{T}\n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A − tI_n),\n\nwhere A \\in \\mathbb{R}^{n \\times n}, S := \\{t \\in \\mathbb{R} : \\text{det}(A − tI_n) \\neq 0\\}.\nFind the gradient \\nabla f(x), if f(x) = \\text{tr}\\left( AX^2BX^{-T} \\right).",
    "crumbs": [
      "Exercises",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/exercises/matrix_calculus.html#matrix-calculus",
    "href": "docs/exercises/matrix_calculus.html#matrix-calculus",
    "title": "Matrix calculus",
    "section": "",
    "text": "Find the derivatives of f(x) = Ax, \\quad \\nabla_x f(x) = ?, \\nabla_A f(x) = ?\nFind \\nabla f(x), if f(x) = c^Tx.\nFind \\nabla f(x), if f(x) = \\dfrac{1}{2}x^TAx + b^Tx + c.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = -e^{-x^Tx}.\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert ^2_2.\nFind \\nabla f(x), if f(x) = \\Vert x\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), if f(x) = \\Vert Ax\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = \\dfrac{-1}{1 + x^T x}.\nCalculate df(x) and \\nabla f(x) for the function f(x) = \\log(x^{T}\\mathrm{A}x).\nFind f^\\prime(X), if f(X) = \\det X\nNote: here under f^\\prime(X) assumes first order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X)\n\nFind f^{\\prime\\prime}(X), if f(X) = \\log \\det X\nNote: here under f^{\\prime\\prime}(X) assumes second order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X) + \\frac{1}{2}\\mathbf{tr}(\\Delta X^T f^{\\prime\\prime}(X) \\Delta X)\n\nFind gradient and hessian of f : \\mathbb{R}^n \\to \\mathbb{R}, if:\n\nf(x) = \\log \\sum\\limits_{i=1}^m \\exp (a_i^T x + b_i), \\;\\;\\;\\; a_1, \\ldots, a_m \\in \\mathbb{R}^n; \\;\\;\\;  b_1, \\ldots, b_m  \\in \\mathbb{R}\n\nWhat is the gradient, Jacobian, Hessian? Is there any connection between those three definitions?\nCalculate: \\dfrac{\\partial }{\\partial X} \\sum \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X} \\prod \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X}\\text{tr}(X), \\;\\; \\dfrac{\\partial }{\\partial X} \\text{det}(X)\nCalculate the Frobenious norm derivative: \\dfrac{\\partial}{\\partial X}\\Vert X\\Vert _F^2\nCalculate the gradient of the softmax regression \\nabla_\\theta L in binary case (K = 2) n - dimensional objects:\n\nh_\\theta(x) = \\begin{bmatrix} P(y = 1 \\vert x; \\theta) \\\\ P(y = 2 \\vert x; \\theta) \\\\ \\vdots \\\\ P(y = K \\vert x; \\theta) \\end{bmatrix} = \\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)T} x) }} \\begin{bmatrix} \\exp(\\theta^{(1)T} x ) \\\\ \\exp(\\theta^{(2)T} x ) \\\\ \\vdots \\\\ \\exp(\\theta^{(K)T} x ) \\\\ \\end{bmatrix}\n\n\nL(\\theta) = - \\left[ \\sum_{i=1}^n  (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right]\n\nFind \\nabla f(X), if f(X) = \\text{tr } AX\nFind \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X\nFind \\nabla f(X), if f(X) = \\ln \\langle Ax, x\\rangle, A \\in \\mathbb{S^n_{++}}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{3}\\Vert x\\Vert _2^3\nCalculate \\nabla f(X), if f(X) = \\Vert  AX - B\\Vert _F, X \\in \\mathbb{R}^{k \\times n}, A \\in \\mathbb{R}^{m \\times k}, B \\in \\mathbb{R}^{m \\times n}\nCalculate the derivatives of the loss function with respect to parameters \\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial b} for the single object x_i (or, n = 1)\n\n\n\nIllustration\n\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\langle x, x\\rangle^{\\langle x, x\\rangle}, x \\in \\mathbb{R}^p \\setminus \\{0\\}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{\\langle Ax, x\\rangle}{\\Vert x\\Vert _2^2}, x \\in \\mathbb{R}^p \\setminus \\{0\\}, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert ^2_F, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\Vert xx^T\\Vert _2\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac1n \\sum\\limits_{i=1}^n \\log \\left( 1 + \\exp(a_i^{T}x)  \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i \\in \\mathbb R^n, \\; \\mu&gt;0.\nMatch functions with their gradients:\n\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}^{-1}\nf(\\mathrm{X}) = \\det \\mathrm{X}\nf(\\mathrm{X}) = \\ln \\det \\mathrm{X}\n\n\n\\nabla f(\\mathrm{X}) = \\mathrm{X}^{-1}\n\\nabla f(\\mathrm{X}) = \\mathrm{I}\n\\nabla f(\\mathrm{X}) = \\det (\\mathrm{X})\\cdot (\\mathrm{X}^{-1})^{T}\n\\nabla f(\\mathrm{X}) = -\\left(\\mathrm{X}^{-2}\\right)^{T}\n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A − tI_n),\n\nwhere A \\in \\mathbb{R}^{n \\times n}, S := \\{t \\in \\mathbb{R} : \\text{det}(A − tI_n) \\neq 0\\}.\nFind the gradient \\nabla f(x), if f(x) = \\text{tr}\\left( AX^2BX^{-T} \\right).",
    "crumbs": [
      "Exercises",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/exercises/separation.html",
    "href": "docs/exercises/separation.html",
    "title": "Separation",
    "section": "",
    "text": "Let S_1, S_2 be closed convex sets such that: S_1 \\cap S_2 = \\varnothing. Is it true that \\exists p: (p,x) &lt; (p,y) \\;\\; \\forall x \\in S_1, \\forall y \\in S_2\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_1 x_2 \\ge 1, x_1 &gt; 0\\right\\},\\quad S_2 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_2 \\le \\frac{4}{x_1 - 1} +9\\right\\}\n\nFind a supporting hyperplane for a set of S = \\left\\{ x \\in \\mathbb{R}^2 \\mid e^{x_1} \\le x_2\\right\\} at the boundary point x_0 = (0, 1)\nFind a supporting hyperplane for the set of S = \\left\\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2\\right\\} such that separates it from the point x_0 = \\left(-\\frac{5}{4}, \\frac{5}{16}, \\frac{15}{16}\\right)\nПриведите пример двух строго, но не сильно отделимых множеств. Двух отделимых, но не собственно отделимых множеств.\nIllustrate the difference between the tangent hyperplane and the separating hyperplane by considering a convex set with a non-smooth boundary.\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid \\dfrac{x_1^2}{4} + \\dfrac{x_2^2}{9} + \\dfrac{x_3^2}{25} \\le 1 \\} at (-6/5, 12/5, 0), (0, 9/5, 4), (6/5, 0, -4) (any choice)\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2 \\}, which separates it from the points (5/4, 5/16, 15/16), (4/3, 2/3, 13/12), (11/9, 11/27, 1) (any choice)\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_n^2 \\le 1\\right\\}, \\;\\;\\; S_2 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_{n-1}^2 + 1 \\le x_n \\right\\}\n\nFind a supporting hyperplane for a set S = \\left\\{ x \\in \\mathbb{R}^3 \\mid \\frac{x_1^2}{4}+\\frac{x_2^2}{8}+\\frac{x_3^2}{25} \\le 1 \\right\\} at the border point x_0 = \\left(-1, \\frac{12}{5}, \\frac{\\sqrt{3}}{2}\\right)",
    "crumbs": [
      "Exercises",
      "Separation"
    ]
  },
  {
    "objectID": "docs/exercises/separation.html#separation",
    "href": "docs/exercises/separation.html#separation",
    "title": "Separation",
    "section": "",
    "text": "Let S_1, S_2 be closed convex sets such that: S_1 \\cap S_2 = \\varnothing. Is it true that \\exists p: (p,x) &lt; (p,y) \\;\\; \\forall x \\in S_1, \\forall y \\in S_2\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_1 x_2 \\ge 1, x_1 &gt; 0\\right\\},\\quad S_2 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_2 \\le \\frac{4}{x_1 - 1} +9\\right\\}\n\nFind a supporting hyperplane for a set of S = \\left\\{ x \\in \\mathbb{R}^2 \\mid e^{x_1} \\le x_2\\right\\} at the boundary point x_0 = (0, 1)\nFind a supporting hyperplane for the set of S = \\left\\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2\\right\\} such that separates it from the point x_0 = \\left(-\\frac{5}{4}, \\frac{5}{16}, \\frac{15}{16}\\right)\nПриведите пример двух строго, но не сильно отделимых множеств. Двух отделимых, но не собственно отделимых множеств.\nIllustrate the difference between the tangent hyperplane and the separating hyperplane by considering a convex set with a non-smooth boundary.\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid \\dfrac{x_1^2}{4} + \\dfrac{x_2^2}{9} + \\dfrac{x_3^2}{25} \\le 1 \\} at (-6/5, 12/5, 0), (0, 9/5, 4), (6/5, 0, -4) (any choice)\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2 \\}, which separates it from the points (5/4, 5/16, 15/16), (4/3, 2/3, 13/12), (11/9, 11/27, 1) (any choice)\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_n^2 \\le 1\\right\\}, \\;\\;\\; S_2 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_{n-1}^2 + 1 \\le x_n \\right\\}\n\nFind a supporting hyperplane for a set S = \\left\\{ x \\in \\mathbb{R}^3 \\mid \\frac{x_1^2}{4}+\\frac{x_2^2}{8}+\\frac{x_3^2}{25} \\le 1 \\right\\} at the border point x_0 = \\left(-1, \\frac{12}{5}, \\frac{\\sqrt{3}}{2}\\right)",
    "crumbs": [
      "Exercises",
      "Separation"
    ]
  },
  {
    "objectID": "docs/exercises/uncategorized.html",
    "href": "docs/exercises/uncategorized.html",
    "title": "Uncategorized",
    "section": "",
    "text": "Show, that these conditions are equivalent:\n\n  \\|\\nabla f(x) - \\nabla f(z) \\| \\le L \\|x-z\\|\n\nand\n\nf(z) \\le f(x) + \\nabla f(x)^T(z-x) + \\frac L 2 \\|z-x\\|^2\n\nWe say that the function belongs to the class f  \\in C^{k,p}_L (Q) if it is k times continuously differentiable on Q, and the p derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} for \\mathbb{R}^n. Notice that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher is the order of the derivative, the stronger is the limitation (fewer functions belong to the class).\n\nProve that the function belongs to the class C_L^{2,1}. \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x)\\| \\leq L\n\nProve that the last condition can be rewritten in the form without loss of generality:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nShow that for gradient descent with the following stepsize selection strategies:\n\nconstant step h_k = \\dfrac{1}{L}\nDropping sequence h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0.\n\nyou can get the estimation of the function decrease at the iteration of the view:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - some constant, L - Lipschitz constant of the function gradient.",
    "crumbs": [
      "Exercises",
      "Uncategorized"
    ]
  },
  {
    "objectID": "docs/exercises/uncategorized.html#uncategorized",
    "href": "docs/exercises/uncategorized.html#uncategorized",
    "title": "Uncategorized",
    "section": "",
    "text": "Show, that these conditions are equivalent:\n\n  \\|\\nabla f(x) - \\nabla f(z) \\| \\le L \\|x-z\\|\n\nand\n\nf(z) \\le f(x) + \\nabla f(x)^T(z-x) + \\frac L 2 \\|z-x\\|^2\n\nWe say that the function belongs to the class f  \\in C^{k,p}_L (Q) if it is k times continuously differentiable on Q, and the p derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} for \\mathbb{R}^n. Notice that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher is the order of the derivative, the stronger is the limitation (fewer functions belong to the class).\n\nProve that the function belongs to the class C_L^{2,1}. \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x)\\| \\leq L\n\nProve that the last condition can be rewritten in the form without loss of generality:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nShow that for gradient descent with the following stepsize selection strategies:\n\nconstant step h_k = \\dfrac{1}{L}\nDropping sequence h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0.\n\nyou can get the estimation of the function decrease at the iteration of the view:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - some constant, L - Lipschitz constant of the function gradient.",
    "crumbs": [
      "Exercises",
      "Uncategorized"
    ]
  },
  {
    "objectID": "docs/materials/index.html",
    "href": "docs/materials/index.html",
    "title": "Materials",
    "section": "",
    "text": "📝 File📚 bibtex\n\n\nUniversal gradient descent. Alexander Gasnikov - (in Russian) - probably, the most comprehensive book on the modern numerical methods, which covers a lot of theoretical and practical aspects of mathematical programming.\n\n\n@article{gasnikov2017universal,\n  title={Universal gradient descent},\n  author={Gasnikov, Alexander},\n  journal={arXiv preprint arXiv:1711.00394},\n  year={2017}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization: Algorithms and Complexity by Sébastien Bubeck\n\n\n@article{bubeck2015convex,\n  title={Convex optimization: Algorithms and complexity},\n  author={Bubeck, Sébastien and others},\n  journal={Foundations and Trends® in Machine Learning},\n  volume={8},\n  number={3-4},\n  pages={231--357},\n  year={2015},\n  publisher={Now Publishers, Inc.}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization materials by Stephen Boyd and Lieven Vandenberghe\n\n\n@book{boyd2004convex,\n  title={Convex optimization},\n  author={Boyd, Stephen and Vandenberghe, Lieven},\n  year={2004},\n  publisher={Cambridge university press}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright\n\n\n@book{nocedal2006numerical,\n  title={Numerical optimization},\n  author={Nocedal, Jorge and Wright, Stephen},\n  year={2006},\n  publisher={Springer Science & Business Media}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nLectures on Convex Optimization by Yurii Nesterov\n\n\n@book{nesterov2018lectures,\n  title={Lectures on convex optimization},\n  author={Nesterov, Yurii},\n  volume={137},\n  publisher={Springer}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nMinimum-volume ellipsoids\n\n\n@book{todd2016minimum,\n  title={Minimum-volume ellipsoids: Theory and algorithms},\n  author={Todd, Michael J},\n  year={2016},\n  publisher={SIAM}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть I. Введение в выпуклый анализ и теорию оптимизации\n\n\n@article{жадан2014методы,\n  title={Методы оптимизации. Часть 1. Введение в выпуклый анализ и теорию оптимизации: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2014}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть II. Численные алгоритмы\n\n\n@article{жадан2015методы,\n  title={Методы оптимизации. Часть 2. Численные алгоритмы: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2015}\n}",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/materials/index.html#books",
    "href": "docs/materials/index.html#books",
    "title": "Materials",
    "section": "",
    "text": "📝 File📚 bibtex\n\n\nUniversal gradient descent. Alexander Gasnikov - (in Russian) - probably, the most comprehensive book on the modern numerical methods, which covers a lot of theoretical and practical aspects of mathematical programming.\n\n\n@article{gasnikov2017universal,\n  title={Universal gradient descent},\n  author={Gasnikov, Alexander},\n  journal={arXiv preprint arXiv:1711.00394},\n  year={2017}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization: Algorithms and Complexity by Sébastien Bubeck\n\n\n@article{bubeck2015convex,\n  title={Convex optimization: Algorithms and complexity},\n  author={Bubeck, Sébastien and others},\n  journal={Foundations and Trends® in Machine Learning},\n  volume={8},\n  number={3-4},\n  pages={231--357},\n  year={2015},\n  publisher={Now Publishers, Inc.}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nConvex Optimization materials by Stephen Boyd and Lieven Vandenberghe\n\n\n@book{boyd2004convex,\n  title={Convex optimization},\n  author={Boyd, Stephen and Vandenberghe, Lieven},\n  year={2004},\n  publisher={Cambridge university press}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright\n\n\n@book{nocedal2006numerical,\n  title={Numerical optimization},\n  author={Nocedal, Jorge and Wright, Stephen},\n  year={2006},\n  publisher={Springer Science & Business Media}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nLectures on Convex Optimization by Yurii Nesterov\n\n\n@book{nesterov2018lectures,\n  title={Lectures on convex optimization},\n  author={Nesterov, Yurii},\n  volume={137},\n  publisher={Springer}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nMinimum-volume ellipsoids\n\n\n@book{todd2016minimum,\n  title={Minimum-volume ellipsoids: Theory and algorithms},\n  author={Todd, Michael J},\n  year={2016},\n  publisher={SIAM}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть I. Введение в выпуклый анализ и теорию оптимизации\n\n\n@article{жадан2014методы,\n  title={Методы оптимизации. Часть 1. Введение в выпуклый анализ и теорию оптимизации: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2014}\n}\n\n\n\n\n📝 File📚 bibtex\n\n\nМетоды оптимизации, Часть II. Численные алгоритмы\n\n\n@article{жадан2015методы,\n  title={Методы оптимизации. Часть 2. Численные алгоритмы: учебное пособие},\n  author={Жадан, ВГ},\n  journal={М.: МФТИ},\n  year={2015}\n}",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/materials/index.html#courses",
    "href": "docs/materials/index.html#courses",
    "title": "Materials",
    "section": "2 Courses",
    "text": "2 Courses\n\nConvex Optimization and Approximation course by Moritz Hardt @ UC Berkley.\nConvex Optimization course by Ryan Tibshirani @ CMU.\nConvex Optimization course by Lieven Vandenberghe @ UCLA.\nConvex Optimization course by Suvrit Sra @ UC Berkley.\nAdvanced Optimization and Randomized Methods course by Alex Smola and Suvrit Sra @ CMU.\nOptimizaion methods course by Alexandr Katrutsa @ MIPT.\nConvex Analysis and Optimization course by Dimitri Bertsekas @ MIT.\nOptimization for Machine Learning course by Martin Jaggi @ EPFL.\nOptimization for Machine Learning course by Suvrit Sra.\nМетоды оптимизации lectures by Alexander Gasnikov @ MIPT.\nМетоды оптимизации seminars by Daniil Merkulov @ MIPT.",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/materials/index.html#blogs-and-personal-pages",
    "href": "docs/materials/index.html#blogs-and-personal-pages",
    "title": "Materials",
    "section": "3 Blogs and personal pages",
    "text": "3 Blogs and personal pages\n\nI’m a bandit blog by Sébastien Bubeck.\nBlog by Moritz Hardt.\nBlog by Sebastian Pokutta with great cheat sheets on optimization.\nBlog by Sebastian Ruder about NLP and optimization.\nPersonal page of Peter Richtarik with announcements and news.\nPersonal page of Suvrit Sra.\nBlog by Fabian Pedregosa.\nBlog with beatiful insights about modern non-convex optimization.\nMachine Learning Research Blog by Francis Bach.",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/materials/index.html#software-and-apps",
    "href": "docs/materials/index.html#software-and-apps",
    "title": "Materials",
    "section": "4 Software and apps",
    "text": "4 Software and apps\n\nSci hub telegram bot allows you to access almost all the scientific papers in one click.",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/materials/index.html#other",
    "href": "docs/materials/index.html#other",
    "title": "Materials",
    "section": "5 Other",
    "text": "5 Other\n\nNice set of python applied math etudes\nExample functions to test optimization algorithms.\n100 numpy exercises\nCollection of Interactive Machine Learning Examples\nML Python libraries overview (Russ)\nNice Visualisation of some ML ideas\nAn Interactive Tutorial on Numerical Optimization",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html",
    "href": "docs/methods/Autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Suppose we need to solve the following problem:\n\nL(w) \\to \\min_{w \\in \\mathbb{R}^d}\n\nSuch problems typically arise in machine learning, when you need to find optimal hyperparameters w of an ML model (i.e. train a neural network). You may use a lot of algorithms to approach this problem, but given the modern size of the problem, where d could be dozens of billions it is very challenging to solve this problem without information about the gradients using zero-order optimization algorithms. That is why it would be beneficial to be able to calculate the gradient vector \\nabla_w L = \\left( \\frac{\\partial L}{\\partial w_1}, \\ldots, \\frac{\\partial L}{\\partial w_d}\\right)^T. Typically, first-order methods perform much better in huge-scale optimization, while second-order methods require too much memory.",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#problem",
    "href": "docs/methods/Autograd.html#problem",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Suppose we need to solve the following problem:\n\nL(w) \\to \\min_{w \\in \\mathbb{R}^d}\n\nSuch problems typically arise in machine learning, when you need to find optimal hyperparameters w of an ML model (i.e. train a neural network). You may use a lot of algorithms to approach this problem, but given the modern size of the problem, where d could be dozens of billions it is very challenging to solve this problem without information about the gradients using zero-order optimization algorithms. That is why it would be beneficial to be able to calculate the gradient vector \\nabla_w L = \\left( \\frac{\\partial L}{\\partial w_1}, \\ldots, \\frac{\\partial L}{\\partial w_d}\\right)^T. Typically, first-order methods perform much better in huge-scale optimization, while second-order methods require too much memory.",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#finite-differences",
    "href": "docs/methods/Autograd.html#finite-differences",
    "title": "Automatic differentiation",
    "section": "2 Finite differences",
    "text": "2 Finite differences\nThe naive approach to get approximate values of gradients is Finite differences approach. For each coordinate, one can calculate the partial derivative approximation:\n\n\\dfrac{\\partial L}{\\partial w_k} (w) \\approx \\dfrac{L(w+\\varepsilon e_k) - L(w)}{\\varepsilon}, \\quad e_k = (0, \\ldots, \\underset{{\\tiny k}}{1}, \\ldots, 0)\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf the time needed for one calculation of L(w) is T, what is the time needed for calculating \\nabla_w L with this approach?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n2dT, which is extremely long for the huge scale optimization. Moreover, this exact scheme is unstable, which means that you will have to choose between accuracy and stability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nThere is an algorithm to compute \\nabla_w L in \\mathcal{O}(T) operations. 1",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#forward-mode-automatic-differentiation",
    "href": "docs/methods/Autograd.html#forward-mode-automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "3 Forward mode automatic differentiation",
    "text": "3 Forward mode automatic differentiation\nTo dive deep into the idea of automatic differentiation we will consider a simple function for calculating derivatives:\n\nL(w_1, w_2) = w_2 \\log w_1 + \\sqrt{w_2 \\log w_1}\n\nLet’s draw a computational graph of this function:\n\n\n\nIllustration of computation graph of primitive arithmetic operations for the function L(w_1, w_2)\n\n\nLet’s go from the beginning of the graph to the end and calculate the derivative \\dfrac{\\partial L}{\\partial w_1}:\n\n\n\n\n\n\n\n\n\nStep\nFunction\nDerivative\nScheme\n\n\n\n\n1\nw_1 = w_1, w_2 = w_2\n\\dfrac{\\partial w_1}{\\partial w_1} = 1, \\dfrac{\\partial w_2}{\\partial w_1} = 0\n\n\n\n2\nv_1 = \\log w_1\n\\begin{aligned}\\frac{\\partial v_1}{\\partial w_1} &= \\frac{\\partial v_1}{\\partial w_1} \\frac{\\partial w_1}{\\partial w_1}\\\\ &= \\frac{1}{w_1} 1\\end{aligned}\n\n\n\n3\nv_2 = w_2 v_1\n\\begin{aligned}\\frac{\\partial v_2}{\\partial w_1} &= \\frac{\\partial v_2}{\\partial v_1}\\frac{\\partial v_1}{\\partial w_1} + \\frac{\\partial v_2}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1} \\\\&= w_2\\frac{\\partial v_1}{\\partial w_1} + v_1\\frac{\\partial w_2}{\\partial w_1}\\end{aligned}\n\n\n\n4\nv_3 = \\sqrt{v_2}\n\\begin{aligned}\\frac{\\partial v_3}{\\partial w_1} &= \\frac{\\partial v_3}{\\partial v_2}\\frac{\\partial v_2}{\\partial w_1} \\\\ &= \\frac{1}{2\\sqrt{v_2}}\\frac{\\partial v_2}{\\partial w_1}\\end{aligned}\n\n\n\n5\nL = v_2 + v_3\n\\begin{aligned}\\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial v_2}\\frac{\\partial v_2}{\\partial w_1} + \\frac{\\partial L}{\\partial v_3}\\frac{\\partial v_3}{\\partial w_1} \\\\&= 1\\frac{\\partial v_2}{\\partial w_1} + 1\\frac{\\partial v_3}{\\partial w_1}\\end{aligned}\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nMake the same computations for \\dfrac{\\partial L}{\\partial w_2}\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction\nDerivative\nScheme\n\n\n\n\n1\nw_1 = w_1, w_2 = w_2\n\\dfrac{\\partial w_1}{\\partial w_2} = 0, \\dfrac{\\partial w_2}{\\partial w_2} = 1\n\n\n\n2\nv_1 = \\log w_1\n\\begin{aligned}\\frac{\\partial v_1}{\\partial w_2} &= \\frac{\\partial v_1}{\\partial w_2} \\frac{\\partial w_2}{\\partial w_2}\\\\ &= 0 \\cdot 1\\end{aligned}\n\n\n\n3\nv_2 = w_2 v_1\n\\begin{aligned}\\frac{\\partial v_2}{\\partial w_2} &= \\frac{\\partial v_2}{\\partial v_1}\\frac{\\partial v_1}{\\partial w_2} + \\frac{\\partial v_2}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_2} \\\\&= w_2\\frac{\\partial v_1}{\\partial w_2} + v_1\\frac{\\partial w_2}{\\partial w_2}\\end{aligned}\n\n\n\n4\nv_3 = \\sqrt{v_2}\n\\begin{aligned}\\frac{\\partial v_3}{\\partial w_2} &= \\frac{\\partial v_3}{\\partial v_2}\\frac{\\partial v_2}{\\partial w_2} \\\\ &= \\frac{1}{2\\sqrt{v_2}}\\frac{\\partial v_2}{\\partial w_2}\\end{aligned}\n\n\n\n5\nL = v_2 + v_3\n\\begin{aligned}\\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial v_2}\\frac{\\partial v_2}{\\partial w_2} + \\frac{\\partial L}{\\partial v_3}\\frac{\\partial v_3}{\\partial w_2} \\\\&= 1\\frac{\\partial v_2}{\\partial w_2} + 1\\frac{\\partial v_3}{\\partial w_2}\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForward mode automatic differentiation algorithm\n\n\n\n\n\nSuppose, we have a computational graph v_i, i \\in [1; N]. Our goal is to calculate the derivative of the output of this graph with respect to some input variable w_k, i.e. \\dfrac{\\partial v_N}{\\partial w_k}. This idea implies propagation of the gradient with respect to the input variable from start to end, that is why we can introduce the notation:\n\n\\overline{v_i} = \\dfrac{\\partial v_i}{\\partial w_k}\n\n\n\n\nIllustration of forward chain rule to calculate the derivative of the function L with respect to w_k.\n\n\n\nFor i = 1, \\ldots, N:\n\nCompute v_i as a function of its parents (inputs) x_1, \\ldots, x_{t_i}: \n  v_i = v_i(x_1, \\ldots, x_{t_i})\n  \nCompute the derivative \\overline{v_i} using the forward chain rule: \n  \\overline{v_i} = \\sum_{j = 1}^{t_i}\\dfrac{\\partial v_i}{\\partial x_j}\\dfrac{\\partial x_j}{\\partial w_k}\n  \n\n\n\n\n\n\nNote, that this approach does not require storing all intermediate computations, but one can see, that for calculating the derivative \\dfrac{\\partial L}{\\partial w_k} we need \\mathcal{O}(T) operations. This means, that for the whole gradient, we need d\\mathcal{O}(T) operations, which is the same as for finite differences, but we do not have stability issues, or inaccuracies now (the formulas above are exact).",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#backward-mode-automatic-differentiation",
    "href": "docs/methods/Autograd.html#backward-mode-automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "4 Backward mode automatic differentiation",
    "text": "4 Backward mode automatic differentiation\nWe will consider the same function\n\nL(w_1, w_2) = w_2 \\log w_1 + \\sqrt{w_2 \\log w_1}\n\nwith a computational graph:\n\n\n\nIllustration of computation graph of primitive arithmetic operations for the function L(w_1, w_2)\n\n\nAssume, that we have some values of the parameters w_1, w_2 and we have already performed a forward pass (i.e. single propagation through the computational graph from left to right). Suppose, also, that we somehow saved all intermediate values of v_i. Let’s go from the end of the graph to the beginning and calculate the derivatives \\dfrac{\\partial L}{\\partial w_1}, \\dfrac{\\partial L}{\\partial w_1}:\n\n\n\n\n\n\n\n\nStep\nDerivative\nScheme\n\n\n\n\n1\n\\dfrac{\\partial L}{\\partial L} = 1\n\n\n\n2\n\\begin{aligned}\\frac{\\partial L}{\\partial v_3} &= \\frac{\\partial L}{\\partial L} \\frac{\\partial L}{\\partial v_3}\\\\ &= \\frac{\\partial L}{\\partial L} 1\\end{aligned}\n\n\n\n3\n\\begin{aligned}\\frac{\\partial L}{\\partial v_2} &= \\frac{\\partial L}{\\partial v_3}\\frac{\\partial v_3}{\\partial v_2} + \\frac{\\partial L}{\\partial L}\\frac{\\partial L}{\\partial v_2} \\\\&= \\frac{\\partial L}{\\partial v_3}\\frac{1}{2\\sqrt{v_2}} +  \\frac{\\partial L}{\\partial L}1\\end{aligned}\n\n\n\n4\n\\begin{aligned}\\frac{\\partial L}{\\partial v_1} &=\\frac{\\partial L}{\\partial v_2}\\frac{\\partial v_2}{\\partial v_1} \\\\ &= \\frac{\\partial L}{\\partial v_2}w_2\\end{aligned}\n\n\n\n5\n\\begin{aligned}\\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial v_1}\\frac{\\partial v_1}{\\partial w_1} \\\\&= \\frac{\\partial L}{\\partial v_1}\\frac{1}{w_1}\\end{aligned} \\begin{aligned}\\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial v_2}\\frac{\\partial v_2}{\\partial w_2} \\\\&= \\frac{\\partial L}{\\partial v_1}v_1\\end{aligned}\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nNote, that for the same price of computations as it was in the forward mode we have the full vector of gradient \\nabla_w L. Is it a free lunch? What is the cost of acceleration?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nNote, that for using the reverse mode AD you need to store all intermediate computations from the forward pass. This problem could be somehow mitigated with the gradient checkpointing approach, which involves necessary recomputations of some intermediate values. This could significantly reduce the memory footprint of the large machine-learning model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReverse mode automatic differentiation algorithm\n\n\n\n\n\nSuppose, we have a computational graph v_i, i \\in [1; N]. Our goal is to calculate the derivative of the output of this graph with respect to all inputs variable w, i.e. \\nabla_w v_N =  \\left( \\frac{\\partial v_N}{\\partial w_1}, \\ldots, \\frac{\\partial v_N}{\\partial w_d}\\right)^T. This idea implies propagation of the gradient of the function with respect to the intermediate variables from the end to the origin, that is why we can introduce the notation:\n\n\\overline{v_i}  = \\dfrac{\\partial L}{\\partial v_i} = \\dfrac{\\partial v_N}{\\partial v_i}\n\n\n\n\nIllustration of reverse chain rule to calculate the derivative of the function L with respect to the node v_i.\n\n\n\nFORWARD PASS\nFor i = 1, \\ldots, N:\n\nCompute and store the values of v_i as a function of its parents (inputs)\n\nBACKWARD PASS\nFor i = N, \\ldots, 1:\n\nCompute the derivative \\overline{v_i} using the backward chain rule and information from all of its children (outputs) (x_1, \\ldots, x_{t_i}): \n  \\overline{v_i} = \\dfrac{\\partial L}{\\partial v_i} = \\sum_{j = 1}^{t_i} \\dfrac{\\partial L}{\\partial x_j} \\dfrac{\\partial x_j}{\\partial v_i}\n  \n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nWhich of the AD modes would you choose (forward/ reverse) for the following computational graph of primitive arithmetic operations? Suppose, you are needed to compute the jacobian J = \\left\\{ \\dfrac{\\partial L_i}{\\partial w_j} \\right\\}_{i,j}\n\n\n\nWhich mode would you choose for calculating gradients there?\n\n\nNote, that the reverse mode computational time is proportional to the number of outputs here, while the forward mode works proportionally to the number of inputs there. This is why it would be a good idea to consider the forward mode AD.\n\n\n\nThis graph nicely illustrates the idea of choice between the modes. The n = 100 dimension is fixed and the graph presents the time needed for Jacobian calculation w.r.t. x for f(x) = Ax\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhich of the AD modes would you choose (forward/ reverse) for the following computational graph of primitive arithmetic operations? Suppose, you are needed to compute the jacobian J = \\left\\{ \\dfrac{\\partial L_i}{\\partial w_j} \\right\\}_{i,j}. Note, that G is an arbitrary computational graph\n\n\n\nWhich mode would you choose for calculating gradients there?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nIt is generally impossible to say it without some knowledge about the specific structure of the graph G. Note, that there are also plenty of advanced approaches to mix forward and reverse mode AD, based on the specific G structure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeedforward Architecture\n\n\n\n\n\n\n\n\nFeedforward neural network architecture\n\n\nFORWARD\n\nv_0 = x typically we have a batch of data x here as an input.\nFor k = 1, \\ldots, t-1, t:\n\nv_k = \\sigma(v_{k-1}w_k). Note, that practically speaking the data has dimension x  \\in \\mathbb{R}^{b \\times d}, where b is the batch size (for the single data point b=1). While the weight matrix w_k of a k layer has a shape n_{k-1} \\times n_k, where n_k is the dimension of an inner representation of the data.\n\nL = L(v_t) - calculate the loss function.\n\nBACKWARD\n\nv_{t+1} = L, \\dfrac{\\partial L}{\\partial L} = 1\nFor k = t, t-1, \\ldots, 1:\n\n\\underset{b \\times n_k}{\\dfrac{\\partial L}{\\partial v_k}} = \\underset{b \\times n_{k+1}}{\\dfrac{\\partial L}{\\partial v_{k+1}}} \\underset{n_{k+1} \\times n_k}{\\dfrac{\\partial v_{k+1}}{\\partial v_{k}}}\n\\underset{b \\times n_{k-1} \\cdot n_k}{\\dfrac{\\partial L}{\\partial w_k}} = \\underset{b \\times n_{k+1}}{\\dfrac{\\partial L}{\\partial v_{k+1}}} \\cdot  \\underset{n_{k+1} \\times n_{k-1} \\cdot n_k}{\\dfrac{\\partial v_{k+1}}{\\partial w_{k}}}\n\n\n\n\n\n\n\n\n\n\n\n\nGradient propagation through the linear least squares\n\n\n\n\n\n\n\n\nx could be found as a solution of linear system\n\n\nSuppose, we have an invertible matrix A and a vector b, the vector x is the solution of the linear system Ax = b, namely one can write down an analytical solution x = A^{-1}b, in this example we will show, that computing all derivatives \\dfrac{\\partial L}{\\partial A}, \\dfrac{\\partial L}{\\partial b}, \\dfrac{\\partial L}{\\partial x}, i.e. the backward pass, costs approximately the same as the forward pass.\nIt is known, that the differential of the function does not depend on the parametrization:\n\ndL = \\left\\langle\\dfrac{\\partial L}{\\partial x}, dx \\right\\rangle = \\left\\langle\\dfrac{\\partial L}{\\partial A}, dA \\right\\rangle + \\left\\langle\\dfrac{\\partial L}{\\partial b}, db \\right\\rangle\n\nGiven the linear system, we have:\n\n\\begin{align*}\nAx &= b \\\\\ndAx + Adx = db &\\to dx = A^{-1}(db - dAx)\n\\end{align*}\n\nThe straightforward substitution gives us:\n\n\\left\\langle\\dfrac{\\partial L}{\\partial x}, A^{-1}(db - dAx) \\right\\rangle = \\left\\langle\\dfrac{\\partial L}{\\partial A}, dA \\right\\rangle + \\left\\langle\\dfrac{\\partial L}{\\partial b}, db \\right\\rangle\n\n\n\\left\\langle -A^{-T}\\dfrac{\\partial L}{\\partial x} x^T, dA \\right\\rangle + \\left\\langle A^{-T}\\dfrac{\\partial L}{\\partial x},db \\right\\rangle = \\left\\langle\\dfrac{\\partial L}{\\partial A}, dA \\right\\rangle + \\left\\langle\\dfrac{\\partial L}{\\partial b}, db \\right\\rangle\n\nTherefore:\n\n\\dfrac{\\partial L}{\\partial A} = -A^{-T}\\dfrac{\\partial L}{\\partial x} x^T \\quad \\dfrac{\\partial L}{\\partial b} =  A^{-T}\\dfrac{\\partial L}{\\partial x}\n\nIt is interesting, that the most computationally intensive part here is the matrix inverse, which is the same as for the forward pass. Sometimes it is even possible to store the result itself, which makes the backward pass even cheaper.\n\n\n\n\n\n\n\n\n\n\nGradient propagation through the SVD\n\n\n\n\n\nSuppose, we have the rectangular matrix W \\in \\mathbb{R}^{m \\times n}, which has a singular value decomposition:\n\nW = U \\Sigma V^T, \\quad U^TU = I, \\quad V^TV = I, \\quad \\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_{\\min(m,n)})\n\n\nSimilarly to the previous example:\n\n\\begin{split}\nW &= U \\Sigma V^T \\\\\ndW &= dU \\Sigma V^T + U d\\Sigma V^T + U \\Sigma dV^T \\\\\nU^T dW V &= U^TdU \\Sigma V^TV + U^TU d\\Sigma V^TV + U^TU \\Sigma dV^TV \\\\\nU^T dW V &= U^TdU \\Sigma + d\\Sigma + \\Sigma dV^TV\n\\end{split}\n\nNote, that U^T U = I \\to dU^TU + U^T dU = 0. But also dU^TU = (U^T dU)^T, which actually involves, that the matrix U^TdU is antisymmetric:\n\n(U^T dU)^T +  U^T dU = 0 \\quad \\to \\quad \\text{diag}( U^T dU) = (0, \\ldots, 0)\n\nThe same logic could be applied to the matrix V and\n\n\\text{diag}(dV^T V) = (0, \\ldots, 0)\n\nAt the same time, the matrix d \\Sigma is diagonal, which means (look at the 1.) that\n\n\\text{diag}(U^T dW V) = d \\Sigma\n\nHere on both sides, we have diagonal matrices.\nNow, we can decompose the differential of the loss function as a function of \\Sigma - such problems arise in ML problems, where we need to restrict the matrix rank:\n\n\\begin{split}\ndL &= \\left\\langle\\dfrac{\\partial L}{\\partial \\Sigma}, d\\Sigma \\right\\rangle \\\\\n&= \\left\\langle\\dfrac{\\partial L}{\\partial \\Sigma}, \\text{diag}(U^T dW V)\\right\\rangle \\\\\n&= \\text{tr}\\left(\\dfrac{\\partial L}{\\partial \\Sigma}^T \\text{diag}(U^T dW V) \\right)\n\\end{split}\n\nAs soon as we have diagonal matrices inside the product, the trace of the diagonal part of the matrix will be equal to the trace of the whole matrix:\n\n\\begin{split}\ndL &= \\text{tr}\\left(\\dfrac{\\partial L}{\\partial \\Sigma}^T \\text{diag}(U^T dW V) \\right) \\\\\n&= \\text{tr}\\left(\\dfrac{\\partial L}{\\partial \\Sigma}^T U^T dW V \\right)  \\\\\n&= \\left\\langle\\dfrac{\\partial L}{\\partial \\Sigma}, U^T dW V \\right\\rangle \\\\\n&= \\left\\langle U \\dfrac{\\partial L}{\\partial \\Sigma} V^T, dW \\right\\rangle\n\\end{split}\n\nFinally, using another parametrization of the differential\n\n\\left\\langle U \\dfrac{\\partial L}{\\partial \\Sigma} V^T, dW \\right\\rangle = \\left\\langle\\dfrac{\\partial L}{\\partial W}, dW \\right\\rangle\n\n\n\\dfrac{\\partial L}{\\partial W} =  U \\dfrac{\\partial L}{\\partial \\Sigma} V^T,\n\nThis nice result allows us to connect the gradients \\dfrac{\\partial L}{\\partial W} and \\dfrac{\\partial L}{\\partial \\Sigma}.\n\n\n\n\n\n\n4.1 What automatic differentiation (AD) is NOT:\n\nAD is not a finite differences\nAD is not a symbolic derivative\nAD is not just the chain rule\nAD is not just backpropagation\nAD (reverse mode) is time-efficient and numerically stable\nAD (reverse mode) is memory inefficient (you need to store all intermediate computations from the forward pass). :::\n\n\n\n\nDifferent approaches for taking derivatives",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#important-stories-from-matrix-calculus",
    "href": "docs/methods/Autograd.html#important-stories-from-matrix-calculus",
    "title": "Automatic differentiation",
    "section": "5 Important stories from matrix calculus",
    "text": "5 Important stories from matrix calculus\nWe will illustrate some important matrix calculus facts for specific cases\n\n5.1 Univariate chain rule\nSuppose, we have the following functions R: \\mathbb{R} \\to \\mathbb{R} , L: \\mathbb{R} \\to \\mathbb{R} and W \\in \\mathbb{R}. Then\n\n\\dfrac{\\partial R}{\\partial W} = \\dfrac{\\partial R}{\\partial L} \\dfrac{\\partial L}{\\partial W}\n\n\n\n5.2 Multivariate chain rule\nThe simplest example:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), x_2(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\dfrac{\\partial f}{\\partial x_2} \\dfrac{\\partial x_2}{\\partial t}\n\nNow, we’ll consider f: \\mathbb{R}^n \\to \\mathbb{R}:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), \\ldots, x_n(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\ldots + \\dfrac{\\partial f}{\\partial x_n} \\dfrac{\\partial x_n}{\\partial t}\n\nBut if we will add another dimension f: \\mathbb{R}^n \\to \\mathbb{R}^m, than the j-th output of f will be:\n\n\\dfrac{\\partial }{\\partial t} f_j(x_1(t), \\ldots, x_n(t)) = \\sum\\limits_{i=1}^n \\dfrac{\\partial f_j}{\\partial x_i} \\dfrac{\\partial x_i}{\\partial t} = \\sum\\limits_{i=1}^n J_{ji}  \\dfrac{\\partial x_i}{\\partial t},\n\nwhere matrix J \\in \\mathbb{R}^{m \\times n} is the jacobian of the f. Hence, we could write it in a vector way:\n\n\\dfrac{\\partial f}{\\partial t} = J \\dfrac{\\partial x}{\\partial t}\\quad \\iff \\quad \\left(\\dfrac{\\partial f}{\\partial t}\\right)^\\top =  \\left( \\dfrac{\\partial x}{\\partial t}\\right)^\\top J^\\top\n\n\n\n5.3 Backpropagation\nBackpropagation is a specific application of reverse-mode automatic differentiation within neural networks. It is the standard algorithm for computing gradients in neural networks, especially for training with stochastic gradient descent. Here’s how it works:\n\nPerform a forward pass through the network to compute activations and outputs.\nCalculate the loss function at the output, which measures the difference between the network prediction and the actual target values.\nCommence the backward pass by computing the gradient of the loss with respect to the network’s outputs.\nPropagate these gradients back through the network, layer by layer, using the chain rule to calculate the gradients of the loss with respect to each weight and bias.\nThe critical point of backpropagation is that it efficiently calculates the gradient of a complex, multilayered function by decomposing it into simpler derivative calculations. This aspect makes the update of a large number of parameters in deep networks computationally feasible.\n\n\n\n5.4 Jacobian vector product\nThe power of automatic differentiation is encapsulated in the computation of the Jacobian-vector product. Instead of calculating the entire Jacobian matrix, which is computationally expensive and often unnecessary, AD computes the product of the Jacobian and a vector directly. This is crucial for gradients in neural networks where the Jacobian may be very large, but the end goal is the product of this Jacobian with the gradient of the loss with respect to the outputs (vector). The reason why it works so fast in practice is that the Jacobian of the operations is already developed effectively in automatic differentiation frameworks. Typically, we even do not construct or store the full Jacobian, doing matvec directly instead. Note, for some functions (for example, any element-wise function of the input vector) matvec costs linear time, instead of quadratic and requires no additional memory to store a Jacobian.\n\n\n\n\n\n\nExample: element-wise exponent\n\n\n\n\n\n\ny = \\exp{(z)} \\qquad J = \\text{diag}(\\exp(z)) \\qquad \\overline{z} = \\overline{y} J\n\n\n\n\n\nSee the examples of Vector-Jacobian Products from the autodidact library:\ndefvjp(anp.add,         lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, g))\ndefvjp(anp.multiply,    lambda g, ans, x, y : unbroadcast(x, y * g),\n                        lambda g, ans, x, y : unbroadcast(y, x * g))\ndefvjp(anp.subtract,    lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, -g))\ndefvjp(anp.divide,      lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\ndefvjp(anp.true_divide, lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\n\n\n5.5 Hessian vector product\nInterestingly, a similar idea could be used to compute Hessian-vector products, which is essential for second-order optimization or conjugate gradient methods. For a scalar-valued function f : \\mathbb{R}^n \\to \\mathbb{R} with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point x \\in \\mathbb{R}^n is written as \\partial^2 f(x). A Hessian-vector product function is then able to evaluate\n\nv \\mapsto \\partial^2 f(x) \\cdot v\n\nfor any vector v \\in \\mathbb{R}^n.\nThe trick is not to instantiate the full Hessian matrix: if n is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store. Luckily, grad (in the jax/autograd/pytorch/tensorflow) already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity\n\n\\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x),\n\nwhere g(x) = \\partial f(x) \\cdot v is a new vector-valued function that dots the gradient of f at x with the vector v. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know grad is efficient.\nimport jax.numpy as jnp\n\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#code",
    "href": "docs/methods/Autograd.html#code",
    "title": "Automatic differentiation",
    "section": "6 Code",
    "text": "6 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#materials",
    "href": "docs/methods/Autograd.html#materials",
    "title": "Automatic differentiation",
    "section": "7 Materials",
    "text": "7 Materials\n\nAutodidact - a pedagogical implementation of Autograd\nCSC321 Lecture 6\nCSC321 Lecture 10\nWhy you should understand backpropagation :)\nJAX autodiff cookbook\nMaterials from CS207: Systems Development for Computational Science course with very intuitive explanation.\nGreat lecture on AD from Dmitry Kropotov (in Russian).",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/Autograd.html#footnotes",
    "href": "docs/methods/Autograd.html#footnotes",
    "title": "Automatic differentiation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLinnainmaa S. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master’s Thesis (in Finnish), Univ. Helsinki, 1970.↩︎",
    "crumbs": [
      "Methods",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html",
    "href": "docs/methods/adaptive_metrics/CG.html",
    "title": "Conjugate gradients",
    "section": "",
    "text": "Illustration\n\n\nOriginally, the conjugate gradients method was created to solve a system of linear equations.\n\nAx = b\n\nWithout special efforts the problem can be presented in the form of minimization of the quadratic function, and then generalized on a case of non quadratic function. We will start with the parabolic case and try to construct a conjugate gradients method for it. Let us consider the classical problem of minimization of the quadratic function:\n\nf(x) = \\frac{1}{2}x^\\top A x - b^\\top x + c \\to \\min\\limits_{x \\in \\mathbb{R}^n }\n\nHere x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{n \\times n}, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#introduction",
    "href": "docs/methods/adaptive_metrics/CG.html#introduction",
    "title": "Conjugate gradients",
    "section": "",
    "text": "Illustration\n\n\nOriginally, the conjugate gradients method was created to solve a system of linear equations.\n\nAx = b\n\nWithout special efforts the problem can be presented in the form of minimization of the quadratic function, and then generalized on a case of non quadratic function. We will start with the parabolic case and try to construct a conjugate gradients method for it. Let us consider the classical problem of minimization of the quadratic function:\n\nf(x) = \\frac{1}{2}x^\\top A x - b^\\top x + c \\to \\min\\limits_{x \\in \\mathbb{R}^n }\n\nHere x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{n \\times n}, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#method-of-conjugate-gradients-for-the-quadratic-function",
    "href": "docs/methods/adaptive_metrics/CG.html#method-of-conjugate-gradients-for-the-quadratic-function",
    "title": "Conjugate gradients",
    "section": "2 Method of conjugate gradients for the quadratic function",
    "text": "2 Method of conjugate gradients for the quadratic function\nWe will consider symmetric matrices A \\in \\mathbb{S}^n (otherwise, replacing A' = \\frac{A + A^\\top}{2} leads to the same optimization problem). Then:\n\n\\nabla f = Ax - b\n\nThen having an initial guess x_0, vector d_0 = -\\nabla f(x_0) is the direction of the fastest decrease. The procedure of the steepest descent in this direction is provided by the procedure of line search:\n\n\\begin{align*}\ng(\\alpha) &= f(x_0 + \\alpha d_0) \\\\\n          &= \\frac{1}{2}(x_0 + \\alpha d_0)^\\top A (x_0 + \\alpha d_0) - b^\\top (x_0 + \\alpha d_0) + c\\\\\n          &= \\frac{1}{2}\\alpha^2 {d_0}^\\top A d_0 + {d_0}^\\top (A x_0 - b) \\alpha + (\\frac{1}{2} {x_0}^\\top A x_0 + {x_0}^\\top d_0 + c)\n\\end{align*}\n\nAssuming that the point of the zero derivative in this parabola is the minimum (for positive matrices it is guaranteed, otherwise it is not a fact), and also, rewriting this problem for the arbitrary (k) direction of the method, we have:\n\ng'(\\alpha_k) = (d_k^\\top A d_k)\\alpha_k + d_k^\\top(A x_k - b) = 0\n\n\n\\alpha_k = -\\frac{d_k^\\top (A x_k - b)}{d_k^\\top A d_k} = \\dfrac{d_k^\\top d_k}{d_k^\\top A d_k}.\n\nThen let’s start our method, as the method of the steepest descent:\n\nx_1 = x_0 - \\alpha_0 \\nabla f(x_0)\n\nNote, however, that if the next step is built in the same way (the fastest descent), we will “lose” some of the work that was done in the first step and we will get a classic situation for the fastest descent:\n\n\n\nhttp://fourier.eng.hmc.edu/e176/lectures/\n\n\nIn order to avoid this, we introduce the concept of A-conjugate vectors: let’s say that two vectors x, y are A-conjugate relative to each other if they are executed:\n\nx^\\top A y = 0\n\nThis concept becomes particularly interesting when matrix A is positive defined, then x,y vectors will be orthogonal if the scalar product is defined by the matrix A. Therefore, this property is also called A - orthogonality.\n\n\n\nIllustration\n\n\nThen we will build the method in such a way that the next direction is A - orthogonal with the previous one:\n\nd_1 = -\\nabla f(x_1) + \\beta_0 d_0,\n\nwhere \\beta_0 is selected in a way that d_1 \\perp_A d_0:\n\nd_1^\\top A d_0 = -\\nabla f(x_1)^\\top Ad_0 + \\beta_0 d_0^\\top A d_0 = 0\n\n\n\\beta_0 = \\frac{\\nabla f(x_1)^\\top A d_0}{d_0^\\top A d_0}\n\nIt’s interesting that all received A directions are A- orthogonal to each other. (proved by induction)\nThus, we formulate an algorithm:\n\nLet k = 0 and x_k = x_0, count d_k = d_0 = -\\nabla f(x_0).\nBy the procedure of line search we find the optimal length of step:\n\nCalculate \\alpha minimizing f(x_k + \\alpha_k d_k) by the formula\n\n  \\alpha_k = -\\frac{d_k^\\top (A x_k - b)}{d_k^\\top A d_k}\n  \n\nWe’re doing an algorithm step:\n\n\n  x_{k+1} = x_k + \\alpha_k d_k\n  \n\nupdate the direction: d_{k+1} = -\\nabla f(x_{k+1}) + \\beta_k d_k, where \\beta_k is calculated by the formula:\n\n\n  \\beta_k = \\frac{\\nabla f(x_{k+1})^\\top A d_k}{d_k^\\top A d_k}.\n  \n\nRepeat steps 2-4 until n directions are built, where n is the dimension of space (dimension of x).",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#method-of-conjugate-gradients-for-non-quadratic-function",
    "href": "docs/methods/adaptive_metrics/CG.html#method-of-conjugate-gradients-for-non-quadratic-function",
    "title": "Conjugate gradients",
    "section": "3 Method of conjugate gradients for non-quadratic function:",
    "text": "3 Method of conjugate gradients for non-quadratic function:\nIn case we do not have an analytic expression for a function or its gradient, we will most likely not be able to solve the one-dimensional minimization problem analytically. Therefore, step 2 of the algorithm is replaced by the usual line search procedure. But there is the following mathematical trick for the fourth point:\nFor two iterations, it is fair:\n\nx_{k+1} - x_k = c d_k,\n\nwhere c is some kind of constant. Then for the quadratic case, we have:\n\n\\nabla f(x_{k+1}) - \\nabla f(x_k) = (A x_{k+1} - b) - (A x_k - b) = A(x_{k+1}-x_k) = cA d_k\n\nExpressing from this equation the work Ad_k = \\dfrac{1}{c} \\left( \\nabla f(x_{k+1}) - \\nabla f(x_k)\\right), we get rid of the “knowledge” of the function in step definition \\beta_k, then point 4 will be rewritten as:\n\n\\beta_k = \\frac{\\nabla f(x_{k+1})^\\top (\\nabla f(x_{k+1}) - \\nabla f(x_k))}{d_k^\\top (\\nabla f(x_{k+1}) - \\nabla f(x_k))}.\n\nThis method is called the Polack - Ribier method.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#examples",
    "href": "docs/methods/adaptive_metrics/CG.html#examples",
    "title": "Conjugate gradients",
    "section": "4 Examples",
    "text": "4 Examples\n\n4.1 Example 1\nProve that if a set of vectors d_1, \\ldots, d_k - are A-conjugate (each pair of vectors is A-conjugate), these vectors are linearly independent. A \\in \\mathbb{S}^n_{++}.\nSolution:\nWe’ll show, that if \\sum\\limits_{i=1}^k\\alpha_k d_k = 0, than all coefficients should be equal to zero:\n\n  \\begin{align*}\n  0 &= \\sum\\limits_{i=1}^n\\alpha_k d_k \\\\\n    &= d_j^\\top A \\left( \\sum\\limits_{i=1}^n\\alpha_k d_k \\right) \\\\\n    &=  \\sum\\limits_{i=1}^n \\alpha_k d_j^\\top A d_k  \\\\\n    &=  \\alpha_j d_j^\\top A d_j  + 0 + \\ldots + 0\\\\\n  \\end{align*}\n  \nThus, \\alpha_j = 0, for all other indices one have perform the same process",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#references",
    "href": "docs/methods/adaptive_metrics/CG.html#references",
    "title": "Conjugate gradients",
    "section": "5 References",
    "text": "5 References\n\nAn Introduction to the Conjugate Gradient Method Without the Agonizing Pain\nThe Concept of Conjugate Gradient Descent in Python by Ilya Kuzovkin\nPicture of bestinitial guess in SD",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#code",
    "href": "docs/methods/adaptive_metrics/CG.html#code",
    "title": "Conjugate gradients",
    "section": "6 Code",
    "text": "6 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Conjugate gradients"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html",
    "href": "docs/methods/adaptive_metrics/Newton.html",
    "title": "Newton method",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\n\n\nIllustration\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}.\n\n\n\nLet us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\nIllustration\n\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\nYour browser does not support the video tag.\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#intuition",
    "href": "docs/methods/adaptive_metrics/Newton.html#intuition",
    "title": "Newton method",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\n\n\nIllustration\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}.\n\n\n\nLet us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\nIllustration\n\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\nYour browser does not support the video tag.\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#convergence",
    "href": "docs/methods/adaptive_metrics/Newton.html#convergence",
    "title": "Newton method",
    "section": "2 Convergence",
    "text": "2 Convergence\nYour browser does not support the video tag.\nLet’s try to get an estimate of how quickly the classical Newton method converges. We will try to enter the necessary data and constants as needed in the conclusion (to illustrate the methodology of obtaining such estimates).\n\n\\begin{align*}\nx_{k+1} - x^* = x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) - x^* = x_k - x^* -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) = \\\\\n= x_k - x^* - \\left[ f''(x_k)\\right]^{-1}  \\int_0^1 f''(x^* + \\tau (x_k - x^*))  (x_k - x^*) d\\tau = \\\\\n= \\left( 1 - \\left[ f''(x_k)\\right]^{-1} \\int_0^1 f''(x^* + \\tau (x_k - x^*)) d \\tau\\right) (x_k - x^*)= \\\\\n= \\left[ f''(x_k)\\right]^{-1} \\left( f''(x_k) - \\int_0^1 f''(x^* + \\tau (x_k - x^*)) d \\tau\\right) (x_k - x^*) = \\\\\n= \\left[ f''(x_k)\\right]^{-1} \\left( \\int_0^1 \\left( f''(x_k) - f''(x^* + \\tau (x_k - x^*)) d \\tau\\right)\\right) (x_k - x^*)= \\\\\n= \\left[ f''(x_k)\\right]^{-1} G_k (x_k - x^*)\n\\end{align*}\n\nUsed here is: G_k = \\int_0^1 \\left( f''(x_k) - f''(x^* + \\tau (x_k - x^*)) d \\tau\\right). Let’s try to estimate the size of G_k:\n\n\\begin{align*}\n\\| G_k\\| = \\left\\| \\int_0^1 \\left( f''(x_k) - f''(x^* + \\tau (x_k - x^*)) d \\tau\\right)\\right\\| \\leq \\\\\n\\leq \\int_0^1 \\left\\| f''(x_k) - f''(x^* + \\tau (x_k - x^*))   \\right\\|d\\tau \\leq \\qquad \\text{(Hessian's Lipschitz continuity)}\\\\\n\\leq \\int_0^1 M\\|x_k - x^* - \\tau (x_k - x^*)\\| d \\tau = \\int_0^1 M\\|x_k - x^*\\|(1- \\tau)d \\tau = \\frac{r_k}{2}M,\n\\end{align*}\n\nwhere r_k = \\| x_k - x^* \\|.\nSo, we have:\n\nr_{k+1}  \\leq \\left\\|\\left[ f''(x_k)\\right]^{-1}\\right\\| \\cdot \\frac{r_k}{2}M \\cdot r_k\n\nAlready smells like quadratic convergence. All that remains is to estimate the value of Hessian’s reverse.\nBecause of Hessian’s Lipschitz continuity and symmetry:\n\n\\begin{align*}\nf''(x_k) - f''(x^*) \\succeq - Mr_k I_n \\\\\nf''(x_k) \\succeq f''(x^*) - Mr_k I_n \\\\\nf''(x_k) \\succeq \\mu I_n - Mr_k I_n \\\\\nf''(x_k) \\succeq (\\mu- Mr_k )I_n \\\\\n\\end{align*}\n\nSo, (here we should already limit the necessity of being f''(x_k) \\succ 0 for such estimations, i.e. r_k &lt; \\frac{\\mu}{M}).\n\n\\begin{align*}\n\\left\\|\\left[ f''(x_k)\\right]^{-1}\\right\\| \\leq (\\mu - Mr_k)^{-1}\n\\end{align*}\n\n\nr_{k+1}  \\leq \\dfrac{r_k^2 M}{2(\\mu - Mr_k)}\n\nThe convergence condition r_{k+1} &lt; r_k imposes additional conditions on r_k:  \\;\\;\\; r_k &lt; \\frac{2 \\mu}{3M}\nThus, we have an important result: Newton’s method for the function with Lipschitz positive Hessian converges quadratically near (\\| x_0 - x^* \\| &lt; \\frac{2 \\mu}{3M}) to the solution.\n\n2.1 Theorem\nLet f(x) be a strongly convex twice continuously differentiated function at \\mathbb{R}^n, for the second derivative of which inequalities are executed: \\mu I_n\\preceq f''(x) \\preceq L I_n. Then Newton’s method with a constant step locally converges to solving the problem with superlinear speed. If, in addition, Hessian is Lipschitz continuous, then this method converges locally to x^* at a quadratic rate.",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#summary",
    "href": "docs/methods/adaptive_metrics/Newton.html#summary",
    "title": "Newton method",
    "section": "3 Summary",
    "text": "3 Summary\nIt’s nice:\n\nquadratic convergence near the solution x^*\naffinity invariance\nthe parameters have little effect on the convergence rate\n\nIt’s not nice:\n\nit is necessary to store the hessian on each iteration: \\mathcal{O}(n^2) memory\nit is necessary to solve linear systems: \\mathcal{O}(n^3) operations\nthe Hessian can be degenerate at x^*\nthe hessian may not be positively determined \\to direction -(f''(x))^{-1}f'(x) may not be a descending direction\n\n\n3.1 Possible directions\n\nNewton’s damped method (adaptive stepsize)\nQuasi-Newton methods (we don’t calculate the Hessian, we build its estimate - BFGS)\nQuadratic evaluation of the function by the first order oracle (superlinear convergence)\nThe combination of the Newton method and the gradient descent (interesting direction)\nHigher order methods (most likely useless)",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#materials",
    "href": "docs/methods/adaptive_metrics/Newton.html#materials",
    "title": "Newton method",
    "section": "4 Materials",
    "text": "4 Materials\n\nGoing beyond least-squares – I : self-concordant analysis of Newton method\nGoing beyond least-squares – II : Self-concordant analysis for logistic regression\nPicture with gradient and Newton field was taken from this tweet by Keenan Crane.\nAbout global damped Newton convergence issue. Open In Colab",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#code",
    "href": "docs/methods/adaptive_metrics/Newton.html#code",
    "title": "Newton method",
    "section": "5 Code",
    "text": "5 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "Adaptive metric methods",
      "Newton method"
    ]
  },
  {
    "objectID": "docs/methods/adaptive_metrics/index.html",
    "href": "docs/methods/adaptive_metrics/index.html",
    "title": "Adaptive metric methods",
    "section": "",
    "text": "It is known, that antigradient -\\nabla f (x_0) is the direction of the steepest descent of the function f(x) at point x_0. However, we can introduce another concept for choosing the best direction of function decreasing.\nGiven f(x) and a point x_0. Define B_\\varepsilon(x_0) = \\{x \\in \\mathbb{R}^n : d(x, x_0) = \\varepsilon^2 \\} as the set of points with distance \\varepsilon to x_0. Here we presume the existence of a distance function d(x, x_0).\n\nx^* = \\text{arg}\\min_{x \\in B_\\varepsilon(x_0)} f(x)\n\nThen, we can define another steepest descent direction in terms of minimizer of function on a sphere:\n\ns = \\lim_{\\varepsilon \\to 0} \\frac{x^* - x_0}{\\varepsilon}\n\nLet us assume that the distance is defined locally by some metric A:\n\nd(x, x_0) = (x-x_0)^\\top A (x-x_0)\n\nLet us also consider first order Taylor approximation of a function f(x) near the point x_0:\n\n\\tag{A1}\nf(x_0 + \\delta x) \\approx f(x_0) + \\nabla f(x_0)^\\top \\delta x\n\nNow we can explicitly pose a problem of finding s, as it was stated above.\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} f(x_0 + \\delta x) \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing \\text{(A1)} it can be written as:\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} \\nabla f(x_0)^\\top \\delta x \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing Lagrange multipliers method, we can easily conclude, that the answer is:\n\n\\delta x = - \\frac{2 \\varepsilon^2}{\\nabla f (x_0)^\\top A^{-1} \\nabla f (x_0)} A^{-1} \\nabla f\n\nWhich means, that new direction of steepest descent is nothing else, but A^{-1} \\nabla f(x_0).\nIndeed, if the space is isotropic and A = I, we immediately have gradient descent formula, while Newton method uses local Hessian as a metric matrix.\n\n\n\n\n\n\n\n\nNewton method\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuasi Newton methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nConjugate gradients\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural gradient descent\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Methods",
      "Adaptive metric methods"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#summary",
    "href": "docs/methods/fom/GD.html#summary",
    "title": "Gradient descent",
    "section": "1 Summary",
    "text": "1 Summary\nA classical problem of function minimization is considered.\n\n\\tag{GD}\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\n\nThe bottleneck (for almost all gradient methods) is choosing step-size, which can lead to the dramatic difference in method’s behavior.\nOne of the theoretical suggestions: choosing stepsize inversly proportional to the gradient Lipschitz constant \\eta_k = \\dfrac{1}{L}.\nIn huge-scale applications the cost of iteration is usually defined by the cost of gradient calculation (at least \\mathcal{O}(p)).\nIf function has Lipschitz-continious gradient, then method could be rewritten as follows:\n\n\n\\begin{align*}x_{k+1} &= x_{k}-\\dfrac{1}{L} \\nabla f\\left(x_{k}\\right)= \\\\\n&= \\arg \\min\\limits_{x \\in \\mathbb{R}^{n}}\\left\\{f\\left(x_{k}\\right)+\\left\\langle\\nabla f\\left(x_{k}\\right), x-x_{k}\\right\\rangle+\\frac{L}{2}\\left\\|x-x_{k}\\right\\|_{2}^{2}\\right\\} \\end{align*}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#intuition",
    "href": "docs/methods/fom/GD.html#intuition",
    "title": "Gradient descent",
    "section": "2 Intuition",
    "text": "2 Intuition\n\n2.1 Direction of local steepest descent\nLet’s consider a linear approximation of the differentiable function f along some direction h, \\|h\\|_2 = 1:\n\nf(x + \\eta h) = f(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta)\n\nWe want h to be a decreasing direction:\n\nf(x + \\eta h) &lt; f(x)\n\n\nf(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta) &lt; f(x)\n\nand going to the limit at \\eta \\rightarrow 0:\n\n\\langle f'(x), h \\rangle \\leq 0\n\nAlso from Cauchy–Bunyakovsky–Schwarz inequality:\n\n|\\langle f'(x), h \\rangle | \\leq \\| f'(x) \\|_2 \\| h \\|_2 \\;\\;\\;\\to\\;\\;\\; \\langle f'(x), h \\rangle \\geq -\\| f'(x) \\|_2 \\| h \\|_2 = -\\| f'(x) \\|_2\n\nThus, the direction of the antigradient\n\nh = -\\dfrac{f'(x)}{\\|f'(x)\\|_2}\n\ngives the direction of the steepest local decreasing of the function f.\nThe result of this method is\n\nx_{k+1} = x_k - \\eta f'(x_k)\n\n\n\n2.2 Gradient flow ODE\nLet’s consider the following ODE, which is referred as Gradient Flow equation.\n\n\\tag{GF}\n\\frac{dx}{dt} = -f'(x(t))\n\nand discretize it on a uniform grid with \\eta step:\n\n\\frac{x_{k+1} - x_k}{\\eta} = -f'(x_k),\n\nwhere x_k \\equiv x(t_k) and \\eta = t_{k+1} - t_k - is the grid step.\nFrom here we get the expression for x_{k+1}\n\nx_{k+1} = x_k - \\eta f'(x_k),\n\nwhich is exactly gradient descent.\n\n\n2.3 Necessary local minimum condition\n\n\\begin{align*}\n& f'(x) = 0\\\\\n& -\\eta f'(x) = 0\\\\\n& x - \\eta f'(x) = x\\\\\n& x_k - \\eta f'(x_k) = x_{k+1}\n\\end{align*}\n\nThis is, surely, not a proof at all, but some kind of intuitive explanation.\n\n\n2.4 Minimizer of Lipschitz parabola\nSome general highlights about Lipschitz properties are needed for explanation. If a function f: \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and its gradient satisfies Lipschitz conditions with constant L, then \\forall x,y \\in \\mathbb{R}^n:\n\n|f(y) - f(x) - \\langle \\nabla f(x), y-x \\rangle| \\leq \\frac{L}{2} \\|y-x\\|^2,\n\nwhich geometrically means, that if we’ll fix some point x_0 \\in \\mathbb{R}^n and define two parabolas:\n\n\\phi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2} \\|x-x_0\\|^2,\n\n\n\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2} \\|x-x_0\\|^2.\n\nThen\n\n\\phi_1(x) \\leq f(x) \\leq \\phi_2(x) \\quad \\forall x \\in \\mathbb{R}^n.\n\nNow, if we have global upper bound on the function, in a form of parabola, we can try to go directly to its minimum.\n\n\\begin{align*}\n& \\nabla \\phi_2(x) = 0 \\\\\n& \\nabla f(x_0) + L (x^* - x_0) = 0 \\\\\n& x^* = x_0 - \\frac{1}{L}\\nabla f(x_0) \\\\\n& x_{k+1} = x_k - \\frac{1}{L} \\nabla f(x_k)\n\\end{align*}\n\n\n\n\nIllustration\n\n\nThis way leads to the \\frac{1}{L} stepsize choosing. However, often the L constant is not known.\nBut if the function is twice continuously differentiable and its gradient has Lipschitz constant L, we can derive a way to estimate this constant \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x) \\| \\leq L\n\nor\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#stepsize-choosing-strategies",
    "href": "docs/methods/fom/GD.html#stepsize-choosing-strategies",
    "title": "Gradient descent",
    "section": "3 Stepsize choosing strategies",
    "text": "3 Stepsize choosing strategies\nStepsize choosing strategy \\eta_k significantly affects convergence. General line search algorithms might help in choosing scalar parameter.\n\n3.1 Constant stepsize\nFor f \\in C_L^{1,1}:\n\n\\eta_k = \\eta\n\n\nf(x_k) - f(x_{k+1}) \\geq \\eta \\left(1 - \\frac{1}{2}L\\eta \\right) \\|\\nabla f(x_k)\\|^2\n\nWith choosing \\eta = \\frac{1}{L}, we have:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2\n\n\n\n3.2 Fixed sequence\n\n\\eta_k = \\dfrac{1}{\\sqrt{k+1}}\n\nThe latter 2 strategies are the simplest in terms of implementation and analytical analysis. It is clear that this approach does not often work very well in practice (the function geometry is not known in advance).\n\n\n3.3 Exact line search aka steepest descent\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_{k+1}) = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nMore theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.\nInteresting theoretical property of this method is that each following iteration is orthogonal to the previous one:\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nOptimality conditions:\n\n\\nabla f(x_{k+1})^\\top \\nabla f(x_k) = 0\n\n\n\n3.4 Goldstein-Armijo",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#convergence-analysis",
    "href": "docs/methods/fom/GD.html#convergence-analysis",
    "title": "Gradient descent",
    "section": "4 Convergence analysis",
    "text": "4 Convergence analysis\n\n4.1 Convex case\n\n4.1.1 Lipischitz continuity of the gradient\nAssume that f: \\mathbb{R}^n \\to \\mathbb{R} is convex and differentiable, and additionally \n\\|\\nabla f(x) − \\nabla f(y) \\| \\leq L \\|x − y \\| \\; \\forall x, y \\in \\mathbb{R}^n\n\ni.e. , \\nabla f is Lipschitz continuous with constant L &gt; 0.\nSince \\nabla f Lipschitz with constant L, which means \\nabla^2 f \\preceq LI, we have \\forall x, y, z:\n\n(x − y)^\\top(\\nabla^2 f(z) − LI)(x − y) \\leq 0\n\n\n(x − y)^\\top\\nabla^2 f(z)(x − y) \\leq L \\|x-y\\|^2\n\nNow we’ll consider second order Taylor approximation of f(y) and Taylor’s Remainder Theorem (we assume, that the function f is continuously differentiable), we have \\forall x, y, \\exists z ∈ [x, y]:\n\n\\begin{align*}\nf(y) &= f(x) + \\nabla f(x)^\\top(y − x) + \\frac{1}{2}(x − y)^\\top \\nabla^2 f(z)(x − y) \\\\\n& \\leq f(x) + \\nabla f(x)^\\top(y − x) + \\frac{L}{2} \\|x-y\\|^2\n\\end{align*}\n\nFor the gradient descent we have x = x_k, y = x_{k+1}, x_{k+1} = x_k - \\eta_k\\nabla f(x_k):\n\n\\begin{align*}\nf(x_{k+1}) &\\leq  f(x_k) + \\nabla f(x_k)^\\top(-\\eta_k\\nabla f(x_k)) + \\frac{L}{2} \\| \\eta_k\\nabla f(x_k) \\|^2  \\\\\n& \\leq f(x_k) - \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\|\\nabla f(x_k)\\|^2\n\\end{align*}\n\n\n\n4.1.2 Optimal constant stepsize\nNow, if we’ll consider constant stepsize strategy and will maximize \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\to \\max\\limits_{\\eta}, we’ll get \\eta = \\dfrac{1}{L}.\n\nf(x_{k+1}) \\leq f(x_k) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2\n\n\n\n4.1.3 Convexity\n\nf(x_{k}) \\leq f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*)\n\nThat’s why we have:\n\n\\begin{align*}\nf(x_{k+1}) & \\leq  f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2 \\\\\n& = f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_k − x^* − \\dfrac{1}{L}\\nabla f(x_k)\\|^2\\right) \\\\\n& =  f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_{k+1} − x^*\\|^2\\right)\n\\end{align*}\n\nThus, summing over all iterations, we have:\n\n\\begin{align*}\n\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) &\\leq \\dfrac{L}{2} \\left(\\|x_0 − x^*\\|^2 − \\|x_{k} − x^*\\|^2\\right) \\\\\n& \\leq  \\dfrac{L}{2} \\|x_0 − x^*\\|^2 =  \\dfrac{LR^2}{2},\n\\end{align*}\n\nwhere R = \\|x_0 - x^*\\|. And due to function monotonicity:\n\nf(x_k) - f(x^*) \\leq \\dfrac{1}{k}\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) \\leq \\dfrac{LR^2}{2k} = \\dfrac{R^2}{2\\eta k}\n\n\n\n\n4.2 Strongly convex case\nIf the function is strongly convex:\n\nf(y) \\geq f(x) + \\nabla f(x)^\\top (y − x) + \\dfrac{\\mu}{2}\\|y − x \\|^2 \\; \\forall x, y \\in \\mathbb{R}^n\n\n…\n\n\\|x_{k+1} − x^*\\|^2 \\leq (1 − \\eta \\mu)\\|x_k − x^* \\|^2\n ## Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert f(x_k) - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert x_k - x^* \\Vert \\leq\n\n\n\n\nConvexLipschitz-continuous function(G)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{GR}{k}\nSublinear\n\n\n\nConvexLipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{LR^2}{k}\nSublinear\n\n\n\n\\mu-Strongly convexLipschitz-continuous gradient(L)\n\nLinear\n(1 - \\eta \\mu)^k R^2\n\n\n\\mu-Strongly convexLipschitz-continuous hessian(M)\n\nLocally linear R &lt; \\overline{R}\n\\dfrac{\\overline{R}R}{\\overline{R} - R} \\left( 1 - \\dfrac{2\\mu}{L+3\\mu}\\right)\n\n\n\n\nR = \\| x_0 - x^*\\| - initial distance\n\\overline{R} = \\dfrac{2\\mu}{M}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/GD.html#materials",
    "href": "docs/methods/fom/GD.html#materials",
    "title": "Gradient descent",
    "section": "5 Materials",
    "text": "5 Materials\n\nThe zen of gradient descent. Moritz Hardt\nGreat visualization\nCheatsheet on the different convergence theorems proofs",
    "crumbs": [
      "Methods",
      "First order methods",
      "Gradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html",
    "href": "docs/methods/fom/Mirror_descent.html",
    "title": "Mirror descent",
    "section": "",
    "text": "Метод зеркального спуска является естественным обобщением метода проекции субградиента в случае обобщения l_2 нормы на более общий случай какой-то функции расстояния.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#возвращение-к-истокам",
    "href": "docs/methods/fom/Mirror_descent.html#возвращение-к-истокам",
    "title": "Mirror descent",
    "section": "1 Возвращение к истокам",
    "text": "1 Возвращение к истокам\nПусть задано выпуклое замкнутое множество S \\in \\mathbb{R}^n, кроме того, есть алгоритм оптимизации, возвращающий последовательность точек x_1, \\ldots, x_k, \\ldots. Тогда запишем (не)равенство треугольника для расстояния Брэгмана, полагая y = x_{k+1}, x = x_k и произвольный z \\in S (который мы в дальнейшем для целостности изложения будем обозначать y)\n\n\\begin{align*}\n\\langle -\\nabla V_{x_k}(x_{k+1}), x_{k+1}-z\\rangle &= V_{x_k}(z) - V_{x_{k+1}}(z) - V_{x_k}(x_{k+1}) \\\\\n\\tag{baseMD}\n\\langle -\\nabla V_{x_k}(x_{k+1}), x_{k+1}-y\\rangle &= V_{x_k}(y) - V_{x_{k+1}}(y) - V_{x_k}(x_{k+1})\n\\end{align*}\n\nПросуммируем полученные равенства:\n\n\\sum\\limits_{k = 0}^{T-1}\\langle -\\nabla V_{x_k}(x_{k+1}), x_{k+1}-y\\rangle = V_{x_0}(y) - V_{x_{T}}(y) - \\sum\\limits_{k = 0}^{T-1}V_{x_k}(x_{k+1})\n\nИмея ввиду полученное уравнение, давайте, наконец, попробуем сформулировать метод зеркального спуска:\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right)\n\nПосмотрим внимательнее на условие проекции для точки x_{k+1}:\n\n\\langle \\alpha_k g_k,x_{k+1} - y\\rangle + \\langle \\nabla V_{x_k}(x_{k+1}),x_{k+1} - y\\rangle \\leq 0\n\n\n\\langle \\alpha_k g_k,x_{k+1} - y\\rangle \\leq - \\langle \\nabla V_{x_k}(x_{k+1}),x_{k+1} - y\\rangle\n\nПопробуем теперь получить наше базовое неравенство, используя (baseMD):\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_k - y\\rangle &\\leq - \\langle \\nabla V_{x_k}(x_{k+1}),x_{k+1} - y\\rangle - \\langle \\alpha_k g_k, x_{k+1} - x_k\\rangle = \\\\\n& = V_{x_k}(y) - V_{x_{k+1}}(y) - V_{x_k}(x_{k+1})- \\langle \\alpha_k g_k, x_{k+1} - x_k\\rangle \\leq\\\\\n&\\leq V_{x_k}(y) - V_{x_{k+1}}(y) - \\frac{1}{2}\\|x_k - x_{k+1}\\|^2- \\langle \\alpha_k g_k, x_{k+1} - x_k\\rangle \\leq \\\\\n&\\leq V_{x_k}(y) - V_{x_{k+1}}(y) + \\left(\\langle \\alpha_k g_k, x_k - x_{k+1}\\rangle- \\frac{1}{2}\\|x_k - x_{k+1}\\|^2 \\right) \\leq \\\\\n&\\leq V_{x_k}(y) - V_{x_{k+1}}(y) + \\frac{\\alpha_k^2}{2} \\|g_k\\|_*^2\n\\end{align*}\n\nТЕЛЕСКОПИРУЕМ\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle \\alpha_k g_k, x_k - y\\rangle &\\leq  V_{x_0}(y) - V_{x_{T}}(y) + \\sum\\limits_{k = 0}^{T-1}\\frac{\\alpha_k^2}{2} \\|g_k\\|_*^2 \\\\\n&\\leq V_{x_0}(y)  + \\sum\\limits_{k = 0}^{T-1}\\frac{\\alpha_k^2}{2} \\|g_k\\|_*^2 \\\\\n&\\leq M + \\dfrac{\\alpha^2 G^2 T}{2}\n\\end{align*}\n\nЗдесь мы подразумеваем \\|g_k\\|_* \\leq G равномерно по k, а V_{x_0}(y) \\leq M. В итоге:\n\n\\begin{align*}\nf(\\overline{x}) - f^* &= f \\left( \\frac{1}{T}\\sum\\limits_{k=0}^{T-1} x_k \\right) - f^* \\leq \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1} f(x_k) - f^* \\right) \\\\\n& \\leq  \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^* \\rangle\\right) \\\\\n& \\leq \\dfrac{M}{\\alpha T} + \\dfrac{\\alpha G^2}{2} \\leq \\sqrt{\\dfrac{2 M G^2}{T}}\n\\end{align*}\n\nВыбирая шаг \\alpha_k = \\alpha = \\sqrt{\\dfrac{2M}{G^2 T}}\n\n1.1 Алгоритм зеркального спуска (mirror descent):\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right)\n\nИнтересные фишки:\n\nТакая же скорость сходимости, как и для метода проекции субградиента.\nРаботает в существенно более широком классе практических задач\n\n\n\n1.2 Онлайн версия\nСовершенно ясно, что в наших оценках на каждом шаге может быть новая функция f_k(x) на заданном классе. Поэтому, аналогичные оценки получаются и для онлайн постановки:\n\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq \\sqrt{2 M G^2 T}\n\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq \\sqrt{\\dfrac{2 M G^2}{T}}\n\n\n\n1.3 Еще одна интерпретация\nДавайте покажем, что полученный алгоритм имеет еще одну очень интуитивную интерпретацию:\n\ny_{k} = \\nabla \\phi(x_k) Отображение в сопряженное пространство с помощью функции \\nabla \\phi(x)\ny_{k+1} = y_k - \\alpha_k \\nabla f_k(x_k) Градиентный шаг в сопряженном пространстве\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S}V_{\\nabla \\phi^*(y_{k+1})}(x) Обратное отображение с помощью функции \\nabla \\phi^*(x) и проекция на бюджетное множество\n\n\n\n\nIllustration\n\n\nДля доказательства эквивалентности таких записей, следует сначала доказать факт того, что:\n\n\\left( \\nabla \\phi(x) \\right)^{-1} =  \\nabla \\phi^*(y)\n\nДля этого пусть y = \\nabla \\phi(x). Заметим, что для сопряженной функции справедливо неравенство Фенхеля - Юнга: \\phi^*(y) + \\phi(x) \\geq xy, в случае, если \\phi(x) - дифференцируема, такое преобразование называется преобразованием Лежандра и выполняется равенство: \\phi^*(y) + \\phi(x) = xy. Дифференцируя равенство по y, получаем \\nabla \\phi^*(y) = x. Таким образом,\n\n\\nabla\\phi^*(y) = \\nabla\\phi^*(\\nabla \\phi(x)) = x, \\qquad \\nabla\\phi(x) = \\nabla\\phi(\\nabla \\phi^*(y)) = y\n\nДоказательство:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{\\nabla \\phi^*(y_{k+1})}(x) \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x) - \\phi(\\nabla \\phi^*(y_{k+1})) - \\left\\langle \\nabla \\phi (\\nabla \\phi^*(y_{k+1})),x - \\nabla \\phi^*(y_{k+1})\\right\\rangle\\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x)  - \\left\\langle y_{k+1},x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x)  - \\left\\langle \\nabla \\phi(x_k) - \\alpha_k g_k,x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x) - \\phi(x_k) - \\left\\langle \\nabla \\phi(x_k),x \\right\\rangle + \\left\\langle \\alpha_k g_k,x \\right\\rangle  \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{x_k}(x) + \\left\\langle \\alpha_k g_k,x \\right\\rangle \\right\\}\n\\end{align*}\n\nВ последней строчке мы пришли к той формулировке, которую писали раньше. Заметим так же, еще одну интересную концепцию:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left(f(x_k)+  \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right)\n\\end{align*}\n\nЗдесь левая часть минимизируемого выражения представляет собой аппроксимацию первого порядка, а правая часть представляет собой проекционный член.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#нафига",
    "href": "docs/methods/fom/Mirror_descent.html#нафига",
    "title": "Mirror descent",
    "section": "2 НАФИГА?",
    "text": "2 НАФИГА?\nРезонный вопрос, ведь в случае, если мы выбрали \\|\\cdot\\| = \\|\\cdot\\|_2 Евклидову норму и Евклидово расстояние, то этот метод в точности совпадает с тем, что мы уже называем метод проекции субградиента.\nЗначит, надо предоставить сценарий, когда МЗС работает лучше, давайте рассмотрим S = \\Delta_n - вероятностный симплекс, а так же следующее расстояние Брэгмана V_x(y) = \\sum_{i \\in [n]} y_i \\log \\frac{y_i}{x_i} = D(y \\| x). Норма в прямом пространстве при этом \\|\\|_1, а в сопряженном - \\|\\|_\\infty. Кроме того, для заданной дивергенции Брэгмана справедливо:\n\nx_0 = \\left( 1/n, \\ldots, 1/n \\right) \\; \\to \\; V_{x_0}(x) \\leq \\log n \\;\\;\\forall x \\in \\Delta_n\n\nТогда алгоритм зеркального спуска запишется в виде:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\sum_{i \\in [n]} x_i \\log \\frac{x_i}{x_{k_i}} \\right) \\\\\n&= x_k \\cdot \\dfrac{e^{-\\alpha_k g_k}}{\\|x_k \\cdot e^{-\\alpha_k g_k}\\|_1}\n\\end{align*}\n\nА оценки с учетом того, что M = \\log n, \\|g_k\\|_\\infty \\leq G запишутся, как:\n\n\\begin{align*}\nf(\\overline{x}) - f^*  \\leq \\sqrt{\\dfrac{2 \\log (n) G^2}{T}}\n\\end{align*}\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n\nTs = np.logspace(0,4, 10)\n\nm = 10\nn = 1000\nA = np.random.randn(m, n)\nx_true = np.random.randn(n)\nx_true[x_true &lt; 0] = 0\nx_true = x_true/(np.linalg.norm(x_true, 1))\n\nb = A@x_true\n\nx0 = np.ones(n)/n\n\n\ndef f(x):\n    return np.linalg.norm(A@x - b, 1)\n\ndef grad(x):\n    return np.sum(A.T * np.sign(A@x - b), axis=1)\n\ndef mirror_descent(x0, grad, T):\n    n = len(x0)\n    M = np.log(n)\n##     G = np.linalg.norm(A,np.inf)*1+np.linalg.norm(b,np.inf)\n##     alpha = np.sqrt(2*M/(G**2*T))\n    alpha = 0.0001\n    xk = x0\n    sequence = []\n##     print('MD %.3f'%alpha)\n    for i in range(int(T)):\n        sequence.append(xk)\n        g = grad(xk)\n        xk = xk * np.exp(-alpha * g) / np.sum(xk * np.exp(-alpha * g))\n    return sequence\n\ndef projection_subgradient_descent(x0, grad, T):\n    n = len(x0)\n    M = 0.5\n##     G = np.linalg.norm(A,2)*1+np.linalg.norm(b,2)\n##     alpha = np.sqrt(2*M/(G**2*T))\n    alpha = 0.0001\n##     print('GD %.3f'%alpha)\n    xk = x0\n    sequence = []\n    for i in range(int(T)):\n        sequence.append(xk)\n        g = grad(xk) \n        xk = xk - alpha*g\n        xk[xk&lt;0] = 0\n        xk = xk/(np.linalg.norm(xk, 1))\n    return sequence\n\nresult_md = []\nresult_gd = []\n\nfor T in Ts:\n    print(T)\n    md_T = mirror_descent(x0, grad, T)\n    gd_T = projection_subgradient_descent(x0, grad, T)\n    \n    x_md = np.mean(md_T, axis = 0)\n    x_gd = np.mean(gd_T, axis = 0)\n    \n    result_md.append(f(x_md) - f(x_true))\n    result_gd.append(f(x_gd) - f(x_true))\n\n    \nplt.loglog(Ts, result_gd, label = 'GD')\nplt.loglog(Ts, result_md, label = 'MD')\nplt.xlabel('T')\nplt.ylabel(r'$f(\\overline{x}) - f(x^*)$')\nplt.legend()\n\n\n\npng",
    "crumbs": [
      "Methods",
      "First order methods",
      "Mirror descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/SAG.html",
    "href": "docs/methods/fom/SAG.html",
    "title": "Stochastic average gradient",
    "section": "",
    "text": "0.1 Summary\nA classical problem of minimizing finite sum of the smooth and convex functions was considered.\n\n\\min\\limits_{x \\in \\mathbb{R}^{p}} g(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost. Baseline solution to the problem is to calculate the loss function and the corresponding gradient vector only on the small subset of indicies from i = 1, \\ldots, n, which usually refers as Stochastic gradient descent. The authors claim, that the convergence rate of proposed algorithm is the same a for the full Gradient Descent method (\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) for convex functions and \\mathcal{O}\\left(\\dfrac{1}{k}\\right) for strongly convex objectives), but the iteration costs remain the same as for the stochastic version.\nThe method itself takes the following form:\n\n\\tag{SAG}\nx_{k+1}=x_{k}-\\frac{\\alpha_{k}}{p} \\sum_{i=1}^{p} y^{i}_{k}\n\nwhere at each iteration only a random summand of a gradient is updated:\n\n\\tag{SAG}\ny^{i}_{k}=\\left\\{\\begin{array}{ll}{f_{i}^{\\prime}\\left(x_{k}\\right)}, & {\\text { if } i=i_{k}} \\\\ {y^{i}_{k-1}}, & {\\text { otherwise }}\\end{array}\\right.\n\n\nThere is a dependency on dimensionality factor n in bounds. However, it can be improved using restart technique.\nEmpirical results were only shown on logistic regression with Tikhonov regularization problems on different datasets.\nBatch and non-uniform versions are also presented in the paper.\nThe first known paper, that contains proof of linear convergence for the convex case.\n\n\n\n0.2 Bounds\nFor a constant step size \\alpha = \\dfrac{1}{16 L}, where L stands for the Lipschitz constant of a gradient of each function f_i(x) (in practice, it means that L = \\max\\limits_{i=1, \\ldots, n} L_i).\n\n\\mathbb{E}\\left[g\\left(\\overline{x}_{k}\\right)\\right]-g\\left(x^{*}\\right) \\leqslant \\frac{32 n}{k} C_{0},\n\nwhere C_0=g\\left(x_0\\right)-g\\left(x^*\\right)+\\frac{4L}{n} \\| x_0 - x^\\ast\\|^2 +\\frac{\\sigma^2}{16L} in convex case and\n\n\\mathbb{E}\\left[g\\left(x_{k}\\right)\\right]-g\\left(x^*\\right) \\leqslant\\left(1-\\min \\left\\{\\frac{\\mu}{16 L}, \\frac{1}{8 n}\\right\\}\\right)^{k} C_{0}\n\nin \\mu - strongly convex case.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Stochastic average gradient"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html",
    "href": "docs/methods/fom/Subgradient descent.html",
    "title": "Subgradient descent",
    "section": "",
    "text": "Рассматривается классическая задача выпуклой оптимизации:\n\n\\min_{x \\in S} f(x),\n\nПодразумевается, что f(x) - выпуклая функция на выпуклом множестве S. Для начала будем рассматривать задачу безусловной минимизации (БМ), S = \\mathbb{R}^n\nВектор g называется субградиентом функции f(x): S \\to \\mathbb{R} в точке x_0, если \\forall x \\in S:\n\nf(x)  \\geq f(x_0) +  \\langle g, x - x_0 \\rangle\n\nГрадиентный спуск предполагает, что функция f(x) является дифференцируемой в каждой точке задачи. Теперь же, мы будем предполагать лишь выпуклость.\nИтак, мы имеем оракул первого порядка:\nВход: x \\in \\mathbb R^n\nВыход: \\partial f(x) и f(x)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#introduction",
    "href": "docs/methods/fom/Subgradient descent.html#introduction",
    "title": "Subgradient descent",
    "section": "",
    "text": "Рассматривается классическая задача выпуклой оптимизации:\n\n\\min_{x \\in S} f(x),\n\nПодразумевается, что f(x) - выпуклая функция на выпуклом множестве S. Для начала будем рассматривать задачу безусловной минимизации (БМ), S = \\mathbb{R}^n\nВектор g называется субградиентом функции f(x): S \\to \\mathbb{R} в точке x_0, если \\forall x \\in S:\n\nf(x)  \\geq f(x_0) +  \\langle g, x - x_0 \\rangle\n\nГрадиентный спуск предполагает, что функция f(x) является дифференцируемой в каждой точке задачи. Теперь же, мы будем предполагать лишь выпуклость.\nИтак, мы имеем оракул первого порядка:\nВход: x \\in \\mathbb R^n\nВыход: \\partial f(x) и f(x)",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#algorithm",
    "href": "docs/methods/fom/Subgradient descent.html#algorithm",
    "title": "Subgradient descent",
    "section": "2 Algorithm",
    "text": "2 Algorithm\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nгде g_k - произвольный субградиент функции f(x) в т. x_k, g_k \\in \\partial f (x_k)\n\n2.1 Bounds\n\n2.1.1 Vanilla version\nЗапишем как близко мы подошли к оптимуму x^* = \\text{arg}\\min\\limits_{x \\in \\mathbb{R}^n} f(x) = \\text{arg} f^* на последней итерации:\n\n\\begin{align*}\n\\| x_{k+1} - x^* \\|^2 & = \\|x_k - x^* - \\alpha_k g_k\\|^2 = \\\\\n                      & = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\\end{align*}\n\nДля субградиента: \\langle g_k, x_k - x^* \\rangle \\leq f(x_k) - f(x^*) = f(x_k) - f^*. Из написанного выше:\n\n\\begin{align*}\n2\\alpha_k \\langle g_k, x_k - x^* \\rangle =  \\| x_k - x^* \\|^2 + \\alpha_k^2 g_k^2 - \\| x_{k+1} - x^* \\|^2\n\\end{align*}\n\nПросуммируем полученное неравенство для k = 0, \\ldots, T-1\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k \\langle g_k, x_k - x^* \\rangle &=  \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq \\| x_0 - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2\n\\end{align*}\n\nЗдесь мы предположили R^2 = \\|x_0 - x^*\\|^2, \\qquad \\|g_k\\| \\leq G. Предполагая \\alpha_k = \\alpha (постоянный шаг), имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq \\dfrac{R^2}{2 \\alpha} + \\dfrac{\\alpha}{2}G^2 T\n\\end{align*}\n\nМинимизация правой части по \\alpha дает \\alpha^* = \\dfrac{R}{G}\\sqrt{\\dfrac{1}{T}}\n\n\\begin{align*}\n\\tag{Subgradient Bound}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T}\n\\end{align*}\n\nТогда (используя неравенство Йенсена и свойство субградиента f(x^*) \\geq f(x_k) + \\langle g_k, x^* - x_k \\rangle) запишем оценку на т.н. Regret, а именно:\n\n\\begin{align*}\nf(\\overline{x}) - f^* &= f \\left( \\frac{1}{T}\\sum\\limits_{k=0}^{T-1} x_k \\right) - f^* \\leq \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1} (f(x_k) - f^* )\\right) \\\\\n& \\leq  \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^* \\rangle\\right) \\\\\n& \\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nВажные моменты:\n\nПолучение оценок не для x_T, а для среднего арифметического по итерациям \\overline{x} - типичный трюк при получении оценок для методов, где есть выпуклость, но нет удобного убывания на каждой итерации. Нет гарантий успеха на каждой итерации, но есть гарантия успеха в среднем\nДля выбора оптимального шага необходимо знать (предположить) число итераций заранее. Возможный выход: инициализировать T небольшим значением, после достижения этого количества итераций удваивать T и рестартовать алгоритм. Более интеллектуальный способ: адаптивный выбор длины шага.\n\n\n\n2.1.2 Steepest subgradient descent\nПопробуем выбирать на каждой итерации длину шага более оптимально. Тогда:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\nМинимизируя выпуклую правую часть по \\alpha_k, получаем:\n\n\\alpha_k = \\dfrac{\\langle g_k, x_k - x^*\\rangle}{\\| g_k\\|^2}\n\nОценки изменятся следующим образом:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 - \\dfrac{\\langle g_k, x_k - x^*\\rangle^2}{\\| g_k\\|^2}\n\n\n\\langle g_k, x_k - x^*\\rangle^2 = \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) \\| g_k\\|^2\n\n\n\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\sum\\limits_{k=0}^{T-1}\\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 \\right) G^2\n\n\n\\dfrac{1}{T}\\left(\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle \\right)^2 \\leq \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq R^2  G^2\n\nЗначит,\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle  \\leq GR \\sqrt{T}\n\nЧто приводит к абсолютно такой же оценке \\mathcal{O}\\left(\\dfrac{1}{\\sqrt{T}}\\right) на невязку по значению функции. На самом деле, для такого класса функций нельзя получить результат лучше, чем \\dfrac{1}{\\sqrt{T}} или \\dfrac{1}{\\varepsilon^2} по итерациям\n\n\n2.1.3 Online learning\nРассматривается следующая игра: есть игрок и природа. На каждом из k = 0, \\ldots, T-1 шагов:\n\nИгрок выбирает действие x_k\nПрирода (возможно, враждебно) выбирает выпуклую функцию f_k, сообщает игроку значение f(x_k), g_k \\in \\partial f(x_k)\nИгрок вычисляет следующее действие, чтобы минимизировать регрет:\n\n\n\\tag{Regret}\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x)\n\nВ такой постановке цель игрока состоит в том, чтобы выбрать стратегию, которая минимизирует разницу его действия с наилучшим выбором на каждом шаге.\nНесмотря на весьма сложную (на первый взгляд) постановку задачи, существует стратегия, при которой регрет растет как \\sqrt{T}, что означает, что усредненный регрет \\dfrac{1}{T} R_{T-1} падает, как \\dfrac{1}{\\sqrt{T}}\nЕсли мы возьмем оценку (Subgradient Bound) для субградиентного метода, полученную выше, мы имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq G \\|x_0 - x^*\\| \\sqrt{T}\n\\end{align*}\n\nОднако, в её выводе мы нигде не использовали тот факт, что x^* = \\text{arg}\\min\\limits_{x \\in S} f(x). Более того, мы вообще не использовали никакой специфичности точки x^*. Тогда можно записать это для произвольной точки y:\n\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq G \\|x_0 - y\\| \\sqrt{T}\n\nЗапишем тогда оценки для регрета, взяв y = \\text{arg}\\min\\limits_{x \\in S}\\sum\\limits_{k = 0}^{T-1} f_k(x):\n\n\\begin{align*}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\sum\\limits_{k = 0}^{T-1} f_k(y) = \\\\\n&= \\sum\\limits_{k = 0}^{T-1} \\left( f_k(x_k) - f_k(y)\\right) \\leq \\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq \\\\\n&\\leq G \\|x_0 - y\\| \\sqrt{T}\n\\end{align*}\n\nИтого мы имеем для нашей стратегии с постоянным шагом:\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq G \\| x_0 - x^* \\| \\dfrac{1}{\\sqrt{T}}, \\qquad \\alpha_k = \\alpha = \\dfrac{\\|x_0 - x^*\\|}{G}\\sqrt{\\dfrac{1}{T}}",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#examples",
    "href": "docs/methods/fom/Subgradient descent.html#examples",
    "title": "Subgradient descent",
    "section": "3 Examples",
    "text": "3 Examples\n\n3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R}^n} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\nAlgorithm will be written as:\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\nwhere signum function is taken element-wise.\n\n\n\nIllustration\n\n\n\n\n3.2 Support vector machines\nLet D = \\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^n, y_i \\in \\{\\pm 1\\}\\}\nWe need to find \\omega \\in \\mathbb{R}^n and b \\in \\mathbb{R} such that\n\n\\min_{\\omega \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\dfrac{1}{2}\\|\\omega\\|_2^2 + C\\sum\\limits_{i=1}^m max[0, 1 - y_i(\\omega^\\top x_i + b)]",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#bounds-1",
    "href": "docs/methods/fom/Subgradient descent.html#bounds-1",
    "title": "Subgradient descent",
    "section": "4 Bounds",
    "text": "4 Bounds\n\n\n\n\n\n\n\n\n\nConditions\nf(\\bar{x}) - f(x^*)\\leq\nType of convergence\n\\Vert x_k - x^* \\Vert \\leq\n\n\n\n\nConvexLipschitz-continuous function(G)\n\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) \\; \\dfrac{GR}{\\sqrt{k}}\nSublinear\n\n\n\nConvexLipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{LR^2}{k}\nSublinear\n\n\n\n\\mu-Strongly convexLipschitz-continuous gradient(L)\n\nLinear\n(1 - \\eta \\mu)^k R^2\n\n\n\\mu-Strongly convexLipschitz-continuous hessian(M)\n\nLocally linear R &lt; \\overline{R}\n\\dfrac{\\overline{R}R}{\\overline{R} - R} \\left( 1 - \\dfrac{2\\mu}{L+3\\mu}\\right)\n\n\n\n\nR = \\| x_0 - x^*\\| - initial distance\n\\overline{R} = \\dfrac{2\\mu}{M}\n\\overline{x} = \\dfrac{1}{k}\\sum_{i=1}^k x_i\n\\|g_k\\| \\leq G",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#code",
    "href": "docs/methods/fom/Subgradient descent.html#code",
    "title": "Subgradient descent",
    "section": "5 Code",
    "text": "5 Code\n\nOpen In Colab - Wolfe’s example and why we usually have oscillations in non-smooth optimization.\nOpen In Colab - Linear least squares with l_1- regularization.",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#references",
    "href": "docs/methods/fom/Subgradient descent.html#references",
    "title": "Subgradient descent",
    "section": "6 References",
    "text": "6 References\n\nGreat cheatsheet by Sebastian Pokutta\nLecture on subgradient methods @ Berkley\nIllustration of l1 regularization",
    "crumbs": [
      "Methods",
      "First order methods",
      "Subgradient descent"
    ]
  },
  {
    "objectID": "docs/methods/index.html",
    "href": "docs/methods/index.html",
    "title": "Methods",
    "section": "",
    "text": "\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. }  g_i(x) \\leq& 0, \\; i = 1,\\ldots,m\\\\\nh_j(x) =& 0, \\; j = 1,\\ldots,k\\\\\n\\end{split}\n\nSome necessary or/and sufficient conditions are known (See Optimality conditions. KKT and Convex optimization problem.\n\nIn fact, there might be very challenging to recognize the convenient form of optimization problem.\nAnalytical solution of KKT could be inviable.\n\n\n\nTypically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\n\n\nIllustration of iterative method approaches to the solution x^*\n\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x\n\n\n\n\n\n\nDepending on the maximum order of derivative available from the oracle we call the oracles as zero order, first order, second order oravle and etc.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#general-formulation",
    "href": "docs/methods/index.html#general-formulation",
    "title": "Methods",
    "section": "",
    "text": "\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. }  g_i(x) \\leq& 0, \\; i = 1,\\ldots,m\\\\\nh_j(x) =& 0, \\; j = 1,\\ldots,k\\\\\n\\end{split}\n\nSome necessary or/and sufficient conditions are known (See Optimality conditions. KKT and Convex optimization problem.\n\nIn fact, there might be very challenging to recognize the convenient form of optimization problem.\nAnalytical solution of KKT could be inviable.\n\n\n\nTypically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\n\n\nIllustration of iterative method approaches to the solution x^*\n\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x\n\n\n\n\n\n\nDepending on the maximum order of derivative available from the oracle we call the oracles as zero order, first order, second order oravle and etc.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#unsolvability-of-numerical-optimization-problem",
    "href": "docs/methods/index.html#unsolvability-of-numerical-optimization-problem",
    "title": "Methods",
    "section": "2 Unsolvability of numerical optimization problem",
    "text": "2 Unsolvability of numerical optimization problem\nIn general, optimization problems are unsolvable. ¯\\(ツ)/¯\nConsider the following simple optimization problem of a function over unit cube:\n\n\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. } &  x \\in \\mathbb{C}^n\n\\end{split}\n\nWe assume, that the objective function f (\\cdot) : \\mathbb{R}^n \\to \\mathbb{R} is Lipschitz continuous on \\mathbb{B}^n:\n\n| f (x) − f (y) | \\leq L \\| x − y \\|_{\\infty} \\forall x,y \\in \\mathbb{C}^n,\n\nwith some constant L (Lipschitz constant). Here \\mathbb{C}^n - the n-dimensional unit cube\n\n\\mathbb{C}^n = \\{x \\in \\mathbb{R}^n \\mid 0 \\leq x_i \\leq 1, i = 1, \\ldots, n\\}\n\nOur goal is to find such \\tilde{x}: \\vert f(\\tilde{x}) - f^*\\vert \\leq \\varepsilon for some positive \\varepsilon. Here f^* is the global minima of the problem. Uniform grid with p points on each dimension guarantees at least this quality:\n\n\\| \\tilde{x} − x_* \\|_{\\infty} \\leq \\frac{1}{2p},\n\nwhich means, that\n\n|f (\\tilde{x}) − f (x_*)| \\leq \\frac{L}{2p}\n\nOur goal is to find the p for some \\varepsilon. So, we need to sample \\left(\\frac{L}{2 \\varepsilon}\\right)^n points, since we need to measure function in p^n points. Doesn’t look scary, but if we’ll take L = 2, n = 11, \\varepsilon = 0.01, computations on the modern personal computers will take 31,250,000 years.\n\n2.1 Stopping rules\n\nArgument closeness:\n\n  \\| x_k - x_*  \\|_2 &lt; \\varepsilon\n  \nFunction value closeness:\n\n  \\| f_k - f^* \\|_2 &lt; \\varepsilon\n  \nCloseness to a critical point\n\n  \\| f'(x_k) \\|_2 &lt; \\varepsilon\n  \n\nBut x_* and f^* = f(x_*) are unknown!\nSometimes, we can use the trick:\n\n\\|x_{k+1} - x_k \\| = \\|x_{k+1} - x_k + x_* - x_* \\| \\leq \\|x_{k+1} - x_* \\| + \\| x_k - x_* \\| \\leq 2\\varepsilon\n\nNote: it’s better to use relative changing of these values, i.e. \\dfrac{\\|x_{k+1} - x_k \\|_2}{\\| x_k \\|_2}.\n\n\n\n\n\n\nExample\n\n\n\n\n\nSuppose, you are trying to estimate the vector x_{true} with some approximation x_{approx}. One can choose between two relative errors:\n\n\\dfrac{\\|x_{approx} - x_{true}\\|}{\\|x_{approx}\\|} \\quad \\dfrac{\\|x_{approx} - x_{true}\\|}{\\|x_{true}\\|}\n\nIf both x_{approx} and x_{true} are close to each other, then the difference between them is small, while if your approximation is far from the truth (say, x_{approx} = 10x_{true} or x_{approx} = 0.01 x_{true} they differ drastically).\n\n\n\n\n\n\n2.2 Local nature of the methods\n\n\n\nIllustration of the idea of locality in black-box optimization",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/index.html#contents-of-the-chapter",
    "href": "docs/methods/index.html#contents-of-the-chapter",
    "title": "Methods",
    "section": "3 Contents of the chapter",
    "text": "3 Contents of the chapter\n\n\n\n\n\n\n\n\nGradient descent\n\n\n\n\n\n\n\n\n\n\nNewton method\n\n\n\n\n\n\n\n\n\n\nQuasi Newton methods\n\n\n\n\n\n\n\n\n\n\nSubgradient descent\n\n\n\n\n\n\n\n\n\n\nProjected subgradient descent\n\n\n\n\n\n\n\n\n\n\nLinear Programming and simplex algorithm\n\n\n\n\n\n\n\n\n\n\nMirror descent\n\n\n\n\n\n\n\n\n\n\nAutomatic differentiation\n\n\n\n\n\n\n\n\n\n\nStochastic gradient descent\n\n\n\n\n\n\n\n\n\n\nStochastic average gradient\n\n\n\n\n\n\n\n\n\n\nADAM: A Method for Stochastic Optimization\n\n\n\n\n\n\n\n\n\n\nLookahead Optimizer: k steps forward, 1 step back\n\n\n\n\n\n\n\n\n\n\nBee algorithm\n\n\n\n\n\n\n\n\n\n\nBinary search\n\n\n\n\n\n\n\n\n\n\nConjugate gradients\n\n\n\n\n\n\n\n\n\n\nGolden search\n\n\n\n\n\n\n\n\n\n\nInexact Line Search\n\n\n\n\n\n\n\n\n\n\nNatural gradient descent\n\n\n\n\n\n\n\n\n\n\nNelder–Mead\n\n\n\n\n\n\n\n\n\n\nSimulated annealing\n\n\n\n\n\n\n\n\n\n\nSuccessive parabolic interpolation\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html",
    "href": "docs/methods/line_search/golden_search.html",
    "title": "Golden search",
    "section": "",
    "text": "The idea is quite similar to the dichotomy method. There are two golden points on the line segment (left and right) and the insightful idea is, that on the next iteration one of the points will remain the golden point.\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Line search",
      "Golden search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html#idea",
    "href": "docs/methods/line_search/golden_search.html#idea",
    "title": "Golden search",
    "section": "",
    "text": "The idea is quite similar to the dichotomy method. There are two golden points on the line segment (left and right) and the insightful idea is, that on the next iteration one of the points will remain the golden point.\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Line search",
      "Golden search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html#algorithm",
    "href": "docs/methods/line_search/golden_search.html#algorithm",
    "title": "Golden search",
    "section": "2 Algorithm",
    "text": "2 Algorithm\ndef golden_search(f, a, b, epsilon):\n    tau = (sqrt(5) + 1) / 2\n    y = a + (b - a) / tau**2\n    z = a + (b - a) / tau\n    while b - a &gt; epsilon:\n        if f(y) &lt;= f(z):\n            b = z\n            z = y\n            y = a + (b - a) / tau**2\n        else:\n            a = y\n            y = z\n            z = a + (b - a) / tau\n    return (a + b) / 2",
    "crumbs": [
      "Methods",
      "Line search",
      "Golden search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html#bounds",
    "href": "docs/methods/line_search/golden_search.html#bounds",
    "title": "Golden search",
    "section": "3 Bounds",
    "text": "3 Bounds\n\n|x_{k+1} - x_*| \\leq b_{k+1} - a_{k+1} = \\left( \\frac{1}{\\tau} \\right)^{N-1} (b - a) \\approx 0.618^k(b-a),\n\nwhere \\tau = \\frac{\\sqrt{5} + 1}{2}.\n\nThe geometric progression constant more than the dichotomy method - 0.618 worse than 0.5\nThe number of function calls is less than for the dichotomy method - 0.707 worse than 0.618 - (for each iteration of the dichotomy method, except for the first one, the function is calculated no more than 2 times, and for the gold method - no more than one)",
    "crumbs": [
      "Methods",
      "Line search",
      "Golden search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html",
    "href": "docs/methods/line_search/inexact.html",
    "title": "Inexact Line Search",
    "section": "",
    "text": "The strategy of inexact line search is practical and has a significant geometric interpretation:",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html#sufficient-decrease",
    "href": "docs/methods/line_search/inexact.html#sufficient-decrease",
    "title": "Inexact Line Search",
    "section": "1 Sufficient Decrease",
    "text": "1 Sufficient Decrease\nConsider a scalar function \\phi(\\alpha) at a point x_k:\n\n\\phi(\\alpha) = f(x_k - \\alpha\\nabla f(x_k)), \\alpha \\geq 0\n\nThe first-order approximation of \\phi(\\alpha) near \\alpha = 0 is:\n\n\\phi(\\alpha) \\approx f(x_k) - \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\n\n\n\nIllustration of Taylor approximation of \\phi^I_0(\\alpha)\n\n\nThe inexact line search condition, known as the Armijo condition, states that \\alpha should provide sufficient decrease in the function f, satisfying:\n\nf(x_k - \\alpha \\nabla f (x_k)) \\leq f(x_k) - c_1 \\cdot \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\n\n\n\nIllustration of sufficient decrease condition with coefficient c_1\n\n\nfor some constant c_1 \\in (0,1). Note that setting c_1 = 1 corresponds to the first-order Taylor approximation of \\phi(\\alpha). However, this condition can accept very small values of \\alpha, potentially slowing down the solution process. Typically, c_1 \\approx 10^{−4} is used in practice.\n\n\n\n\n\n\nExample\n\n\n\n\n\nIf f(x) represents a cost function in an optimization problem, choosing an appropriate c_1 value is crucial. For instance, in a machine learning model training scenario, an improper c_1 might lead to either very slow convergence or missing the minimum.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of c_1 affect the convergence speed in optimization problems?",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html#goldstein-conditions",
    "href": "docs/methods/line_search/inexact.html#goldstein-conditions",
    "title": "Inexact Line Search",
    "section": "2 Goldstein Conditions",
    "text": "2 Goldstein Conditions\nConsider two linear scalar functions \\phi_1(\\alpha) and \\phi_2(\\alpha):\n\n\\phi_1(\\alpha) = f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|^2\n\n\n\\phi_2(\\alpha) = f(x_k) - c_2 \\alpha \\|\\nabla f(x_k)\\|^2\n\nThe Goldstein-Armijo conditions locate the function \\phi(\\alpha) between \\phi_1(\\alpha) and \\phi_2(\\alpha). Typically, c_1 = \\rho and c_2 = 1 - \\rho, with \\rho \\in (0.5, 1).\n\n\n\nIllustration of Goldstein conditions",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html#curvature-condition",
    "href": "docs/methods/line_search/inexact.html#curvature-condition",
    "title": "Inexact Line Search",
    "section": "3 Curvature Condition",
    "text": "3 Curvature Condition\nTo avoid excessively short steps, we introduce a second criterion:\n\n-\\nabla f (x_k - \\alpha \\nabla f(x_k))^\\top \\nabla f(x_k) \\geq c_2 \\nabla f(x_k)^\\top(- \\nabla f(x_k))\n\n\n\n\nIllustration of curvature condition\n\n\nfor some c_2 \\in (c_1,1). Here, c_1 is from the Armijo condition. The left-hand side is the derivative \\nabla_\\alpha \\phi(\\alpha), ensuring that the slope of \\phi(\\alpha) at the target point is at least c_2 times the initial slope \\nabla_\\alpha \\phi(\\alpha)(0). Commonly, c_2 \\approx 0.9 is used for Newton or quasi-Newton methods. Together, the sufficient decrease and curvature conditions form the Wolfe conditions.\n\n\n\nIllustration of Wolfe condition\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nIn gradient descent algorithms, applying the curvature condition can prevent the algorithm from taking steps that are too small, thus enhancing the efficiency of finding the minimum.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhy is it important to have a balance between the sufficient decrease and curvature conditions in optimization algorithms?",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html#backtracking-line-search",
    "href": "docs/methods/line_search/inexact.html#backtracking-line-search",
    "title": "Inexact Line Search",
    "section": "4 Backtracking Line Search",
    "text": "4 Backtracking Line Search\nBacktracking line search is a technique to find a step size that satisfies the Armijo condition, Goldstein conditions, or other criteria of inexact line search. It begins with a relatively large step size and iteratively scales it down until a condition is met.\n\n4.1 Algorithm:\n\nChoose an initial step size, \\alpha_0, and parameters \\beta \\in (0, 1) and c_1 \\in (0, 1).\nCheck if the chosen step size satisfies the chosen condition (e.g., Armijo condition).\nIf the condition is satisfied, stop; else, set \\alpha := \\beta \\alpha and repeat step 2.\n\nThe step size \\alpha is updated as\n\n\\alpha_{k+1} := \\beta \\alpha_k\n\nin each iteration until the chosen condition is satisfied.\n\n\n\n\n\n\nExample\n\n\n\n\n\nIn machine learning model training, the backtracking line search can be used to adjust the learning rate. If the loss doesn’t decrease sufficiently, the learning rate is reduced multiplicatively until the Armijo condition is met.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhy is it crucial to carefully choose the initial step size and the reduction factor \\beta in backtracking line search algorithms?",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/line_search/inexact.html#references",
    "href": "docs/methods/line_search/inexact.html#references",
    "title": "Inexact Line Search",
    "section": "5 References",
    "text": "5 References\n\nNumerical Optimization by J.Nocedal and S.J.Wright.\n6 Interactive Wolfe Line Search Example by fmin library.",
    "crumbs": [
      "Methods",
      "Line search",
      "Inexact Line Search"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html",
    "href": "docs/methods/zom/bee_algorithm.html",
    "title": "Bee algorithm",
    "section": "",
    "text": "The Bee Algorithm was mathematically first described relatively recently. It is one of the representatives of a large family of algorithms that allow modeling swarm intelligence. This article will provide an example of the application of the Bee Algorithm to search for the global extremum of the function. The two-dimensional Schwefel function, having a large number of local minima, and the Rosenbrock function, whose global minimum lies in a narrow, parabolic valley, were chosen as the target functions.\n\n\nA colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here.\n\n\n\nAnd now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While\n\n\n\n\nTwo standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube x_i \\in [-500, 500] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube x_i \\in [-5, 10] for all i = 1, ..., d, although it may be restricted to the hypercube x_i \\in [-2.048, 2.048] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#algorithm",
    "href": "docs/methods/zom/bee_algorithm.html#algorithm",
    "title": "Bee algorithm",
    "section": "",
    "text": "The Bee Algorithm was mathematically first described relatively recently. It is one of the representatives of a large family of algorithms that allow modeling swarm intelligence. This article will provide an example of the application of the Bee Algorithm to search for the global extremum of the function. The two-dimensional Schwefel function, having a large number of local minima, and the Rosenbrock function, whose global minimum lies in a narrow, parabolic valley, were chosen as the target functions.\n\n\nA colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here.\n\n\n\nAnd now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While\n\n\n\n\nTwo standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube x_i \\in [-500, 500] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube x_i \\in [-5, 10] for all i = 1, ..., d, although it may be restricted to the hypercube x_i \\in [-2.048, 2.048] for all i = 1, ..., d.\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#references",
    "href": "docs/methods/zom/bee_algorithm.html#references",
    "title": "Bee algorithm",
    "section": "2 References",
    "text": "2 References\n\nMETAHEURISTICS FROM DESIGN TO IMPLEMENTATION\nYang X.S. Nature-inspired Metaheuristic Algorithms\nHoney Bees Inspired Optimization Method: The Bees Algorithm\nThe Bees Algorithm Technical Note\nАлгоритм пчел для оптимизации функции\nAn Idea Based on Honey Bee Swarm for Numerical Optimization, Technical Report - TR06\nGrouped Bees Algorithm: A Grouped Version of the Bees Algorithm\nThe Bees Algorithm – A Novel Tool for Complex Optimisation Problems\nМЕТОД ПЧЕЛИНОГО РОЯ ДЛЯ РЕШЕНИЯ ЗАДАЧ НА ПОИСК ЭКСТРЕМУМА ФУНКЦИИ\nA comparative study of the Bees Algorithm as a tool for function optimisation",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Bee algorithm"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html",
    "href": "docs/methods/zom/nelder-mead.html",
    "title": "Nelder–Mead",
    "section": "",
    "text": "Sometimes the multidimensional function is so difficult to evaluate that even expressing the 1^{\\text{st}} derivative for gradient-based methods of finding optimum becomes an impossible task. In this case, we can only rely on the values of the function at each point. Or, in other words, on the 0 order oracle calls.\nLet’s take, for instance, Mishra’s Bird function:\n\nf(x,y) = \\sin{y} \\cdot e^{\\left( 1 - \\cos{x} \\right)^2} + \\cos{x} \\cdot e^{\\left( 1 - \\sin{y} \\right)^2} + (x - y)^2\n\n\n\n\nIllustration\n\n\nThis function is usually subjected to the domain (x+5)^2 + (y+5)^2 &lt; 25, but for the sake of picture beauty we will mainly use domain [-10; 0] \\times [-10; 0].\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#problem",
    "href": "docs/methods/zom/nelder-mead.html#problem",
    "title": "Nelder–Mead",
    "section": "",
    "text": "Sometimes the multidimensional function is so difficult to evaluate that even expressing the 1^{\\text{st}} derivative for gradient-based methods of finding optimum becomes an impossible task. In this case, we can only rely on the values of the function at each point. Or, in other words, on the 0 order oracle calls.\nLet’s take, for instance, Mishra’s Bird function:\n\nf(x,y) = \\sin{y} \\cdot e^{\\left( 1 - \\cos{x} \\right)^2} + \\cos{x} \\cdot e^{\\left( 1 - \\sin{y} \\right)^2} + (x - y)^2\n\n\n\n\nIllustration\n\n\nThis function is usually subjected to the domain (x+5)^2 + (y+5)^2 &lt; 25, but for the sake of picture beauty we will mainly use domain [-10; 0] \\times [-10; 0].\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#algorithm",
    "href": "docs/methods/zom/nelder-mead.html#algorithm",
    "title": "Nelder–Mead",
    "section": "2 Algorithm",
    "text": "2 Algorithm\n\n2.1 Related definitions:\n\n\\textbf{Simplex} – polytope with the least possible number of vertices in n-dimensional space. (So, it’s (n+1)-polytope.) In our 2D case it will be triangle.\n\\textbf{Best point }x_1 – vertex of the simplex, function value in which is the smallest among all vertices.\n\\textbf{Worst point }x_{n+1} – vertex of the simplex, function value in which is the largest among all vertices.\n\\textbf{Other points }x_2, \\ldots, x_n – vertices of the simplex, ordered in such way that f(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1}). This implies that \\{ x_1, x_2, \\ldots, x_n \\} are best points in relation to x_{n+1} and \\{ x_2, \\ldots, x_n, x_{n+1} \\} are worst points in relation to x_1.\n\\textbf{Centroid }x_o – center of mass in the polytope. In Nelder-Mead the centroid is calculated for the polytope, constituted by best vertices. In our 2D case it will be the center of the triangle side, which contains 2 best points x_o = \\dfrac{x_1 + x_2}{2}.\n\n\n\n2.2 Main idea\nThe algorithm maintains the set of test points in the form of simplex. For each point the function value is calculated and points are ordered accordingly. Depending on those values, the simplex exchanges the worst point of the set for the new one, which is closer to the local minimum. In some sense, the simplex is crawling to the minimal value in the domain.\nThe simplex movements finish when its sides become too small (termination condition by sides) or its area becomes too small (termination condition by area). I prefer the second condition, because it takes into account cases when simplex becomes degenerate (three or more vertices on one axis).\n\n\n2.3 Steps of the algorithm\n1. Ordering\nOrder vertices according to values in them:\n\nf(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1})\n\nCheck the termination condition. Possible exit with solution x_{\\min} = x_1.\n2. Centroid calculation\n\nx_o = \\dfrac{\\sum\\limits_{k=1}^{n}{x_k}}{n}\n\n3. Reflection\nCalculate the reflected point x_r:\n\nx_r = x_o + \\alpha \\left( x_o - x_{n+1} \\right)\n\nwhere \\alpha – reflection coefficient, \\alpha &gt; 0. (If \\alpha \\leqslant 0, reflected point x_r will not overlap the centroid)\nThe next step is figured out according to the value of f(x_r) in dependency to values in points x_1 (best) and x_n (second worst):\n\nf(x_r) &lt; f(x_1): Go to step 4.\nf(x_1) \\leqslant f(x_r) &lt; f(x_n): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1.\nf(x_r) \\geqslant f(x_n): Go to step 5.\n\n4. Expansion\nCalculate the expanded point x_e:\n\nx_e = x_o + \\gamma \\left( x_r - x_o \\right)\n\nwhere \\gamma – expansion coefficient, \\gamma &gt; 1. (If \\gamma &lt; 1, expanded point x_e will be contracted towards centroid, if \\gamma = 1: x_e = x_r)\nThe next step is figured out according to the ratio between f(x_e) and f(x_r):\n\nf(x_e) &lt; f(x_r): new simplex with x_{n+1} \\rightarrow x_e. Go to step 1.\nf(x_e) &gt; f(x_r): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1.\n\n5. Contraction\nCalculate the contracted point x_c:\n\nx_c = x_o + \\beta \\left( x_{n+1} - x_o \\right)\n\nwhere \\beta – contraction coefficient, 0 &lt; \\beta \\leqslant 0.5. (If \\beta &gt; 0.5, contraction is insufficient, if \\beta \\leqslant 0, contracted point x_c overlaps the centroid)\nThe next step is figured out according to the ratio between f(x_c) and f(x_{n+1}):\n\nf(x_c) &lt; f(x_{n+1}): new simplex with x_{n+1} \\rightarrow x_c. Go to step 1.\nf(x_c) \\geqslant f(x_{n+1}): Go to step 6.\n\n6. Shrinkage\nReplace all points of simplex x_i with new ones, except for the best point x_1:\n\nx_i = x_1 + \\sigma \\left( x_i - x_1 \\right)\n\nwhere \\sigma – shrinkage coefficient, 0 &lt; \\sigma &lt; 1. (If \\sigma \\geqslant 1, shrinked point x_i overlaps the best point x_1, if \\sigma \\leqslant 0, shrinked point x_i becomes extended)\nGo to step 1.",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#examples",
    "href": "docs/methods/zom/nelder-mead.html#examples",
    "title": "Nelder–Mead",
    "section": "3 Examples",
    "text": "3 Examples\nThis algorithm, as any method in global optimization, is highly dependable on the initial conditions. For instance, if we use different initial simplex or different set of parameters \\{ \\alpha, \\beta, \\gamma, \\sigma \\} the resulting optimal point will differ.\n\n3.1 Some random initial simplex and default set of parameters\n\n\n\nIllustration\n\n\n\n\n3.2 Different initial simplex and same set of parameters\n\n\n\nIllustration\n\n\n\n\n3.3 Same initial simplex and different set of parameters\n\n\n\nIllustration\n\n\n\n\n3.4 Round domain\n\n\n\nIllustration\n\n\n\n\n3.5 Examples with all sets of simplexes\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\n\n\n\nIllustration\n\n\n\n\n\nIllustration",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#code",
    "href": "docs/methods/zom/nelder-mead.html#code",
    "title": "Nelder–Mead",
    "section": "4 Code",
    "text": "4 Code\nOpen In Colab",
    "crumbs": [
      "Methods",
      "Zero order methods",
      "Nelder–Mead"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html",
    "href": "docs/theory/Conjugate function.html",
    "title": "Conjugate function",
    "section": "",
    "text": "Let f: \\mathbb{R}^n \\to \\mathbb{R}. The function f^*: \\mathbb{R}^n \\to \\mathbb{R} is called convex conjugate (Fenchel’s conjugate, dual, Legendre transform) f(x) and is defined as follows:\n\nf^*(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right).\n\nLet’s notice, that the domain of the function f^* is the set of those y, where the supremum is finite.\n\n\n\n\n\n\nFigure 1: Illustration of conjugate function\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nice intuition behind the conjugate function. On the left, we have a slow parabola (say, f(x) = \\frac{x^2}{10}, which implies a small magnitude of the slope with a large magnitude of x_0. On the right, we have the conjugate function f^*(y) = 2.5 y^2, which has a large slope with the small value of y_0.)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nUsing the geometric intuition above, draw the conjugate function to the function below:\n\n\n\n\n\n\nFigure 3: You can use geometric inution from above to draw f^{*}(y).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Take a look at the constant slope x_0 from the y_1 to y_2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraightforward from the definition: \\inf\\limits_{x \\in \\mathbf{dom} \\; f} f(x) = -f^*(0)\nf^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)\n  \n\n\n\n\nThe scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right)  = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y) is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = ax + b.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x\nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nFind f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}.\nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = e^x.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}.\nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x.\nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i.\nHence, f^*(y) = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nRevenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by\n\nM(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n\nThe function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as \nM(p) = (−S)^*(−p).\n Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed).",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html#conjugate-dual-function",
    "href": "docs/theory/Conjugate function.html#conjugate-dual-function",
    "title": "Conjugate function",
    "section": "",
    "text": "Let f: \\mathbb{R}^n \\to \\mathbb{R}. The function f^*: \\mathbb{R}^n \\to \\mathbb{R} is called convex conjugate (Fenchel’s conjugate, dual, Legendre transform) f(x) and is defined as follows:\n\nf^*(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right).\n\nLet’s notice, that the domain of the function f^* is the set of those y, where the supremum is finite.\n\n\n\n\n\n\nFigure 1: Illustration of conjugate function\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nice intuition behind the conjugate function. On the left, we have a slow parabola (say, f(x) = \\frac{x^2}{10}, which implies a small magnitude of the slope with a large magnitude of x_0. On the right, we have the conjugate function f^*(y) = 2.5 y^2, which has a large slope with the small value of y_0.)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nUsing the geometric intuition above, draw the conjugate function to the function below:\n\n\n\n\n\n\nFigure 3: You can use geometric inution from above to draw f^{*}(y).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Take a look at the constant slope x_0 from the y_1 to y_2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraightforward from the definition: \\inf\\limits_{x \\in \\mathbf{dom} \\; f} f(x) = -f^*(0)\nf^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)\n  \n\n\n\n\nThe scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right)  = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y) is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = ax + b.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x\nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nFind f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}.\nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = e^x.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}.\nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}.\nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x.\nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y).\nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i.\nHence, f^*(y) = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nRevenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by\n\nM(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n\nThe function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as \nM(p) = (−S)^*(−p).\n Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed).",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/theory/Conjugate function.html#references",
    "href": "docs/theory/Conjugate function.html#references",
    "title": "Conjugate function",
    "section": "2 References",
    "text": "2 References\n\nGreat intuition behind the Legendre-Fenchel transform.",
    "crumbs": [
      "Theory",
      "Conjugate function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html",
    "href": "docs/theory/Convex_function.html",
    "title": "Convex function",
    "section": "",
    "text": "The function f(x), which is defined on the convex set S \\subseteq \\mathbb{R}^n, is called convex on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1.\nIf the above inequality holds as strict inequality x_1 \\neq x_2 and 0 &lt; \\lambda &lt; 1, then the function is called strictly convex on S.\n\n\n\n\n\n\nFigure 1: Difference between convex and non-convex function\n\n\n\n\n\n\n\n\n\nJensen’s inequality\n\n\n\n\n\nLet f(x) be a convex function on a convex set X \\subseteq \\mathbb{R}^n and let x_i \\in X, 1 \\leq i \\leq m, be arbitrary points from X. Then\n\nf\\left( \\sum_{i=1}^{m} \\lambda_i x_i \\right) \\leq \\sum_{i=1}^{m} \\lambda_i f(x_i)\n\nfor any \\lambda = [\\lambda_1, \\ldots, \\lambda_m] \\in \\Delta_m - probability simplex.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nFirst, note that the point \\sum_{i=1}^{m} \\lambda_i x_i as a convex combination of points from the convex set X belongs to X.\nWe will prove this by induction. For m = 1, the statement is obviously true, and for m = 2, it follows from the definition of a convex function.\nAssume it is true for all m up to m = k, and we will prove it for m = k + 1. Let \\lambda \\in \\Delta{k+1} and\n\nx = \\sum_{i=1}^{k+1} \\lambda_i x_i = \\sum_{i=1}^{k} \\lambda_i x_i + \\lambda_{k+1} x_{k+1}.\n\nAssuming 0 &lt; \\lambda_{k+1} &lt; 1, as otherwise, it reduces to previously considered cases, we have\n\nx = \\lambda_{k+1} x_{k+1} + (1 - \\lambda_{k+1}) \\bar{x},\n\nwhere \\bar{x} = \\sum_{i=1}^{k} \\gamma_i x_i and \\gamma_i = \\frac{\\lambda_i}{1-\\lambda_{k+1}} \\geq 0, 1 \\leq i \\leq k.\nSince \\lambda \\in \\Delta_{k+1}, then \\gamma = [\\gamma_1, \\ldots, \\gamma_k] \\in \\Delta_k. Therefore \\bar{x} \\in X and by the convexity of f(x) and the induction hypothesis:\n\n\\begin{split}\nf\\left( \\sum_{i=1}^{k+1} \\lambda_i x_i \\right) = f\\left( \\lambda_{k+1} x_{k+1} + (1 - \\lambda_{k+1})\\bar{x} \\right) &\\leq \\\\\n\\lambda_{k+1}f(x_{k+1}) + (1 - \\lambda_{k+1})f(\\bar{x}) \\leq \\sum_{i=1}^{k+1} \\lambda_i f(x_i)&\n\\end{split}\n\nThus, initial inequality is satisfied for m = k + 1 as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nf(x) = x^p, \\;  p &gt; 1,\\;  x \\in \\mathbb{R}_+\nf(x) = \\|x\\|^p,\\;  p &gt; 1, x \\in \\mathbb{R}^n\nf(x) = e^{cx},\\;  c \\in \\mathbb{R}, x \\in \\mathbb{R}\nf(x) = -\\ln x,\\;  x \\in \\mathbb{R}_{++}\nf(x) = x\\ln x,\\;  x \\in \\mathbb{R}_{++}\nThe sum of the largest k coordinates f(x) = x_{(1)} + \\ldots + x_{(k)},\\; x \\in \\mathbb{R}^n\nf(X) = \\lambda_{max}(X),\\;  X = X^T\nf(X) = - \\log \\det X, \\;  X \\in S^n_{++}\n\n\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\text{epi } f = \\left\\{[x,\\mu] \\in S \\times \\mathbb{R}: f(x) \\le \\mu\\right\\}\n\nis called epigraph of the function f(x).\n\n\n\n\n\n\nFigure 2: Epigraph of a function\n\n\n\n\n\n\n\n\n\nConvexity of the epigraph is the convexity of the function\n\n\n\n\n\nFor a function f(x), defined on a convex set X, to be convex on X, it is necessary and sufficient that the epigraph of f is a convex set.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nNecessity: Assume f(x) is convex on X. Take any two arbitrary points [x_1, \\mu_1] \\in \\text{epi}f and [x_2, \\mu_2] \\in \\text{epi}f. Also take 0 \\leq \\lambda \\leq 1 and denote x_{\\lambda} = \\lambda x_1 + (1 - \\lambda) x_2, \\mu_{\\lambda} = \\lambda \\mu_1 + (1 - \\lambda) \\mu_2. Then,\n\n\\lambda\\begin{bmatrix}x_1\\\\ \\mu_1\\end{bmatrix} + (1 - \\lambda)\\begin{bmatrix}x_2\\\\ \\mu_2\\end{bmatrix} = \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix}.\n\nFrom the convexity of the set X, it follows that x_{\\lambda} \\in X. Moreover, since f(x) is a convex function,\n\nf(x_{\\lambda}) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2) \\leq \\lambda \\mu_1 + (1 - \\lambda) \\mu_2 = \\mu_{\\lambda}\n\nInequality above indicates that \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix} \\in \\text{epi}f. Thus, the epigraph of f is a convex set.\nSufficiency: Assume the epigraph of f, \\text{epi}f, is a convex set. Then, from the membership of the points [x_1, \\mu_1] and [x_2, \\mu_2] in the epigraph of f, it follows that\n\n  \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix} =  \\lambda\\begin{bmatrix}x_1\\\\ \\mu_1\\end{bmatrix} + (1 - \\lambda)\\begin{bmatrix}x_2\\\\ \\mu_2\\end{bmatrix} \\in \\text{epi}f\n\nfor any 0 \\leq \\lambda \\leq 1, i.e., f(x_{\\lambda}) \\leq \\mu_{\\lambda} = \\lambda \\mu_1 + (1 - \\lambda) \\mu_2. But this is true for all \\mu_1 \\geq f(x_1) and \\mu_2 \\geq f(x_2), particularly when \\mu_1 = f(x_1) and \\mu_2 = f(x_2). Hence we arrive at the inequality\n\nf(x_{\\lambda}) = f (\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2).\n\nSince points x_1 \\in X and x_2 \\in X can be arbitrarily chosen, f(x) is a convex function on X.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\mathcal{L}_\\beta = \\left\\{ x\\in S : f(x) \\le \\beta\\right\\}\n\nis called sublevel set or Lebesgue set of the function f(x).\n\n\n\n\n\n\nFigure 3: Sublevel set of a function with respect to level \\beta",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#convexity-definitions",
    "href": "docs/theory/Convex_function.html#convexity-definitions",
    "title": "Convex function",
    "section": "",
    "text": "The function f(x), which is defined on the convex set S \\subseteq \\mathbb{R}^n, is called convex on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1.\nIf the above inequality holds as strict inequality x_1 \\neq x_2 and 0 &lt; \\lambda &lt; 1, then the function is called strictly convex on S.\n\n\n\n\n\n\nFigure 1: Difference between convex and non-convex function\n\n\n\n\n\n\n\n\n\nJensen’s inequality\n\n\n\n\n\nLet f(x) be a convex function on a convex set X \\subseteq \\mathbb{R}^n and let x_i \\in X, 1 \\leq i \\leq m, be arbitrary points from X. Then\n\nf\\left( \\sum_{i=1}^{m} \\lambda_i x_i \\right) \\leq \\sum_{i=1}^{m} \\lambda_i f(x_i)\n\nfor any \\lambda = [\\lambda_1, \\ldots, \\lambda_m] \\in \\Delta_m - probability simplex.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nFirst, note that the point \\sum_{i=1}^{m} \\lambda_i x_i as a convex combination of points from the convex set X belongs to X.\nWe will prove this by induction. For m = 1, the statement is obviously true, and for m = 2, it follows from the definition of a convex function.\nAssume it is true for all m up to m = k, and we will prove it for m = k + 1. Let \\lambda \\in \\Delta{k+1} and\n\nx = \\sum_{i=1}^{k+1} \\lambda_i x_i = \\sum_{i=1}^{k} \\lambda_i x_i + \\lambda_{k+1} x_{k+1}.\n\nAssuming 0 &lt; \\lambda_{k+1} &lt; 1, as otherwise, it reduces to previously considered cases, we have\n\nx = \\lambda_{k+1} x_{k+1} + (1 - \\lambda_{k+1}) \\bar{x},\n\nwhere \\bar{x} = \\sum_{i=1}^{k} \\gamma_i x_i and \\gamma_i = \\frac{\\lambda_i}{1-\\lambda_{k+1}} \\geq 0, 1 \\leq i \\leq k.\nSince \\lambda \\in \\Delta_{k+1}, then \\gamma = [\\gamma_1, \\ldots, \\gamma_k] \\in \\Delta_k. Therefore \\bar{x} \\in X and by the convexity of f(x) and the induction hypothesis:\n\n\\begin{split}\nf\\left( \\sum_{i=1}^{k+1} \\lambda_i x_i \\right) = f\\left( \\lambda_{k+1} x_{k+1} + (1 - \\lambda_{k+1})\\bar{x} \\right) &\\leq \\\\\n\\lambda_{k+1}f(x_{k+1}) + (1 - \\lambda_{k+1})f(\\bar{x}) \\leq \\sum_{i=1}^{k+1} \\lambda_i f(x_i)&\n\\end{split}\n\nThus, initial inequality is satisfied for m = k + 1 as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nf(x) = x^p, \\;  p &gt; 1,\\;  x \\in \\mathbb{R}_+\nf(x) = \\|x\\|^p,\\;  p &gt; 1, x \\in \\mathbb{R}^n\nf(x) = e^{cx},\\;  c \\in \\mathbb{R}, x \\in \\mathbb{R}\nf(x) = -\\ln x,\\;  x \\in \\mathbb{R}_{++}\nf(x) = x\\ln x,\\;  x \\in \\mathbb{R}_{++}\nThe sum of the largest k coordinates f(x) = x_{(1)} + \\ldots + x_{(k)},\\; x \\in \\mathbb{R}^n\nf(X) = \\lambda_{max}(X),\\;  X = X^T\nf(X) = - \\log \\det X, \\;  X \\in S^n_{++}\n\n\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\text{epi } f = \\left\\{[x,\\mu] \\in S \\times \\mathbb{R}: f(x) \\le \\mu\\right\\}\n\nis called epigraph of the function f(x).\n\n\n\n\n\n\nFigure 2: Epigraph of a function\n\n\n\n\n\n\n\n\n\nConvexity of the epigraph is the convexity of the function\n\n\n\n\n\nFor a function f(x), defined on a convex set X, to be convex on X, it is necessary and sufficient that the epigraph of f is a convex set.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nNecessity: Assume f(x) is convex on X. Take any two arbitrary points [x_1, \\mu_1] \\in \\text{epi}f and [x_2, \\mu_2] \\in \\text{epi}f. Also take 0 \\leq \\lambda \\leq 1 and denote x_{\\lambda} = \\lambda x_1 + (1 - \\lambda) x_2, \\mu_{\\lambda} = \\lambda \\mu_1 + (1 - \\lambda) \\mu_2. Then,\n\n\\lambda\\begin{bmatrix}x_1\\\\ \\mu_1\\end{bmatrix} + (1 - \\lambda)\\begin{bmatrix}x_2\\\\ \\mu_2\\end{bmatrix} = \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix}.\n\nFrom the convexity of the set X, it follows that x_{\\lambda} \\in X. Moreover, since f(x) is a convex function,\n\nf(x_{\\lambda}) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2) \\leq \\lambda \\mu_1 + (1 - \\lambda) \\mu_2 = \\mu_{\\lambda}\n\nInequality above indicates that \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix} \\in \\text{epi}f. Thus, the epigraph of f is a convex set.\nSufficiency: Assume the epigraph of f, \\text{epi}f, is a convex set. Then, from the membership of the points [x_1, \\mu_1] and [x_2, \\mu_2] in the epigraph of f, it follows that\n\n  \\begin{bmatrix}x_{\\lambda}\\\\ \\mu_{\\lambda}\\end{bmatrix} =  \\lambda\\begin{bmatrix}x_1\\\\ \\mu_1\\end{bmatrix} + (1 - \\lambda)\\begin{bmatrix}x_2\\\\ \\mu_2\\end{bmatrix} \\in \\text{epi}f\n\nfor any 0 \\leq \\lambda \\leq 1, i.e., f(x_{\\lambda}) \\leq \\mu_{\\lambda} = \\lambda \\mu_1 + (1 - \\lambda) \\mu_2. But this is true for all \\mu_1 \\geq f(x_1) and \\mu_2 \\geq f(x_2), particularly when \\mu_1 = f(x_1) and \\mu_2 = f(x_2). Hence we arrive at the inequality\n\nf(x_{\\lambda}) = f (\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2).\n\nSince points x_1 \\in X and x_2 \\in X can be arbitrarily chosen, f(x) is a convex function on X.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the function f(x), defined on S \\subseteq \\mathbb{R}^n, the following set:\n\n\\mathcal{L}_\\beta = \\left\\{ x\\in S : f(x) \\le \\beta\\right\\}\n\nis called sublevel set or Lebesgue set of the function f(x).\n\n\n\n\n\n\nFigure 3: Sublevel set of a function with respect to level \\beta",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#criteria-of-convexity",
    "href": "docs/theory/Convex_function.html#criteria-of-convexity",
    "title": "Convex function",
    "section": "2 Criteria of convexity",
    "text": "2 Criteria of convexity\n\n2.1 First-order differential criterion of convexity\nThe differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x)\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x\n\n\n\n\n\n\n\nFigure 4: Convex function is greater or equal than Taylor linear approximation at any point\n\n\n\n\n\n2.2 Second-order differential criterion of convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq 0\n\nIn other words, \\forall y \\in \\mathbb{R}^n:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq 0\n\n\n\n2.3 Connection with epigraph\nThe function is convex if and only if its epigraph is a convex set.\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet a norm \\Vert \\cdot \\Vert be defined in the space U. Consider the set:\n\nK := \\{(x,t) \\in U \\times \\mathbb{R}^+ : \\Vert x \\Vert \\leq t \\}\n\nwhich represents the epigraph of the function x \\mapsto \\Vert x \\Vert. This set is called the cone norm. According to the statement above, the set K is convex.\nIn the case where U = \\mathbb{R}^n and \\Vert x \\Vert = \\Vert x \\Vert_2 (Euclidean norm), the abstract set K transitions into the set:\n\n\\{(x,t) \\in \\mathbb{R}^n \\times \\mathbb{R}^+ : \\Vert x \\Vert_2 \\leq t \\}\n\n\n\n\n\n\n\n2.4 Connection with sublevel set\nIf f(x) - is a convex function defined on the convex set S \\subseteq \\mathbb{R}^n, then for any \\beta sublevel set \\mathcal{L}_\\beta is convex.\nThe function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is closed if and only if for any \\beta sublevel set \\mathcal{L}_\\beta is closed.\n\n\n2.5 Reduction to a line\nf: S \\to \\mathbb{R} is convex if and only if S is a convex set and the function g(t) = f(x + tv) defined on \\left\\{ t \\mid x + tv \\in S \\right\\} is convex for any x \\in S, v \\in \\mathbb{R}^n, which allows checking convexity of the scalar function to establish convexity of the vector function.",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#strong-convexity",
    "href": "docs/theory/Convex_function.html#strong-convexity",
    "title": "Convex function",
    "section": "3 Strong convexity",
    "text": "3 Strong convexity\nf(x), defined on the convex set S \\subseteq \\mathbb{R}^n, is called \\mu-strongly convex (strongly convex) on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0.\n\n\n\n\n\n\nFigure 5: Strongly convex function is greater or equal than Taylor quadratic approximation at any point\n\n\n\n\n3.1 Criteria of strong convexity\n\n3.1.1 First-order differential criterion of strong convexity\nDifferentiable f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is \\mu-strongly convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x) + \\dfrac{\\mu}{2}\\|y-x\\|^2\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x + \\dfrac{\\mu}{2}\\|\\Delta x\\|^2\n\n\n\n3.1.2 Second-order differential criterion of strong convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is called \\mu-strongly convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq \\mu I\n\nIn other words:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq \\mu \\|y\\|^2\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet f(x) be a differentiable function on a convex set X \\subseteq \\mathbb{R}^n. Then f(x) is strongly convex on X with a constant \\mu &gt; 0 if and only if\n\nf(x) - f(x_0) \\geq \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{\\mu}{2} \\| x - x_0 \\|^2\n\nfor all x, x_0 \\in X.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nNecessity: Let 0 &lt; \\lambda \\leq 1. According to the definition of a strongly convex function,\n\nf(\\lambda x + (1 - \\lambda) x_0) \\leq \\lambda f(x) + (1 - \\lambda) f(x_0) - \\frac{\\mu}{2} \\lambda (1 - \\lambda) \\| x - x_0 \\|^2\n\nor equivalently,\n\nf(x) - f(x_0) - \\frac{\\mu}{2} (1 - \\lambda) \\| x - x_0 \\|^2 \\geq \\frac{1}{\\lambda} [f(\\lambda x + (1 - \\lambda) x_0) - f(x_0)] =\n\n\n= \\frac{1}{\\lambda} [f(x_0 + \\lambda(x - x_0)) - f(x_0)] = \\frac{1}{\\lambda} [\\lambda \\langle \\nabla f(x_0), x - x_0 \\rangle + o(\\lambda)] =\n\n\n= \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{o(\\lambda)}{\\lambda}.\n\nThus, taking the limit as \\lambda \\downarrow 0, we arrive at the initial statement.\nSufficiency: Assume the inequality in the theorem is satisfied for all x, x_0 \\in X. Take x_0 = \\lambda x_1 + (1 - \\lambda) x_2, where x_1, x_2 \\in X, 0 \\leq \\lambda \\leq 1. According to the inequality, the following inequalities hold:\n\nf(x_1) - f(x_0) \\geq \\langle \\nabla f(x_0), x_1 - x_0 \\rangle + \\frac{\\mu}{2} \\| x_1 - x_0 \\|^2,\n\n\nf(x_2) - f(x_0) \\geq \\langle \\nabla f(x_0), x_2 - x_0 \\rangle + \\frac{\\mu}{2} \\| x_2 - x_0 \\|^2.\n\nMultiplying the first inequality by \\lambda and the second by 1 - \\lambda and adding them, considering that\n\nx_1 - x_0 = (1 - \\lambda)(x_1 - x_2), \\quad x_2 - x_0 = \\lambda(x_2 - x_1),\n\nand \\lambda(1 - \\lambda)^2 + \\lambda^2(1 - \\lambda) = \\lambda(1 - \\lambda), we get\n\n\\begin{split}\n\\lambda f(x_1) + (1 - \\lambda) f(x_2) - f(x_0) - \\frac{\\mu}{2} \\lambda (1 - \\lambda) \\| x_1 - x_2 \\|^2 \\geq \\\\\n\\langle \\nabla f(x_0), \\lambda x_1 + (1 - \\lambda) x_2 - x_0 \\rangle = 0.\n\\end{split}\n\nThus, inequality from the definition of a strongly convex function is satisfied. It is important to mention, that \\mu = 0 stands for the convex case and corresponding differential criterion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet X \\subseteq \\mathbb{R}^n be a convex set, with \\text{int}X \\neq \\emptyset. Furthermore, let f(x) be a twice continuously differentiable function on X. Then f(x) is strongly convex on X with a constant \\mu &gt; 0 if and only if\n\n\\langle y, \\nabla^2 f(x) y \\rangle \\geq \\mu \\| y \\|^2 \\quad\n\nfor all x \\in X and y \\in \\mathbb{R}^n.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nThe target inequality is trivial when y = \\mathbf{0}_n, hence we assume y \\neq \\mathbf{0}_n.\nNecessity: Assume initially that x is an interior point of X. Then x + \\alpha y \\in X for all y \\in \\mathbb{R}^n and sufficiently small \\alpha. Since f(x) is twice differentiable,\n\nf(x + \\alpha y) = f(x) + \\alpha \\langle \\nabla f(x), y \\rangle + \\frac{\\alpha^2}{2} \\langle y, \\nabla^2 f(x) y \\rangle + o(\\alpha^2).\n\nBased on the first order criterion of strong convexity, we have\n\n\\frac{\\alpha^2}{2} \\langle y, \\nabla^2 f(x) y \\rangle + o(\\alpha^2) = f(x + \\alpha y) - f(x) - \\alpha \\langle \\nabla f(x), y \\rangle \\geq \\frac{\\mu}{2} \\alpha^2 \\| y \\|^2.\n\nThis inequality reduces to the target inequality after dividing both sides by \\alpha^2 and taking the limit as \\alpha \\downarrow 0.\nIf x \\in X but x \\notin \\text{int}X, consider a sequence \\{x_k\\} such that x_k \\in \\text{int}X and x_k \\rightarrow x as k \\rightarrow \\infty. Then, we arrive at the target inequality after taking the limit.\nSufficiency: Using Taylor’s formula with the Lagrange remainder and the target inequality, we obtain for x + y \\in X:\n\nf(x + y) - f(x) - \\langle \\nabla f(x), y \\rangle = \\frac{1}{2} \\langle y, \\nabla^2 f(x + \\alpha y) y \\rangle \\geq \\frac{\\mu}{2} \\| y \\|^2,\n\nwhere 0 \\leq \\alpha \\leq 1. Therefore,\n\nf(x + y) - f(x) \\geq \\langle \\nabla f(x), y \\rangle + \\frac{\\mu}{2} \\| y \\|^2.\n\nConsequently, by the first order criterion of strong convexity, the function f(x) is strongly convex with a constant \\mu. It is important to mention, that \\mu = 0 stands for the convex case and corresponding differential criterion.",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#facts",
    "href": "docs/theory/Convex_function.html#facts",
    "title": "Convex function",
    "section": "4 Facts",
    "text": "4 Facts\n\nf(x) is called (strictly) concave, if the function -f(x) - is (strictly) convex.\nJensen’s inequality for the convex functions:\n\n  f \\left( \\sum\\limits_{i=1}^n \\alpha_i x_i \\right) \\leq \\sum\\limits_{i=1}^n \\alpha_i f(x_i)\n  \nfor \\alpha_i \\geq 0; \\quad \\sum\\limits_{i=1}^n \\alpha_i = 1 (probability simplex)\nFor the infinite dimension case:\n\n  f \\left( \\int\\limits_{S} x p(x)dx \\right) \\leq \\int\\limits_{S} f(x)p(x)dx\n  \nIf the integrals exist and p(x) \\geq 0, \\quad \\int\\limits_{S} p(x)dx = 1.\nIf the function f(x) and the set S are convex, then any local minimum x^* = \\text{arg}\\min\\limits_{x \\in S} f(x) will be the global one. Strong convexity guarantees the uniqueness of the solution.\nLet f(x) - be a convex function on a convex set S \\subseteq \\mathbb{R}^n. Then f(x) is continuous \\forall x \\in \\textbf{ri}(S).",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#operations-that-preserve-convexity",
    "href": "docs/theory/Convex_function.html#operations-that-preserve-convexity",
    "title": "Convex function",
    "section": "5 Operations that preserve convexity",
    "text": "5 Operations that preserve convexity\n\nNon-negative sum of the convex functions: \\alpha f(x) + \\beta g(x), (\\alpha \\geq 0 , \\beta \\geq 0).\nComposition with affine function f(Ax + b) is convex, if f(x) is convex.\nPointwise maximum (supremum) of any number of functions: If f_1(x), \\ldots, f_m(x) are convex, then f(x) = \\max \\{f_1(x), \\ldots, f_m(x)\\} is convex.\nIf f(x,y) is convex on x for any y \\in Y: g(x) = \\underset{y \\in Y}{\\operatorname{sup}}f(x,y) is convex.\nIf f(x) is convex on S, then g(x,t) = t f(x/t) - is convex with x/t \\in S, t &gt; 0.\nLet f_1: S_1 \\to \\mathbb{R} and f_2: S_2 \\to \\mathbb{R}, where \\operatorname{range}(f_1) \\subseteq S_2. If f_1 and f_2 are convex, and f_2 is increasing, then f_2 \\circ f_1 is convex on S_1.",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#other-forms-of-convexity",
    "href": "docs/theory/Convex_function.html#other-forms-of-convexity",
    "title": "Convex function",
    "section": "6 Other forms of convexity",
    "text": "6 Other forms of convexity\n\nLog-convex: \\log f is convex; Log convexity implies convexity.\nLog-concavity: \\log f concave; not closed under addition!\nExponentially convex: [f(x_i + x_j )] \\succeq 0, for x_1, \\ldots , x_n\nOperator convex: f(\\lambda X + (1 − \\lambda )Y ) \\preceq \\lambda f(X) + (1 − \\lambda )f(Y)\nQuasiconvex: f(\\lambda x + (1 − \\lambda) y) \\leq \\max \\{f(x), f(y)\\}\nPseudoconvex: \\langle \\nabla f(y), x − y \\rangle \\geq 0 \\longrightarrow f(x) \\geq f(y)\nDiscrete convexity: f : \\mathbb{Z}^n \\to \\mathbb{Z}; “convexity + matroid theory.”\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nShow, that f(x) = c^\\top x + b is convex and concave.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nShow, that f(x) = x^\\top Ax, where A\\succeq 0 - is convex on \\mathbb{R}^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nShow, that f(A) = \\lambda_{max}(A) - is convex, if A \\in S^n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nPL inequality holds if the following condition is satisfied for some \\mu &gt; 0, \n\\Vert \\nabla f(x) \\Vert^2 \\geq \\mu (f(x) - f^*) \\forall x\n The example of a function, that satisfies the PL-condition, but is not convex. \nf(x,y) = \\dfrac{(y - \\sin x)^2}{2}",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Convex_function.html#references",
    "href": "docs/theory/Convex_function.html#references",
    "title": "Convex function",
    "section": "7 References",
    "text": "7 References\n\nSteven Boyd lectures\nSuvrit Sra lectures\nMartin Jaggi lectures\nExample of Pl non-convex function Open in Colab",
    "crumbs": [
      "Theory",
      "Convex function"
    ]
  },
  {
    "objectID": "docs/theory/Dual norm.html",
    "href": "docs/theory/Dual norm.html",
    "title": "Dual norm",
    "section": "",
    "text": "p-norm and q-norm are dual if this holds\n\n\nLet \\Vert x\\Vert be the norm in the primal space x \\in S \\subseteq \\mathbb{R}^n, then the following expression defines dual norm:\n\n\\Vert y\\Vert _\\star = \\sup\\limits_{\\Vert x\\Vert  \\leq 1} \\langle y,x\\rangle\n\nThe intuition for the finite-dimensional space is how the linear function (element of the dual space) f_y(\\cdot) could stretch the elements of the primal space with respect to their size, i.e. \\Vert y\\Vert _* = \\sup\\limits_{x \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert x\\Vert }.",
    "crumbs": [
      "Theory",
      "Dual norm"
    ]
  },
  {
    "objectID": "docs/theory/Dual norm.html#dual-norm",
    "href": "docs/theory/Dual norm.html#dual-norm",
    "title": "Dual norm",
    "section": "",
    "text": "p-norm and q-norm are dual if this holds\n\n\nLet \\Vert x\\Vert be the norm in the primal space x \\in S \\subseteq \\mathbb{R}^n, then the following expression defines dual norm:\n\n\\Vert y\\Vert _\\star = \\sup\\limits_{\\Vert x\\Vert  \\leq 1} \\langle y,x\\rangle\n\nThe intuition for the finite-dimensional space is how the linear function (element of the dual space) f_y(\\cdot) could stretch the elements of the primal space with respect to their size, i.e. \\Vert y\\Vert _* = \\sup\\limits_{x \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert x\\Vert }.",
    "crumbs": [
      "Theory",
      "Dual norm"
    ]
  },
  {
    "objectID": "docs/theory/Dual norm.html#properties",
    "href": "docs/theory/Dual norm.html#properties",
    "title": "Dual norm",
    "section": "2 Properties",
    "text": "2 Properties\n\nOne can easily define the dual norm as:\n\n  \\Vert y\\Vert _* = \\sup\\limits_{x \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert x\\Vert }\n  \nThe dual norm is also a norm itself\nFor any x \\in E, y \\in E^*: x^\\top y \\leq \\Vert x\\Vert  \\cdot \\Vert y\\Vert _*\n\\left(\\Vert x\\Vert _p\\right)_* = \\Vert x\\Vert _q if \\dfrac{1}{p} + \\dfrac{1}{q} = 1, where p, q \\geq 1\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe Euclidian norm is self dual \\left(\\Vert x\\Vert_2\\right)_\\star = \\Vert x\\Vert _2.",
    "crumbs": [
      "Theory",
      "Dual norm"
    ]
  },
  {
    "objectID": "docs/theory/Dual norm.html#examples",
    "href": "docs/theory/Dual norm.html#examples",
    "title": "Dual norm",
    "section": "3 Examples",
    "text": "3 Examples\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet f(x) = \\Vert x\\Vert. Prove that f^\\star(y) = \\mathbb{O}_{\\Vert y\\Vert _\\star \\leq 1}\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nBy definition of the conjugate function:\n\nf^*(y) = \\sup_{x} \\{ \\langle y, x \\rangle - f(x) \\} = \\sup_{x} \\{ \\langle y, x \\rangle - \\|x\\| \\}\n\nConsider the case \\|y\\|_* &gt; 1. By definition of the dual norm,\n\n\\Vert y\\Vert _* = \\sup\\limits_{\\Vert x\\Vert  \\leq 1} \\langle y,x\\rangle &gt; 1\n\nWhich means, that there is some x^\\dagger, such that \\|x^\\dagger\\|\\leq 1, but \\langle y,x^\\dagger\\rangle &gt; 1. Now consider the vector \\bar{x} = tx^\\dagger, where t \\in \\mathbb{R}^+. The value of the conjugate function is a supremum, therefore we have the following relation:\n\n\\begin{split}\nf^*(y) &\\geq \\langle y, \\bar{x} \\rangle - \\|\\bar{x}\\| = \\langle y, tx^\\dagger \\rangle - t\\|x^\\dagger\\|\\\\\n&= t(\\langle y, x^\\dagger \\rangle - \\|x^\\dagger\\|) \\to \\infty \\text{ with } t \\to \\infty\n\\end{split}\n\nThus, \\|y\\|_* &gt; 1 does not belong to the \\text{dom } f^*.\nConsider the case \\|y\\|_* \\leq 1. From the definition of the dual norm:\n\n\\langle y, x \\rangle \\leq \\| y \\|_* \\| x \\| \\leq \\| x \\|\n\nEquality holds when x=0. Therefore\n\nf^*(y) = \\sup_{x} \\{ \\langle y, x \\rangle - \\|x\\| \\} = 0",
    "crumbs": [
      "Theory",
      "Dual norm"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html",
    "href": "docs/theory/Matrix_calculus.html",
    "title": "Matrix calculus",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct that a positive semidefinite matrix has all non-negative positive entries?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nAnswer: No.\nA positive semidefinite matrix does not necessarily have all non-negative entries. The definition of a positive semidefinite matrix is that for any non-zero vector x, the quadratic form x^\\top A x &gt; 0. This implies that the matrix has non-negative eigenvalues, but the individual entries of the matrix could be negative. For example, the matrix \nA = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n is positive semidefinite, because x^\\top A x = x_1^2 + x_2^2 - 2x_1x_2 \\geq 0 but it contains negative entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct that a matrix should be positive definite if it is symmetric?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nAnswer: No.\nA matrix being symmetric does not necessarily mean it is positive definite. A symmetric matrix can have negative or zero eigenvalues, in which case it would be either negative definite, indefinite, or positive semidefinite. A matrix is positive definite only if all its eigenvalues are positive. For instance, the matrix \nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n is symmetric but not positive definite because it has a negative eigenvalue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is positive definite it should be symmetric?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nIt is a tricky question. By definition a matrix is called positive definite only if it is symmetric. But in fact, if we talk about the real field x \\in \\mathbb{R}^n, it is easy to imagine a non-symmetric matrix for which x^T A x \\geq 0 is satisfied, but the matrix will not be symmetric: \nA = \\begin{pmatrix} 1 & 0 \\\\ -3 & 1 \\end{pmatrix}\n However, the reader will not be able to come up with such a matrix if we allow the vector x \\in \\mathbb{C}^n to have complex entries and replace the transpose in the definition with a complex conjugation.\n\n\n\n\n\n\n\n\n\n\n\n\nLet A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nThis operation in a naive form requires \\mathcal{O}(n^2) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\n\n\n\n\n\n\n\nSimple yet important idea on matrix computations.\n\n\n\n\n\nSuppose, you have the following expression\n\nb = A_1 A_2 A_3 x,\n\nwhere the A_1, A_2, A_3 \\in \\mathbb{R}^{3 \\times 3} - random square dense matrices and x \\in \\mathbb{R}^n - vector. You need to compute b.\nWhich one way is the best to do it?\n\nA_1 A_2 A_3 x (from left to right)\n\\left(A_1 \\left(A_2 \\left(A_3 x\\right)\\right)\\right) (from right to left)\nIt does not matter\nThe results of the first two options will not be the same.\n\nCheck the simple code snippet after all.\n\n\n\n\n\n\n\nNorm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 3. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\nIf we use spectral matrix norm, we can get:\n\n\\kappa(A) = \\dfrac{\\sigma_{\\text{max}}(A)}{\\sigma _{\\text{min}}(A)}\n\nIf, moreover, A \\in \\mathbb{S}^n_{++}: \\kappa(A) = \\dfrac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)}\n\n\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad\n\n\n\n\n\n\n\nFigure 3: Usual form of SVD\n\n\n\n\n\n\n\n\n\nFigure 4: Economic form of SVD\n\n\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that\n\n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A),\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\|A\\|_F^2 = \\langle A, A\\rangle = \\text{tr }(A^T A)\nUsing SVD: A = U \\Sigma V^T \\quad A^T = V \\Sigma U^T\n\\|A\\|_F^2 = \\text{tr }(V \\Sigma U^T U \\Sigma V^T) = \\text{tr }(V \\Sigma^2 V^T) = \\text{tr }(V^T V \\Sigma^2) = \\text{tr }(\\Sigma^2) = \\sum\\limits_{1}^q \\sigma_i^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\sigma(A) = \\lambda(A) &gt; 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\sigma(A) = \\sqrt{\\lambda(A^TA)} = |\\lambda(A)|\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\n\n\n\nFigure 5: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\nOne can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\n\n\n\nFigure 6: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?\n\n\n\n\n\n\n\nThe determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det}  AB = (\\text{det} A)(\\text{det}  B);\n\\text{det}  A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#basic-linear-algebra-background",
    "href": "docs/theory/Matrix_calculus.html#basic-linear-algebra-background",
    "title": "Matrix calculus",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct that a positive semidefinite matrix has all non-negative positive entries?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nAnswer: No.\nA positive semidefinite matrix does not necessarily have all non-negative entries. The definition of a positive semidefinite matrix is that for any non-zero vector x, the quadratic form x^\\top A x &gt; 0. This implies that the matrix has non-negative eigenvalues, but the individual entries of the matrix could be negative. For example, the matrix \nA = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n is positive semidefinite, because x^\\top A x = x_1^2 + x_2^2 - 2x_1x_2 \\geq 0 but it contains negative entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct that a matrix should be positive definite if it is symmetric?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nAnswer: No.\nA matrix being symmetric does not necessarily mean it is positive definite. A symmetric matrix can have negative or zero eigenvalues, in which case it would be either negative definite, indefinite, or positive semidefinite. A matrix is positive definite only if all its eigenvalues are positive. For instance, the matrix \nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n is symmetric but not positive definite because it has a negative eigenvalue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is positive definite it should be symmetric?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nIt is a tricky question. By definition a matrix is called positive definite only if it is symmetric. But in fact, if we talk about the real field x \\in \\mathbb{R}^n, it is easy to imagine a non-symmetric matrix for which x^T A x \\geq 0 is satisfied, but the matrix will not be symmetric: \nA = \\begin{pmatrix} 1 & 0 \\\\ -3 & 1 \\end{pmatrix}\n However, the reader will not be able to come up with such a matrix if we allow the vector x \\in \\mathbb{C}^n to have complex entries and replace the transpose in the definition with a complex conjugation.\n\n\n\n\n\n\n\n\n\n\n\n\nLet A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nThis operation in a naive form requires \\mathcal{O}(n^2) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\n\n\n\n\n\n\n\nSimple yet important idea on matrix computations.\n\n\n\n\n\nSuppose, you have the following expression\n\nb = A_1 A_2 A_3 x,\n\nwhere the A_1, A_2, A_3 \\in \\mathbb{R}^{3 \\times 3} - random square dense matrices and x \\in \\mathbb{R}^n - vector. You need to compute b.\nWhich one way is the best to do it?\n\nA_1 A_2 A_3 x (from left to right)\n\\left(A_1 \\left(A_2 \\left(A_3 x\\right)\\right)\\right) (from right to left)\nIt does not matter\nThe results of the first two options will not be the same.\n\nCheck the simple code snippet after all.\n\n\n\n\n\n\n\nNorm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 3. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\nIf we use spectral matrix norm, we can get:\n\n\\kappa(A) = \\dfrac{\\sigma_{\\text{max}}(A)}{\\sigma _{\\text{min}}(A)}\n\nIf, moreover, A \\in \\mathbb{S}^n_{++}: \\kappa(A) = \\dfrac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)}\n\n\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad\n\n\n\n\n\n\n\nFigure 3: Usual form of SVD\n\n\n\n\n\n\n\n\n\nFigure 4: Economic form of SVD\n\n\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that\n\n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A),\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\|A\\|_F^2 = \\langle A, A\\rangle = \\text{tr }(A^T A)\nUsing SVD: A = U \\Sigma V^T \\quad A^T = V \\Sigma U^T\n\\|A\\|_F^2 = \\text{tr }(V \\Sigma U^T U \\Sigma V^T) = \\text{tr }(V \\Sigma^2 V^T) = \\text{tr }(V^T V \\Sigma^2) = \\text{tr }(\\Sigma^2) = \\sum\\limits_{1}^q \\sigma_i^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\sigma(A) = \\lambda(A) &gt; 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\\sigma(A) = \\sqrt{\\lambda(A^TA)} = |\\lambda(A)|\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\n\n\n\nFigure 5: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\nOne can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\n\n\n\nFigure 6: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?\n\n\n\n\n\n\n\nThe determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det}  AB = (\\text{det} A)(\\text{det}  B);\n\\text{det}  A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#optimization-bingo",
    "href": "docs/theory/Matrix_calculus.html#optimization-bingo",
    "title": "Matrix calculus",
    "section": "2 Optimization bingo",
    "text": "2 Optimization bingo\n\n2.1 Gradient\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then vector, which contains all first-order partial derivatives:\n\n\\nabla f(x) = \\dfrac{df}{dx} = \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x_1} \\\\\n    \\frac{\\partial f}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial x_n}\n\\end{pmatrix}\n\nnamed gradient of f(x). This vector indicates the direction of the steepest ascent. Thus, vector −\\nabla f(x) means the direction of the steepest descent of the function in the point. Moreover, the gradient vector is always orthogonal to the contour line in the point.\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the gradient is: \n\\nabla f(x, y) =\n\\begin{bmatrix}\n2x \\\\\n2y \\\\\n\\end{bmatrix}\n This gradient points in the direction of the steepest ascent of the function.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the magnitude of the gradient relate to the steepness of the function?\n\n\n\n\n\n\n2.2 Hessian\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then matrix, containing all the second order partial derivatives:\n\nf''(x) = \\nabla^2 f(x) = \\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix}\n    \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_n \\partial x_n}\n\\end{pmatrix}\n\nIn fact, Hessian could be a tensor in such a way: \\left(f(x): \\mathbb{R}^n \\to \\mathbb{R}^m \\right) is just 3d tensor, every slice is just hessian of corresponding scalar function \\left( \\nabla^2f_1(x), \\ldots, \\nabla^2f_m(x)\\right).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the Hessian is:\n\nH_f(x, y) = \\begin{bmatrix} 2 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the curvature of the function in different directions.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow can the Hessian matrix be used to determine the concavity or convexity of a function?\n\n\n\n\n\n\n\n\n\n\nSchwartz theorem\n\n\n\n\n\nLet f: \\mathbb{R}^n \\rightarrow \\mathbb{R} be a function. If the mixed partial derivatives \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} and \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} are both continuous on an open set containing a point a, then they are equal at the point a. That is, \n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (a) = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} (a)\n\nGiven the Schwartz theorem, if the mixed partials are continuous on an open set, the Hessian matrix is symmetric. That means the entries above the main diagonal mirror those below the main diagonal:\n\n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} \\quad \\nabla^2 f(x)  =(\\nabla^2 f(x))^T\n\nThis symmetry simplifies computations and analysis involving the Hessian matrix in various applications, particularly in optimization.\n\n\n\n\n\n\n\n\n\n\nSchwartz counterexample\n\n\n\n\n\n\nf(x,y) =\n\\begin{cases}\n    \\frac{xy\\left(x^2 - y^2\\right)}{x^2 + y^2} & \\text{ for } (x,\\, y) \\ne (0,\\, 0),\\\\\n    0 & \\text{ for } (x, y) = (0, 0).\n\\end{cases}\n\n                        \n                                            \nOne can verify, that \\frac{\\partial^2 f}{ \\partial x \\partial y} (0, 0) \\neq \\frac{\\partial^2 f}{ \\partial y \\partial x} (0, 0), although the mixed partial derivatives do exist, and at every other point the symmetry does hold.\n\n\n\n\n\n\n2.3 Jacobian\nThe extension of the gradient of multidimensional f(x):\\mathbb{R}^n\\to\\mathbb{R}^m is the following matrix:\n\nJ_f = f'(x) = \\dfrac{df}{dx^T} = \\begin{pmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_1} & \\dots  & \\frac{\\partial f_m}{\\partial x_1} \\\\\n    \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_2} & \\dots  & \\frac{\\partial f_m}{\\partial x_2} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_1}{\\partial x_n} & \\frac{\\partial f_2}{\\partial x_n} & \\dots  & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function\n\nf(x, y) = \\begin{bmatrix}\nx + y \\\\\nx - y \\\\\n\\end{bmatrix},\n the Jacobian is: \nJ_f(x, y) = \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the rate of change of the function with respect to its inputs.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the Jacobian matrix relate to the gradient for scalar-valued functions?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nCan we somehow connect those three definitions above (gradient, jacobian, and hessian) using a single correct statement?\n\n\n\n\n\n\n2.4 Summary\n\nf(x) : X \\to Y; \\qquad \\frac{\\partial f(x)}{\\partial x} \\in G\n\n\n\n\n\n\n\n\n\n\nX\nY\nG\nName\n\n\n\n\n\\mathbb{R}\n\\mathbb{R}\n\\mathbb{R}\nf'(x) (derivative)\n\n\n\\mathbb{R}^n\n\\mathbb{R}\n\\mathbb{R^n}\n\\dfrac{\\partial f}{\\partial x_i} (gradient)\n\n\n\\mathbb{R}^n\n\\mathbb{R}^m\n\\mathbb{R}^{n \\times m}\n\\dfrac{\\partial f_i}{\\partial x_j} (jacobian)\n\n\n\\mathbb{R}^{m \\times n}\n\\mathbb{R}\n\\mathbb{R}^{m \\times n}\n\\dfrac{\\partial f}{\\partial x_{ij}}\n\n\n\n\n\n2.5 Taylor approximations\nTaylor approximations provide a way to approximate functions locally by polynomials. The idea is that for a smooth function, we can approximate it by its tangent (for the first order) or by its parabola (for the second order) at a point.\n\n2.5.1 First-order Taylor approximation\nThe first-order Taylor approximation, also known as the linear approximation, is centered around some point x_0. If f: \\mathbb{R}^n \\rightarrow \\mathbb{R} is a differentiable function, then its first-order Taylor approximation is given by:\n\nf_{x_0}^I(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0)\n\nWhere:\n\nf(x_0) is the value of the function at the point x_0.\n\\nabla f(x_0) is the gradient of the function at the point x_0.\n\nIt is very usual to replace the f(x) with f_{x_0}^I(x) near the point x_0 for simple analysis of some approaches.\n\n\n\n\n\n\nFigure 7: First order Taylor approximation near the point x_0\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x) = e^x around the point x_0 = 0, the first order Taylor approximation is: \nf_{x_0}^I(x) = 1 + x\n The second-order Taylor approximation is: \nf_{x_0}^{II}(x) = 1 + x + \\frac{x^2}{2}\n These approximations provide polynomial representations of the function near the point x_0.\n\n\n\n\n\n\n2.5.2 Second-order Taylor approximation\nThe second-order Taylor approximation, also known as the quadratic approximation, includes the curvature of the function. For a twice-differentiable function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, its second-order Taylor approximation centered at some point x_0 is:\n\nf_{x_0}^{II}(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0) + \\frac{1}{2} (x - x_0)^T \\nabla^2 f(x_0) (x - x_0)\n\nWhere:\n\n\\nabla^2 f(x_0) is the Hessian matrix of f at the point x_0.\n\n\n\n\n\n\n\nFigure 8: Second order Taylor approximation near the point x_0\n\n\n\nWhen using the linear approximation of the function is not sufficient one can consider replacing the f(x) with f_{x_0}^{II}(x) near the point x_0. In general, Taylor approximations give us a way to locally approximate functions. The first-order approximation is a plane tangent to the function at the point x_0, while the second-order approximation includes the curvature and is represented by a parabola. These approximations are especially useful in optimization and numerical methods because they provide a tractable way to work with complex functions.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCalculate first and second order Taylor approximation of the function f(x) = \\dfrac{1}{2}x^T A x - b^T x + c\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFirst-order Taylor Approximation\n\nFunction evaluation at x_0: \n  f(x_0) = \\dfrac{1}{2}x_0^T A x_0 - b^T x_0 + c\n  \nGradient of f(x): The gradient of f(x) is: \n  \\nabla f(x) = \\frac12(A+A^T) x - b\n   So, at x_0: \n  \\nabla f(x_0) = \\frac12(A+A^T) x_0 - b\n  \nFirst-order approximation: Using the first-order Taylor expansion formula: \n  f_{x_0}^I(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0)\n   Substituting the expressions: \n  \\begin{split}\n  f_{x_0}^I(x) &= \\left( \\dfrac{1}{2}x_0^T A x_0 - b^T x_0 + c \\right) + (\\frac12(A+A^T) x_0 - b)^T (x - x_0)\\\\\n  &= c - \\frac12x_0^T(A+A^T)x_0 + \\left(\\frac12(A+A^T) x_0 - b\\right)x\n  \\end{split}\n  \n\nSecond-order Taylor Approximation\n\nHessian matrix of f(x): The Hessian of f(x) is: \n  \\nabla^2 f(x) = \\frac12(A+A^T)\n   Since A is constant, it is the same at x_0.\nSecond-order approximation: Using the second-order Taylor expansion formula: \n  f_{x_0}^{II}(x) = f_{x_0}^I(x) + \\dfrac{1}{2}(x - x_0)^T A (x - x_0)\n   Substituting the expressions: \n  \\begin{split}\n  f_{x_0}^{II}(x) &= c - \\frac12x_0^T(A+A^T)x_0 + \\left(\\frac12(A+A^T) x_0 - b\\right)x + \\dfrac{1}{2}(x - x_0)^T A (x - x_0) \\\\\n  &= \\dfrac{1}{2}x^T A x - b^Tx + c = f(x)\n  \\end{split}\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhy might one choose to use a Taylor approximation instead of the original function in certain applications?\n\n\n\n\nNote, that even the second-order approximation could become inaccurate very quickly. The code for the picture below is available here: 👨‍💻\nYour browser does not support the video tag.",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#derivatives",
    "href": "docs/theory/Matrix_calculus.html#derivatives",
    "title": "Matrix calculus",
    "section": "3 Derivatives",
    "text": "3 Derivatives\n\n3.1 Naive approach\nThe basic idea of the naive approach is to reduce matrix/vector derivatives to the well-known scalar derivatives.  One of the most important practical tricks here is to separate indices of sum (i) and partial derivatives (k). Ignoring this simple rule tends to produce mistakes.\n\n\n3.2 Differential approach\nThe guru approach implies formulating a set of simple rules, which allows you to calculate derivatives just like in a scalar case. It might be convenient to use the differential notation here. 3\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x \\in S be an interior point of the set S, and let D : U \\rightarrow V be a linear operator. We say that the function f is differentiable at the point x with derivative D if for all sufficiently small h \\in U the following decomposition holds: \nf(x + h) = f(x) + D[h] + o(\\|h\\|)\n If for any linear operator D : U \\rightarrow V the function f is not differentiable at the point x with derivative D, then we say that f is not differentiable at the point x.\n\n\n\n\n\n3.2.1 Differentials\nAfter obtaining the differential notation of df we can retrieve the gradient using the following formula:\n\ndf(x) = \\langle \\nabla f(x), dx\\rangle\n\nThen, if we have a differential of the above form and we need to calculate the second derivative of the matrix/vector function, we treat “old” dx as the constant dx_1, then calculate d(df) = d^2f(x)\n\nd^2f(x) = \\langle \\nabla^2 f(x) dx_1, dx\\rangle = \\langle H_f(x) dx_1, dx\\rangle\n\n\n\n3.2.2 Properties\nLet A and B be the constant matrices, while X and Y are the variables (or matrix functions).\n\ndA = 0\nd(\\alpha X) = \\alpha (dX)\nd(AXB) = A(dX )B\nd(X+Y) = dX + dY\nd(X^T) = (dX)^T\nd(XY) = (dX)Y + X(dY)\nd\\langle X, Y\\rangle = \\langle dX, Y\\rangle+ \\langle X, dY\\rangle\nd\\left( \\dfrac{X}{\\phi}\\right) = \\dfrac{\\phi dX - (d\\phi) X}{\\phi^2}\nd\\left( \\det X \\right) = \\det X \\langle X^{-T}, dX \\rangle\nd\\left(\\text{tr } X \\right) = \\langle I, dX\\rangle\ndf(g(x)) = \\dfrac{df}{dg} \\cdot dg(x)\nH = (J(\\nabla f))^T\nd(X^{-1})=-X^{-1}(dX)X^{-1}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\nabla^2 f(x), if f(x) = \\langle Ax, x\\rangle - \\langle b, x\\rangle + c.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet’s write down differential df \n\\begin{split}\ndf &= d\\left(\\langle Ax, x\\rangle - \\langle b, x\\rangle + c\\right) \\\\\n&= \\langle Ax, dx\\rangle + \\langle x, Adx\\rangle - \\langle b, dx\\rangle \\\\\n&= \\langle Ax, dx\\rangle + \\langle A^Tx, dx\\rangle - \\langle b, dx\\rangle \\\\\n&= \\langle (A+A^T)x - b, dx\\rangle  \\\\\n\\end{split}\n\nWhich essentially means \\nabla f = (A+A^T)x - b.\nLet’s find d^2f = d(df), assuming, that dx=dx_1 = \\text{const}: \n\\begin{split}\nd^2f &= d\\left(\\langle (A+A^T)x - b, dx_1\\rangle\\right) \\\\\n&= \\langle (A+A^T)dx, dx_1\\rangle \\\\\n&= \\langle dx, (A+A^T)^Tdx_1\\rangle \\\\\n&= \\langle (A+A^T)dx_1, dx\\rangle\n\\end{split}\n\nThus, the hessian is \\nabla^2 f = (A+A^T).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(x), if f(x) = \\ln \\langle x, Ax\\rangle.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nIt is essential for A to be positive definite, because it is a logarithm argument. So, A \\in \\mathbb{S}^n_{++}Let’s find the differential first: \n\\begin{split}\n    df &= d \\left( \\ln \\langle x, Ax\\rangle \\right) = \\dfrac{d \\left( \\langle x, Ax\\rangle \\right)}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle dx, Ax\\rangle +  \\langle x, d(Ax)\\rangle}{ \\langle x, Ax\\rangle} = \\\\\n    &= \\dfrac{\\langle Ax, dx\\rangle + \\langle x, Adx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle Ax, dx\\rangle + \\langle A^T x, dx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle (A + A^T) x, dx\\rangle}{ \\langle x, Ax\\rangle}\n\\end{split}\n\nNote, that our main goal is to derive the form df = \\langle \\cdot, dx\\rangle \ndf = \\left\\langle  \\dfrac{2 A x}{ \\langle x, Ax\\rangle} , dx\\right\\rangle\n Hence, the gradient is \\nabla f(x) = \\dfrac{2 A x}{ \\langle x, Ax\\rangle}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\Vert AX - B\\Vert_F.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLet’s write down differential df \n\\begin{split}\ndf &= d\\left(\\Vert AX - B\\Vert_F\\right) = d\\left(\\langle AX - B, AX - B \\rangle^{\\frac12}\\right) \\\\\n&= \\frac12 \\langle AX - B, AX - B \\rangle^{-\\frac12} 2\\langle AX - B, d\\left(AX - B\\right) \\rangle \\\\\n&= \\dfrac{\\langle (AX-B), AdX\\rangle}{\\sqrt{\\langle AX - B, AX - B \\rangle}} =\\dfrac{\\langle A^T(AX-B), dX\\rangle}{\\Vert AX - B\\Vert_F}\n\\end{split}\n\nWhich essentially means \\nabla f = \\dfrac{A^T(AX-B)}{\\Vert AX - B\\Vert_F}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\ndf(X) = d\\left(\\langle S, X\\rangle - \\log \\det X\\right) = \\langle S, dX\\rangle - \\dfrac{d\\left(\\det X\\right)}{\\det X} = \\langle S, dX\\rangle - \\dfrac{\\det X \\langle X^{-T}, dX \\rangle}{\\det X} = \\langle S - X^{-T}, dX \\rangle\nAnd the gradient is \\nabla f(X) =  S - X^{-T}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the gradient \\nabla f(x) and the Hessian \\nabla^2f(x), if f(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet’s start by computing the differential df. We have: \nf(x) = \\ln \\left( 1 + \\exp\\langle a, x\\rangle\\right)\n\nUsing the chain rule, the differential is: \ndf = d \\left( \\ln \\left( 1 + \\exp\\langle a, x\\rangle\\right) \\right)\n= \\frac{d \\left( 1 + \\exp\\langle a, x\\rangle \\right)}{1 + \\exp\\langle a, x\\rangle}\n\nNow, compute the differential of the exponential term: \nd \\left( \\exp\\langle a, x\\rangle \\right) = \\exp\\langle a, x\\rangle \\langle a, dx\\rangle\n\nSubstituting this into the previous equation, we get: \ndf = \\frac{\\exp\\langle a, x\\rangle \\langle a, dx\\rangle}{1 + \\exp\\langle a, x\\rangle}\n\nTo express df in the desired form, we write: \ndf = \\left\\langle \\frac{\\exp\\langle a, x\\rangle}{1 + \\exp\\langle a, x\\rangle} a, dx\\right\\rangle\n\nRecall that the sigmoid function is defined as: \n\\sigma(t) = \\frac{1}{1 + \\exp(-t)}\n Thus, we can rewrite the differential as: \ndf = \\langle \\sigma(\\langle a, x\\rangle) a, dx \\rangle\n\nTherefore, the gradient is: \n\\nabla f(x) = \\sigma(\\langle a, x\\rangle) a\n\nNow, let’s find the Hessian \\nabla^2 f(x) by differentiating the gradient: \nd(\\nabla f(x)) = d\\left( \\sigma(\\langle a, x\\rangle) a \\right)\n Since a is constant, we only need to differentiate the sigmoid term. Using the chain rule: \nd\\left( \\sigma(\\langle a, x\\rangle) \\right) = \\sigma(\\langle a, x\\rangle)(1 - \\sigma(\\langle a, x\\rangle)) \\langle a, dx\\rangle\n\nTherefore: \nd(\\nabla f(x)) = \\sigma(\\langle a, x\\rangle)(1 - \\sigma(\\langle a, x\\rangle)) \\langle a, dx\\rangle a\n\nWe can now express the Hessian as: \n\\nabla^2 f(x) = \\sigma(\\langle a, x\\rangle)(1 - \\sigma(\\langle a, x\\rangle)) a a^T\n\nThus, the Hessian matrix is: \n\\nabla^2 f(x) = \\sigma(\\langle a, x\\rangle)(1 - \\sigma(\\langle a, x\\rangle)) a a^T",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#references",
    "href": "docs/theory/Matrix_calculus.html#references",
    "title": "Matrix calculus",
    "section": "4 References",
    "text": "4 References\n\nConvex Optimization book by S. Boyd and L. Vandenberghe - Appendix A. Mathematical background.\nNumerical Optimization by J. Nocedal and S. J. Wright. - Background Material.\nMatrix decompositions Cheat Sheet.\nGood introduction\nThe Matrix Cookbook\nMSU seminars (Rus.)\nOnline tool for analytic expression of a derivative.\nDeterminant derivative\nIntroduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares - book by Stephen Boyd & Lieven Vandenberghe.\nNumerical Linear Algebra course at Skoltech",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#footnotes",
    "href": "docs/theory/Matrix_calculus.html#footnotes",
    "title": "Matrix calculus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA full introduction to applied linear algebra can be found in Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares - book by Stephen Boyd & Lieven Vandenberghe, which is indicated in the source. Also, a useful refresher for linear algebra is in Appendix A of the book Numerical Optimization by Jorge Nocedal Stephen J. Wright.↩︎\nA good cheat sheet with matrix decomposition is available at the NLA course website.↩︎\nThe most comprehensive and intuitive guide about the theory of taking matrix derivatives is presented in these notes by Dmitry Kropotov team.↩︎",
    "crumbs": [
      "Theory",
      "Matrix calculus"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html",
    "href": "docs/theory/Rates_of_convergence.html",
    "title": "Rates of convergence",
    "section": "",
    "text": "In order to compare the performance of algorithms we need to define a terminology for different types of convergence. Let r_k = \\{\\|x_k - x^*\\|_2\\} be a sequence in \\mathbb{R}^n that converges to zero.\n\n\nWe can define linear convergence in two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and 0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note that this type of convergence is also sometimes called exponential or geometric. We call the q the convergence rate.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose you have two sequences with linear convergence rates q_1 = 0.1 and q_2 = 0.7, which one is faster?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{3^k}\n\nWe can immediately conclude that the sequence converges linearly with parameters q = \\dfrac{1}{3} and C = 0.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{4}{3^k}\n\nWill this sequence be convergent? What is the convergence rate?\n\n\n\n\n\n\n\nIf the sequence r_k converges to zero but does not have linear convergence, the convergence is said to be sublinear. Sometimes we can consider the following class of sublinear convergence:\n\n\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and 0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression.\n\n\n\nThe convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or 0 &lt; C_k &lt; \\infty, C_k \\to 0. Note that superlinear convergence is also linear convergence (one can even say it is linear convergence with q=0).\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and 0 &lt; C &lt; \\infty.\n\n\n\nDifference between the convergence speed\n\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#speed-of-convergence",
    "href": "docs/theory/Rates_of_convergence.html#speed-of-convergence",
    "title": "Rates of convergence",
    "section": "",
    "text": "In order to compare the performance of algorithms we need to define a terminology for different types of convergence. Let r_k = \\{\\|x_k - x^*\\|_2\\} be a sequence in \\mathbb{R}^n that converges to zero.\n\n\nWe can define linear convergence in two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and 0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note that this type of convergence is also sometimes called exponential or geometric. We call the q the convergence rate.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose you have two sequences with linear convergence rates q_1 = 0.1 and q_2 = 0.7, which one is faster?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{3^k}\n\nWe can immediately conclude that the sequence converges linearly with parameters q = \\dfrac{1}{3} and C = 0.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{4}{3^k}\n\nWill this sequence be convergent? What is the convergence rate?\n\n\n\n\n\n\n\nIf the sequence r_k converges to zero but does not have linear convergence, the convergence is said to be sublinear. Sometimes we can consider the following class of sublinear convergence:\n\n\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and 0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression.\n\n\n\nThe convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or 0 &lt; C_k &lt; \\infty, C_k \\to 0. Note that superlinear convergence is also linear convergence (one can even say it is linear convergence with q=0).\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and 0 &lt; C &lt; \\infty.\n\n\n\nDifference between the convergence speed\n\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1.",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#how-to-determine-convergence-type",
    "href": "docs/theory/Rates_of_convergence.html#how-to-determine-convergence-type",
    "title": "Rates of convergence",
    "section": "2 How to determine convergence type",
    "text": "2 How to determine convergence type\n\n2.1 Root test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of non-negative numbers, converging to zero, and let\n\nq = \\lim_{k \\to \\infty} \\sup_k \\; r_k ^{1/k}\n\n\nIf 0 \\leq q \\lt 1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q = 1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case q \\gt 1 is impossible.\n\n\n\n2.2 Ratio test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of strictly positive numbers converging to zero. Let\n\nq = \\lim_{k \\to \\infty} \\dfrac{r_{k+1}}{r_k}\n\n\nIf there exists q and 0 \\leq q \\lt  1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q does not exist, but q = \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k} \\lt  1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with a constant not exceeding q.\nIf \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} =1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\gt 1 is impossible.\nIn all other cases (i.e., when \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\lt  1 \\leq  \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k}) we cannot claim anything concrete about the convergence rate \\{r_k\\}_{k=m}^\\infty.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{k}\n\nDetermine the convergence\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{k^2}\n\nDetermine the convergence\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{k^q}, q &gt; 1\n\nDetermine the convergence\n\n\n\n\n\n\n\n\n\n\nTry to use the root test here\n\n\n\n\n\nConsider the following sequence:\n\nr_k = \\dfrac{1}{k^k}\n\nDetermine the convergence",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#references",
    "href": "docs/theory/Rates_of_convergence.html#references",
    "title": "Rates of convergence",
    "section": "3 References",
    "text": "3 References\n\nCode for convergence plots - Open In Colab\nCMC seminars (ru)\nNumerical Optimization by J.Nocedal and S.J.Wright",
    "crumbs": [
      "Theory",
      "Rates of convergence"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html",
    "href": "docs/theory/convex sets/Affine_sets.html",
    "title": "Affine set",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line passing through them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\theta \\in \\mathbb{R}\n\n\n\n\n\n\n\nFigure 1: Illustration of a line between two vectors x_1 and x_2",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#line",
    "href": "docs/theory/convex sets/Affine_sets.html#line",
    "title": "Affine set",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line passing through them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\theta \\in \\mathbb{R}\n\n\n\n\n\n\n\nFigure 1: Illustration of a line between two vectors x_1 and x_2",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#affine-set",
    "href": "docs/theory/convex sets/Affine_sets.html#affine-set",
    "title": "Affine set",
    "section": "2 Affine set",
    "text": "2 Affine set\nThe set A is called affine if for any x_1, x_2 from A the line passing through them also lies in A, i.e. \n\n\\forall \\theta \\in \\mathbb{R}, \\forall x_1, x_2 \\in A: \\theta x_1 + (1- \\theta) x_2 \\in A\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\mathbb{R}^n is an affine set.\nThe set of solutions \\left\\{x \\mid \\mathbf{A}x =  \\mathbf{b} \\right\\} is also an affine set.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#related-definitions",
    "href": "docs/theory/convex sets/Affine_sets.html#related-definitions",
    "title": "Affine set",
    "section": "3 Related definitions",
    "text": "3 Related definitions\n\n3.1 Affine combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called affine combination of x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1.\n\n\n3.2 Affine hull\nThe set of all affine combinations of points in set S is called the affine hull of S:\n\n\\mathbf{aff}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1\\right\\}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe set \\mathbf{aff}(S) is the smallest affine set containing S.\n\n\n\n\n\n\n3.3 Interior\nThe interior of the set S is defined as the following set:\n\n\\mathbf{int} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\subset S\\}\n\nwhere B(\\mathbf{x}, \\varepsilon) = \\mathbf{x} + \\varepsilon B is the ball centered at point \\mathbf{x} with radius \\varepsilon.\n\n\n3.4 Relative Interior\nThe relative interior of the set S is defined as the following set:\n\n\\mathbf{relint} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\cap \\mathbf{aff} (S) \\subseteq S\\}\n\n\n\n\n\n\n\nFigure 2: Difference between interior and relative interior\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny non-empty convex set S \\subseteq \\mathbb{R}^n has a non-empty relative interior \\mathbf{relint}(S).\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nGive an example of a set S \\subseteq \\mathbb{R}^n, which has an empty interior, but at the same time has a non-empty relative interior \\mathbf{relint}(S).",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Affine set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html",
    "href": "docs/theory/convex sets/Convex_set.html",
    "title": "Convex set",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2\n\n\n\n\n\n\nThe set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets.\n\n\n\n\n\n\nLet x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0.\n\n\n\nThe set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets.\n\n\n\n\n\n\nThe Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#definitions",
    "href": "docs/theory/convex sets/Convex_set.html#definitions",
    "title": "Convex set",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2\n\n\n\n\n\n\nThe set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets.\n\n\n\n\n\n\nLet x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0.\n\n\n\nThe set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets.\n\n\n\n\n\n\nThe Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex.",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#finding-convexity",
    "href": "docs/theory/convex sets/Convex_set.html#finding-convexity",
    "title": "Convex set",
    "section": "2 Finding convexity",
    "text": "2 Finding convexity\nIn practice, it is very important to understand whether a specific set is convex or not. Two approaches are used for this depending on the context.\n\nBy definition.\nShow that S is derived from simple convex sets using operations that preserve convexity.\n\n\n2.1 By definition\n\nx_1, x_2 \\in S, \\; 0 \\le \\theta \\le 1 \\;\\; \\rightarrow \\;\\; \\theta x_1 + (1-\\theta)x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\Vert \\mathbf{x} - \\mathbf{x}_c \\Vert \\leq r \\}) - is convex.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\Vert x - x_0\\Vert _2 \\leq \\Vert x-y\\Vert _2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\Vert x - a\\Vert _2 \\leq \\theta\\Vert x - b\\Vert _2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}\n\n\n\n\n\n\n\n2.2 Preserving convexity\n\n2.2.1 The linear combination of convex sets is convex\nLet there be 2 convex sets S_x, S_y, let the set\n\nS = \\left\\{s \\mid s = c_1 x + c_2 y, \\; x \\in S_x, \\; y \\in S_y, \\; c_1, c_2 \\in \\mathbb{R}\\right\\}\n\nTake two points from S: s_1 = c_1 x_1 + c_2 y_1, s_2 = c_1 x_2 + c_2 y_2 and prove that the segment between them \\theta  s_1 + (1 - \\theta)s_2, \\theta \\in [0,1] also belongs to S\n\n\\theta s_1 + (1 - \\theta)s_2\n\n\n\\theta (c_1 x_1 + c_2 y_1) + (1 - \\theta)(c_1 x_2 + c_2 y_2)\n\n\nc_1 (\\theta x_1 + (1 - \\theta)x_2) + c_2 (\\theta y_1 + (1 - \\theta)y_2)\n\n\nc_1 x + c_2 y \\in S\n\n\n\n2.2.2 The intersection of any (!) number of convex sets is convex\nIf the desired intersection is empty or contains one point, the property is proved by definition. Otherwise, take 2 points and a segment between them. These points must lie in all intersecting sets, and since they are all convex, the segment between them lies in all sets and, therefore, in their intersection.\n\n\n2.2.3 The image of the convex set under affine mapping is convex\n\nS \\subseteq \\mathbb{R}^n \\text{ convex}\\;\\; \\rightarrow \\;\\; f(S) = \\left\\{ f(x) \\mid x \\in S \\right\\} \\text{ convex} \\;\\;\\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\nExamples of affine functions: extension, projection, transposition, set of solutions of linear matrix inequality \\left\\{ x \\mid x_1 A_1 + \\ldots + x_m A_m \\preceq B\\right\\}. Here A_i, B \\in \\mathbf{S}^p are symmetric matrices p \\times p.\nNote also that the prototype of the convex set under affine mapping is also convex.\n\nS \\subseteq \\mathbb{R}^m \\text{ convex}\\; \\rightarrow \\; f^{-1}(S) = \\left\\{ x \\in \\mathbb{R}^n \\mid f(x) \\in S \\right\\} \\text{ convex} \\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. \n\nP = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}.\n\nDetermine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\\mathbb{V} x \\ge \\alpha\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Theory",
      "Convex sets",
      "Convex set"
    ]
  },
  {
    "objectID": "docs/theory/convex sets/index.html",
    "href": "docs/theory/convex sets/index.html",
    "title": "Convex sets",
    "section": "",
    "text": "In this chapter, a variety of convex sets and related definitions are described.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Theory",
      "Convex sets"
    ]
  },
  {
    "objectID": "docs/visualizations/condition_number_gd.html",
    "href": "docs/visualizations/condition_number_gd.html",
    "title": "How condition number affects gradient descent convergence",
    "section": "",
    "text": "Suppose, we have a strongly convex quadratic function minimization problem solved by the gradient descent method: \nf(x) = \\frac{1}{2} x^T A x - b^T x \\qquad x^{k+1} = x^k - \\alpha_k \\nabla f(x^k).\n\nThe gradient descent method with the learning rate \\alpha_k = \\frac{2}{\\mu + L} converges to the optimal solution x^* with the following guarantee: \n\\|x^{k+1} - x^*\\|_2 = \\left( \\frac{\\kappa-1}{\\kappa+1}\\right)^k \\|x^0 - x^*\\|_2 \\qquad f(x^{k+1}) - f(x^*) \\left( \\frac{\\kappa-1}{\\kappa+1}\\right)^{2k} \\left(f(x^0) - f(x^*)\\right)\n\nYour browser does not support the video tag.\nCode",
    "crumbs": [
      "Visualizations",
      "How condition number affects gradient descent convergence"
    ]
  },
  {
    "objectID": "docs/visualizations/gd_lls.html",
    "href": "docs/visualizations/gd_lls.html",
    "title": "Gradient descent for linear regression",
    "section": "",
    "text": "Your browser does not support the video tag.\nWhat could be simpler than that? However, I’m using this animation as an illustration for people unfamiliar with optimization algorithms how minimizing the loss function corresponds to training a machine learning model ( even though there are only two trainable parameters).\n Code",
    "crumbs": [
      "Visualizations",
      "Gradient descent for linear regression"
    ]
  },
  {
    "objectID": "docs/visualizations/mds.html",
    "href": "docs/visualizations/mds.html",
    "title": "Multidimensional scaling",
    "section": "",
    "text": "Your browser does not support the video tag.\nWhy does everyone use gradient methods to train large models?\nWe’ll demonstrate it with the example of solving the Multidimensional Scaling (MDS) problem. In this problem, we need to draw objects on the plane that we only know how far each of them is from each other. That is, we have a matrix of pairwise distances as input, and coordinates of objects on the plane as output. And these coordinates should be such that the ratio of distances between the objects remains as close as possible to the original ones. As a loss function is the sum of squares of deviations of distances between cities at the current coordinates and the given value.\nSuppose, we have a pairwise distance matrix for N d-dimensional objects D \\in \\mathbb{R}^{N \\times N}. Given this matrix, our goal is to recover the initial coordinates W_i \\in \\mathbb{R}^d, \\; i = 1, \\ldots, N.\n\nL(W) = \\sum_{i, j = 1}^N \\left(\\|W_i - W_j\\|^2_2 - D_{i,j}\\right)^2 \\to \\min_{W \\in \\mathbb{R}^{N \\times d}}\n\nDespite the increase in dimensionality, the entire trajectory of the method can be visualized in the plane. The number of variables in the problem here is 2*(number of cities), since for each of the cities we are looking for 2 coordinates in the plane.\n️We run in a sequence the usual gradient descent, which requires knowledge of the gradient, and the Nelder-Mead method from scipy (a widely used gradient-free method). At first the problem is solved for the 6 most populated European cities, then for 15 and for 34 (that’s all the European cities with millions of residents from wikipedia).\nIt can be seen that the more cities on the map (the higher the dimensionality of the problem), the larger the gap between gradient and gradient-free methods. This is one of the main reasons why we need not only the value of the function being minimized, but also its derivative to solve huge-scale problems.\nIt turns out that for gradient methods (under a set of reasonable assumptions), the number of iterations required before the method converges does not depend directly on the dimensionality of the problem. That is, if you consider a correctly tuned gradient descent on a ten-dimensional problem, it will need, say, at most 20 iterations to converge. And if you take conventionally the same problem but 100500-dimensional, it will need the same 20 iterations. Of course, the cost of one iteration grows with the dimensionality of the problem, but at least the number of iterations does not grow.\nCode",
    "crumbs": [
      "Visualizations",
      "Multidimensional scaling"
    ]
  },
  {
    "objectID": "docs/visualizations/sgd_divergence.html",
    "href": "docs/visualizations/sgd_divergence.html",
    "title": "Why stochastic gradient descent does not converge?",
    "section": "",
    "text": "Many people are used to apply SGD(stochastic gradient descent), but not everyone knows that it is guaranteed(!) to be non convergent in the constant stepsize (learning rate) case even for the world’s nicest function - a strongly convex quadratic (even on average).\nWhy so? The point is that SGD actually solves a different problem built on the selected data at each iteration. And this problem on the batch may be radically different from the full problem (however, the careful reader may note that this does not guarantee a very bad step). That is, at each iteration we actually converge, but to the minimum of a different problem, and each iteration we change the rules of the game for the method, preventing it from taking more than one step.\nAt the end of the attached video, you can see that using selected points from the linear regression problem, we can construct an optimal solution to the batched problem and a gradient to it - this will be called the stochastic gradient for the original problem. Most often the stochasticity of SGD is analyzed using noise in the gradient and less often we consider noise due to randomness/incompleteness of the choice of the problem to be solved (interestingly, these are not exactly the same thing).\nThis is certainly not a reason not to use the method, because convergence to an approximate solution is still guaranteed. For convex problems it is possible to deal with nonconvergence by * gradually decreasing the step (slow convergence) * increasing the patch size (expensive) * using variance reduction methods (more on this later)\nYour browser does not support the video tag.\nCode",
    "crumbs": [
      "Visualizations",
      "Why stochastic gradient descent does not converge?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "Get Started",
    "text": "Get Started\nDive into the wealth of resources available:\n\nTheoryMethodsExercisesApplicationsBenchmarks\n\n\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\nConvex function\n\n\n\n\n\n\n\n\n\n\nConjugate set\n\n\n\n\n\n\n\n\n\n\nConjugate function\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\nDual norm\n\n\n\n\n\n\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\n\n\n\n\n\n\nOptimality conditions. KKT\n\n\n\n\n\n\n\n\n\n\nConvex optimization problem\n\n\n\n\n\n\n\n\n\n\nDuality\n\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nGradient descent\n\n\n\n\n\n\n\n\n\n\nNewton method\n\n\n\n\n\n\n\n\n\n\nQuasi Newton methods\n\n\n\n\n\n\n\n\n\n\nSubgradient descent\n\n\n\n\n\n\n\n\n\n\nProjected subgradient descent\n\n\n\n\n\n\n\n\n\n\nLinear Programming and simplex algorithm\n\n\n\n\n\n\n\n\n\n\nMirror descent\n\n\n\n\n\n\n\n\n\n\nAutomatic differentiation\n\n\n\n\n\n\n\n\n\n\nStochastic gradient descent\n\n\n\n\n\n\n\n\n\n\nStochastic average gradient\n\n\n\n\n\n\n\n\n\n\nADAM: A Method for Stochastic Optimization\n\n\n\n\n\n\n\n\n\n\nLookahead Optimizer: k steps forward, 1 step back\n\n\n\n\n\n\n\n\n\n\nBee algorithm\n\n\n\n\n\n\n\n\n\n\nBinary search\n\n\n\n\n\n\n\n\n\n\nConjugate gradients\n\n\n\n\n\n\n\n\n\n\nGolden search\n\n\n\n\n\n\n\n\n\n\nInexact Line Search\n\n\n\n\n\n\n\n\n\n\nNatural gradient descent\n\n\n\n\n\n\n\n\n\n\nNelder–Mead\n\n\n\n\n\n\n\n\n\n\nSimulated annealing\n\n\n\n\n\n\n\n\n\n\nSuccessive parabolic interpolation\n\n\n\n\n\nNo matching items\n\n  \n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\n\n\nConvex sets\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\nSeparation\n\n\n\n\n\n\nConjugate sets\n\n\n\n\n\n\nConvex functions\n\n\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\n\n\nConjugate functions\n\n\n\n\n\n\nGeneral optimization problems\n\n\n\n\n\n\nDuality\n\n\n\n\n\n\nRates of convergence\n\n\n\n\n\n\nLine search\n\n\n\n\n\n\nCVXPY library\n\n\n\n\n\n\nAutomatic differentiation\n\n\n\n\n\n\nZero order methods\n\n\n\n\n\n\nFirst order methods\n\n\n\n\n\n\nUncategorized\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\nA^* algorithm for path finding\n\n\n\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nKnapsack problem\n\n\n\n\n\n\n\n\n\n\nLinear least squares\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood estimation\n\n\n\n\n\n\n\n\n\n\nMinimum volume ellipsoid\n\n\n\n\n\n\n\n\n\n\nNeural Network Loss Surface Visualization\n\n\n\n\n\n\n\n\n\n\nNeural network Lipschitz constant\n\n\n\n\n\n\n\n\n\n\nPrincipal component analysis\n\n\n\n\n\n\n\n\n\n\nRendezvous problem\n\n\n\n\n\n\n\n\n\n\nTotal variation in-painting\n\n\n\n\n\n\n\n\n\n\nTravelling salesman problem\n\n\n\n\n\n\n\n\n\n\nTwo way partitioning problem\n\n\n\n\n\nNo matching items\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nCNN on FashionMNIST\n\n\n\n\n\n\n\n\n\n\nLinear Least Squares\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#our-principles",
    "href": "index.html#our-principles",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "Our Principles",
    "text": "Our Principles\n\nOpen Education for All: At 💎fmin.xyz, we steadfastly believe in dismantling barriers to education. All content on this site is open-sourced and freely accessible to everyone, echoing our commitment to inclusive learning.\nDesigned for Learners at Various Levels: Whether you are a researcher, a student, or an enthusiast with some prior expertise, 💎fmin.xyz is tailored to cater to your needs, providing a vast array of content ranging from fundamental theories to advanced applications.\nStructured Learning: Every page on 💎fmin.xyz is meticulously structured as a self-sufficient unit, providing concise and comprehensive material on a specific topic. This modular design allows for flexible and personalized learning paths.\nContinuous Improvement: The structure and content of 💎fmin.xyz are dynamic. We are devoted to refining and expanding our resources iteratively, adapting to the evolving needs of our learning community and the advancements in the field."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "Key Features",
    "text": "Key Features\n\nRich Content: Delve into a wide spectrum of topics from matrix calculus, convex sets, to various optimization methods and their applications. With a plethora of exercises and benchmarks, 💎fmin.xyz is a treasure trove of knowledge waiting to be explored.\nInteractive Visualizations: Engage with our extensive collection of Python visualizations designed to elucidate key mathematical concepts, enhancing your understanding and learning experience.\nEfficient Search Functionality: With a user-friendly built-in search engine, finding specific content on 💎fmin.xyz is just a click away. Navigate through our extensive repository with ease and efficiency."
  },
  {
    "objectID": "index.html#technical-insights",
    "href": "index.html#technical-insights",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "Technical Insights",
    "text": "Technical Insights\n\nEasy to Use & Share: Each page is formatted as a Markdown file or Jupyter notebook, facilitating easy copying, sharing, and utilization for your purposes.\nBuilt with Quarto: 💎fmin.xyz is powered by the Quarto system, ensuring a smooth and responsive user experience.\nContribute & Collaborate: Your insights are valuable! Contributions are not just welcomed but encouraged. Check out our guides at the repository and join us in refining and expanding this educational endeavor."
  }
]